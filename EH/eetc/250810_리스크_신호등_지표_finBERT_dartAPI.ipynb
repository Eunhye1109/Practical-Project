{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR-wP_Jrcg1r",
        "outputId": "25db0f59-fc43-49bd-ac72-19fff5ba4ce1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/risk-light-model-v2.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 250810 0.3ver(ÏµúÏ¢Ö)"
      ],
      "metadata": {
        "id": "HIw5aiWqCN37"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ejZlGyIPH_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, io, sys, json, zipfile, time, hashlib, threading, queue\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import xml.etree.ElementTree as ET\n",
        "import argparse\n",
        "\n",
        "# OpenAI (ÌÇ§Îäî ÌôòÍ≤ΩÎ≥ÄÏàò OPENAI_API_KEY Î°ú Ï£ºÏûÖ)\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    OpenAI = None  # ÎùºÏù¥Î∏åÎü¨Î¶¨ ÎØ∏ÏÑ§Ïπò ÌôòÍ≤Ω ÎåÄÎπÑ\n",
        "\n",
        "# -----------------------------\n",
        "# ÏÑ§Ï†ï\n",
        "# -----------------------------\n",
        "KST = timezone(timedelta(hours=9))\n",
        "HTTP_TIMEOUT = 15\n",
        "DEFAULT_DAYS = 50\n",
        "DEFAULT_MAX_ITEMS = 40\n",
        "USER_AGENT = \"CorpSentimentCLI/1.0 (+https://example.local)\"\n",
        "\n",
        "DART_API_KEY = os.getenv(\"DART_API_KEY\", \"\")\n",
        "NAVER_ID = os.getenv(\"NAVER_CLIENT_ID\", \"\")\n",
        "NAVER_SECRET = os.getenv(\"NAVER_CLIENT_SECRET\", \"\")\n",
        "NO_FINBERT = os.getenv(\"NO_FINBERT\", \"0\") == \"1\"  # ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú ÎπÑÌôúÏÑ±Ìôî Í∞ÄÎä•\n",
        "FINBERT_MAX_LOAD_SEC = int(os.getenv(\"FINBERT_MAX_LOAD_SEC\", \"10\"))\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fQBlUfH3Bc9h"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Í≥µÌÜµ Ïú†Ìã∏\n",
        "# -----------------------------\n",
        "def _sha1(text: str) -> str:\n",
        "    import hashlib\n",
        "    return hashlib.sha1((text or \"\").encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def to_kst(dt: datetime) -> datetime:\n",
        "    return (dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt).astimezone(KST)\n",
        "\n",
        "def parse_rfc2822_date(s: str) -> Optional[datetime]:\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        return parsedate_to_datetime(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def clip_days(dt: datetime, days: int = DEFAULT_DAYS) -> bool:\n",
        "    return (to_kst(datetime.utcnow()) - to_kst(dt)) <= timedelta(days=days)\n",
        "\n",
        "def _is_spc_like(name: str) -> bool:\n",
        "    low = (name or \"\").lower()\n",
        "    return any(p.lower() in low for p in SPC_PATTERNS)\n",
        "\n",
        "def _reason_from_metrics(corp_name: str, neg_ratio: float, red_hits: int, label: str, news_count: int) -> str:\n",
        "    \"\"\"OPENAI_API_KEY ÏóÜÍ±∞ÎÇò GPT Ïã§Ìå® Ïãú Ìè¥Î∞± Ìïú Ï§Ñ ÏÑ§Î™Ö.\"\"\"\n",
        "    if news_count == 0:\n",
        "        return f\"ÏµúÍ∑º Í∏∞ÏÇ¨ Ïã†Ìò∏Í∞Ä ÏóÜÏñ¥ Î∂àÌôïÏã§ÏÑ±Ïù¥ Ïª§ {label} ÌåêÎã®Ïù¥Î©∞, Ï∂îÏù¥Î•º Í¥ÄÎßùÌïòÎäî Ìé∏Ïù¥ ÏïàÏ†ÑÌï©ÎãàÎã§.\"\n",
        "    if label == \"Í∏çÏ†ï\":\n",
        "        return f\"ÏµúÍ∑º Î≥¥ÎèÑÏóêÏÑú Î∂ÄÏ†ï Ïã†Ìò∏Í∞Ä ÎìúÎ¨ºÏñ¥({neg_ratio*100:.1f}%¬∑Î†àÎìúÌÇ§ÏõåÎìú {red_hits}Í±¥) Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú {label}ÏûÖÎãàÎã§.\"\n",
        "    if label == \"Ï§ëÎ¶Ω\":\n",
        "        return f\"Î≥¥ÎèÑ ÎÇ¥ Í∏ç¬∑Î∂ÄÏ†ï Ïã†Ìò∏Í∞Ä ÌòºÏû¨Ìï¥({neg_ratio*100:.1f}%¬∑Î†àÎìúÌÇ§ÏõåÎìú {red_hits}Í±¥) {label}ÏúºÎ°ú ÌåêÎã®Îê©ÎãàÎã§.\"\n",
        "    return f\"Î∂ÄÏ†ï Ïã†Ìò∏ ÎπÑÏ§ëÏù¥ ÎÜíÏïÑ({neg_ratio*100:.1f}%¬∑Î†àÎìúÌÇ§ÏõåÎìú {red_hits}Í±¥) {label}ÏûÖÎãàÎã§.\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1) DART corpCode Î°úÎî©/Í≤ÄÏÉâ\n",
        "# -----------------------------\n",
        "def load_corp_list_from_dart(DART_API_KEY: str) -> List[Dict[str, str]]:\n",
        "    if not DART_API_KEY:\n",
        "        raise RuntimeError(\"‚ùå DART_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\")\n",
        "    print(\"‚è≥ DART Í∏∞ÏóÖÎ™©Î°ù Î°úÎî© Ï§ë...\", flush=True)\n",
        "    api_key = DART_API_KEY  # Ïò§ÌÉÄ/Ïä§ÏΩîÌîÑ ÏïàÏ†Ñ Î≥¥Ï†ï\n",
        "    url = f\"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}\"\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    res = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)\n",
        "    res.raise_for_status()\n",
        "    zf = zipfile.ZipFile(io.BytesIO(res.content))\n",
        "    with zf.open(\"CORPCODE.xml\") as f:\n",
        "        tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "    out = []\n",
        "    for el in root.findall(\"list\"):\n",
        "        out.append({\n",
        "            \"corp_code\": (el.findtext(\"corp_code\") or \"\").strip(),\n",
        "            \"corp_name\": (el.findtext(\"corp_name\") or \"\").strip(),\n",
        "            \"stock_code\": (el.findtext(\"stock_code\") or \"\").strip(),\n",
        "            \"modify_date\": (el.findtext(\"modify_date\") or \"\").strip(),\n",
        "        })\n",
        "    print(f\"‚úÖ Í∏∞ÏóÖÎ™©Î°ù {len(out):,}Í±¥ Î°úÎìú ÏôÑÎ£å\", flush=True)\n",
        "    return out\n",
        "\n",
        "def find_corp_candidates(corps: List[Dict[str, str]], keyword: str, limit: int = 20) -> List[Dict[str, str]]:\n",
        "    kw = keyword.strip().lower()\n",
        "    print(f\"üîé ÌõÑÎ≥¥ Í≤ÄÏÉâ: '{keyword}'\", flush=True)\n",
        "    cand = [c for c in corps if kw in c[\"corp_name\"].lower()]\n",
        "    def score(c):\n",
        "        name = c[\"corp_name\"]; sc = 0\n",
        "        if name == keyword: sc += 100\n",
        "        if name.startswith(keyword): sc += 50\n",
        "        if c.get(\"stock_code\"): sc += 10\n",
        "        sc += max(0, 20 - abs(len(name) - len(keyword)))\n",
        "        return -sc\n",
        "    cand.sort(key=score)\n",
        "    print(f\"‚úÖ ÌõÑÎ≥¥ {len(cand)}Í±¥\", flush=True)\n",
        "    return cand[:limit]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Îâ¥Ïä§ ÏàòÏßë\n",
        "# -----------------------------\n",
        "def fetch_news_naver(query: str, display: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    if not (NAVER_ID and NAVER_SECRET):\n",
        "        raise RuntimeError(\"NAVER ÌÇ§ ÏóÜÏùå\")\n",
        "    print(f\"‚è≥ NAVER Îâ¥Ïä§ ÏàòÏßë: '{query}'\", flush=True)\n",
        "    headers = {\n",
        "        \"X-Naver-Client-Id\": NAVER_ID,\n",
        "        \"X-Naver-Client-Secret\": NAVER_SECRET,\n",
        "        \"User-Agent\": USER_AGENT\n",
        "    }\n",
        "    params = {\"query\": query, \"display\": min(100, display), \"start\": 1, \"sort\": \"date\"}\n",
        "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
        "    r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    items = []\n",
        "    for it in data.get(\"items\", []):\n",
        "        dt = parse_rfc2822_date(it.get(\"pubDate\",\"\")) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt, DEFAULT_DAYS):\n",
        "            continue\n",
        "        title = (it.get(\"title\") or \"\").replace(\"<b>\",\"\").replace(\"</b>\",\"\")\n",
        "        desc  = (it.get(\"description\") or \"\").replace(\"<b>\",\"\").replace(\"</b>\",\"\")\n",
        "        link  = it.get(\"link\") or it.get(\"originallink\") or \"\"\n",
        "        items.append({\"title\": title, \"description\": desc, \"link\": link, \"date\": dt.isoformat()})\n",
        "        if len(items) >= DEFAULT_MAX_ITEMS:\n",
        "            break\n",
        "    print(f\"‚úÖ NAVER {len(items)}Í±¥\", flush=True)\n",
        "    return items\n",
        "\n",
        "def fetch_news_google_rss(query: str, max_items: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    print(f\"‚è≥ Google RSS ÏàòÏßë(Î∞±ÏóÖ): '{query}'\", flush=True)\n",
        "    q = requests.utils.quote(query)\n",
        "    url = f\"https://news.google.com/rss/search?q={q}+when:{DEFAULT_DAYS}d&hl=ko&gl=KR&ceid=KR:ko\"\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    r = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    root = ET.fromstring(r.text)\n",
        "    ch = root.find(\"channel\")\n",
        "    out = []\n",
        "    if ch is None: return out\n",
        "    for item in ch.findall(\"item\"):\n",
        "        title = item.findtext(\"title\") or \"\"\n",
        "        desc  = item.findtext(\"description\") or \"\"\n",
        "        link  = item.findtext(\"link\") or \"\"\n",
        "        pub   = item.findtext(\"{http://purl.org/dc/elements/1.1/}date\") or item.findtext(\"pubDate\") or \"\"\n",
        "        dt    = parse_rfc2822_date(pub) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt, DEFAULT_DAYS):\n",
        "            continue\n",
        "        out.append({\"title\": title, \"description\": desc, \"link\": link, \"date\": dt.isoformat()})\n",
        "        if len(out) >= max_items:\n",
        "            break\n",
        "    print(f\"‚úÖ Google RSS {len(out)}Í±¥\", flush=True)\n",
        "    return out\n",
        "\n",
        "def fetch_news(query: str) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        items = fetch_news_naver(query, display=DEFAULT_MAX_ITEMS)\n",
        "        if items:\n",
        "            return items\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è NAVER Ïã§Ìå®: {e}\", flush=True)\n",
        "    try:\n",
        "        return fetch_news_google_rss(query, max_items=DEFAULT_MAX_ITEMS)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Google RSS Ïã§Ìå®: {e}\", flush=True)\n",
        "        return []\n",
        "\n",
        "# ---------------------------\n",
        "# 3) ÌéÄÎçîÎ©òÌÑ∏ Î¶¨Ïä§ÌÅ¨ Ï†êÏàò (Í∏∞Ï°¥ Ìï®Ïàò Ïú†ÏßÄ)\n",
        "# ---------------------------\n",
        "def score_fundamental(corp_code: str, prefer_year: int | None = None) -> dict:\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    BASE = \"https://opendart.fss.or.kr\"\n",
        "    REPRT_PRIORITY = [\"11014\", \"11012\", \"11013\"]  # 3Q > 2Q > 1Q\n",
        "    API_KEY = os.getenv(\"DART_API_KEY\", \"\")\n",
        "    if not API_KEY:\n",
        "        return {\"year\": prefer_year or datetime.now().year, \"reprt_code\": \"11011\", \"flag\": 0, \"reasons\": []}\n",
        "\n",
        "    session = requests.Session()\n",
        "    def _j(path, **kw):\n",
        "        p = {\"crtfc_key\": API_KEY}; p.update(kw)\n",
        "        try:\n",
        "            r = session.get(f\"{BASE}{path}\", params=p, timeout=HTTP_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            if str(data.get(\"status\", \"000\")) != \"000\":\n",
        "                return []\n",
        "            return data.get(\"list\", []) or []\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    y0 = prefer_year or datetime.now().year\n",
        "    chosen = None\n",
        "    for y in [y0, y0 - 1]:\n",
        "        for rc in REPRT_PRIORITY:\n",
        "            if any([\n",
        "                _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/mrhlSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/hyslrChgSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/cprndNrdmpBlce.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "            ]):\n",
        "                chosen = (y, rc); break\n",
        "        if chosen: break\n",
        "    if not chosen:\n",
        "        return {\"year\": y0, \"reprt_code\": \"11011\", \"flag\": 0, \"reasons\": []}\n",
        "    year, rc = chosen\n",
        "\n",
        "    flag = 0; reasons = []\n",
        "\n",
        "    irds = _j(\"/api/irdsSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    if irds: flag += 1; reasons.append(\"Í∞êÏûê Í≥µÏãú\")\n",
        "\n",
        "    hyslr = _j(\"/api/hyslrChgSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    if hyslr: flag += 1; reasons.append(\"ÏµúÎåÄÏ£ºÏ£º Î≥ÄÎèô\")\n",
        "\n",
        "    mr = _j(\"/api/mrhlSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    hold = None\n",
        "    for r in mr:\n",
        "        if (r.get(\"se\") or \"\").strip() == \"ÏÜåÏï°Ï£ºÏ£º\":\n",
        "            try:\n",
        "                s = str(r.get(\"hold_stock_rate\")).replace(\",\",\"\").strip()\n",
        "                hold = float(s) if s not in (\"\",\"-\") else None\n",
        "            except Exception:\n",
        "                hold = None\n",
        "    if hold is not None and hold >= 70:\n",
        "        flag += 1; reasons.append(f\"ÏÜåÏï°Ï£ºÏ£º {hold:.1f}%\")\n",
        "\n",
        "    cur = _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    prv = _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(year - 1), reprt_code=rc)\n",
        "    def _emp_total(rows):\n",
        "        if not rows: return None\n",
        "        s=0; seen=False\n",
        "        for r in rows:\n",
        "            try:\n",
        "                v = float(str(r.get(\"sm\")).replace(\",\",\"\").strip())\n",
        "                s += v; seen=True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return s if seen else None\n",
        "    tcur, tprv = _emp_total(cur), _emp_total(prv)\n",
        "    if tcur and tprv and tprv > 0:\n",
        "        yoy = (tcur - tprv) / tprv * 100\n",
        "        if yoy <= -10:\n",
        "            flag += 1; reasons.append(f\"ÏßÅÏõêÏàò YoY {yoy:.1f}%\")\n",
        "\n",
        "    bd = _j(\"/api/cprndNrdmpBlce.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    def _tof2(x):\n",
        "        try: s=str(x).replace(\",\",\"\").strip(); return float(s) if s not in (\"\",\"-\") else None\n",
        "        except: return None\n",
        "    if bd:\n",
        "        sm = _tof2(bd[0].get(\"sm\")); s1 = _tof2(bd[0].get(\"yy1_below\"))\n",
        "        if sm and s1 and sm > 0 and s1/sm > 0.5:\n",
        "            flag += 1; reasons.append(f\"1ÎÖÑÎÇ¥ ÏÉÅÌôò {s1/sm*100:.1f}%\")\n",
        "\n",
        "    ot = _j(\"/api/otrCprInvstmntSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    def _tof3(x):\n",
        "        try: s=str(x).replace(\",\",\"\").strip(); return float(s) if s not in (\"\",\"-\") else None\n",
        "        except: return None\n",
        "    if any(((_tof3(r.get(\"recent_bsns_year_fnnr_sttus_thstrm_ntpf\")) or 0) < 0) for r in ot):\n",
        "        flag += 1; reasons.append(\"Ï†ÅÏûê ÌîºÌà¨ÏûêÎ≤ïÏù∏ Ï°¥Ïû¨\")\n",
        "\n",
        "    return {\"year\": year, \"reprt_code\": rc, \"flag\": flag, \"reasons\": reasons}\n",
        "\n",
        "# -----------------------------\n",
        "# 4) FinBERT (ÌÉÄÏûÑÏïÑÏõÉ/ÏòµÏÖò) + Î∞±ÏóÖÎ£∞\n",
        "# -----------------------------\n",
        "_FINBERT = {\"tok\": None, \"mdl\": None, \"device\": \"cpu\"}\n",
        "_SENT_CACHE: Dict[str, float] = {}\n",
        "\n",
        "def _load_finbert_worker(out_q: \"queue.Queue\"):\n",
        "    try:\n",
        "        import torch\n",
        "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "        name = \"yiyanghkust/finbert-tone\"\n",
        "        tok = AutoTokenizer.from_pretrained(name)\n",
        "        mdl = AutoModelForSequenceClassification.from_pretrained(name)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        mdl = mdl.to(device)\n",
        "        out_q.put((tok, mdl, device, None))\n",
        "    except Exception as e:\n",
        "        out_q.put((None, None, None, e))\n",
        "\n",
        "def _ensure_finbert_loaded(timeout_sec: int = FINBERT_MAX_LOAD_SEC) -> bool:\n",
        "    if _FINBERT[\"tok\"] is not None: return True\n",
        "    if NO_FINBERT: return False\n",
        "    print(f\"‚è≥ FinBERT Î°úÎî© ÏãúÎèÑ(ÏµúÎåÄ {timeout_sec}s)...\", flush=True)\n",
        "    out_q: \"queue.Queue\" = queue.Queue(maxsize=1)\n",
        "    t = threading.Thread(target=_load_finbert_worker, args=(out_q,), daemon=True)\n",
        "    t.start()\n",
        "    try:\n",
        "        tok, mdl, device, err = out_q.get(timeout=timeout_sec)\n",
        "    except queue.Empty:\n",
        "        print(\"‚ö†Ô∏è FinBERT Î°úÎî© ÌÉÄÏûÑÏïÑÏõÉ ‚Üí Î∞±ÏóÖÎ£∞Î°ú Ï†ÑÌôò\", flush=True)\n",
        "        return False\n",
        "    if err is not None or tok is None or mdl is None:\n",
        "        print(f\"‚ö†Ô∏è FinBERT Î°úÎî© Ïã§Ìå® ‚Üí Î∞±ÏóÖÎ£∞Î°ú Ï†ÑÌôò ({err})\", flush=True)\n",
        "        return False\n",
        "    _FINBERT[\"tok\"], _FINBERT[\"mdl\"], _FINBERT[\"device\"] = tok, mdl, device\n",
        "    print(\"‚úÖ FinBERT Î°úÎî© ÏôÑÎ£å\", flush=True)\n",
        "    return True\n",
        "\n",
        "def analyze_sentiment(texts: List[str]) -> List[float]:\n",
        "    if not texts:\n",
        "        return []\n",
        "    keys = [_sha1(t) for t in texts]\n",
        "    out: List[Optional[float]] = [None] * len(texts)\n",
        "    pending = []\n",
        "    for i, (k, t) in enumerate(zip(keys, texts)):\n",
        "        if k in _SENT_CACHE:\n",
        "            out[i] = _SENT_CACHE[k]\n",
        "        else:\n",
        "            pending.append(i)\n",
        "\n",
        "    use_model = _ensure_finbert_loaded()\n",
        "    if use_model and pending:\n",
        "        import torch\n",
        "        tok, mdl, device = _FINBERT[\"tok\"], _FINBERT[\"mdl\"], _FINBERT[\"device\"]\n",
        "        bs = 16\n",
        "        for s in range(0, len(pending), bs):\n",
        "            idxs = pending[s:s+bs]\n",
        "            batch = [(texts[i] or \"\")[:1024] for i in idxs]\n",
        "            inputs = tok(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = mdl(**inputs).logits\n",
        "            probs = torch.softmax(logits, dim=1)[:,2].detach().cpu().numpy().tolist()\n",
        "            for i, p in zip(idxs, probs):\n",
        "                out[i] = float(p); _SENT_CACHE[keys[i]] = out[i]\n",
        "\n",
        "    # Î∞±ÏóÖÎ£∞\n",
        "    for i in range(len(out)):\n",
        "        if out[i] is None:\n",
        "            low = (texts[i] or \"\").lower()\n",
        "            hits = sum(1 for w in BACKUP_NEG_LEXICON if w in low)\n",
        "            out[i] = min(1.0, hits/3.0)\n",
        "            _SENT_CACHE[keys[i]] = out[i]\n",
        "\n",
        "    return [float(x or 0.0) for x in out]\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Î≤†Ïù¥ÏßÄÏïà Ïä§Î¨¥Îî© Î∞è ÎùºÎ≤®\n",
        "# -----------------------------\n",
        "def smoothed_neg_ratio(neg_probs: List[float], thr: float=0.5, alpha: float=1.0, beta: float=3.0) -> float:\n",
        "    if not neg_probs:\n",
        "        return alpha / (alpha + beta)\n",
        "    n = len(neg_probs)\n",
        "    hits = sum(1 for p in neg_probs if p >= thr)\n",
        "    return (hits + alpha) / (n + alpha + beta)\n",
        "\n",
        "def judge_sentiment_label(neg_ratio: float) -> str:\n",
        "    if neg_ratio >= 0.60: return \"Î∂ÄÏ†ï\"\n",
        "    if neg_ratio >= 0.30: return \"Ï§ëÎ¶Ω\"\n",
        "    return \"Í∏çÏ†ï\"\n",
        "\n",
        "def summarize_company(c: Dict[str, str]) -> str:\n",
        "    listed = \"ÏÉÅÏû•\" if c.get(\"stock_code\") else \"ÎπÑÏÉÅÏû•\"\n",
        "    sx = f\"{c['corp_name']} ({listed}\"\n",
        "    if c.get(\"stock_code\"):\n",
        "        sx += f\", Ï¢ÖÎ™©ÏΩîÎìú {c['stock_code']}\"\n",
        "    sx += f\", corp_code {c['corp_code']})\"\n",
        "    return sx\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Ï¢ÖÌï© ÌèâÍ∞Ä(Îâ¥Ïä§+ÌéÄÎçîÎ©òÌÑ∏+Í∞úÌô©) Ìó¨Ìçº\n",
        "# -----------------------------\n",
        "def _build_prior_context(corp: Dict[str,str], news_count: int) -> dict:\n",
        "    name = corp.get(\"corp_name\",\"\")\n",
        "    return {\n",
        "        \"corp_name\": name,\n",
        "        \"corp_code\": corp.get(\"corp_code\",\"\"),\n",
        "        \"is_listed\": \"ÏÉÅÏû•\" if corp.get(\"stock_code\") else \"ÎπÑÏÉÅÏû•\",\n",
        "        \"name_pattern\": \"SPC ÏùòÏã¨\" if _is_spc_like(name) else \"ÏùºÎ∞òÎ≤ïÏù∏\",\n",
        "        \"parent_hint\": \"Ïø†Ìå° Í¥ÄÎ†® ÏÇ¨Î™Ö\" if \"Ïø†Ìå°\" in name else \"Ï†ïÎ≥¥ÏóÜÏùå\",\n",
        "        \"age_hint\": \"Ï†ïÎ≥¥ÏóÜÏùå\",  # (Ï∂îÌõÑ Ïó∞ÌòÅ/ÏÑ§Î¶ΩÏó∞ÎèÑ Î∂ôÏù¥Î©¥ Ï±ÑÏõÄ)\n",
        "        \"news_count\": news_count,\n",
        "    }\n",
        "\n",
        "def _compute_news_metrics(items: List[Dict[str,Any]]) -> Tuple[float, str, int, List[Tuple[float,Dict[str,Any]]]]:\n",
        "    texts = [f\"{it.get('title','')} {it.get('description','')}\".strip() for it in items]\n",
        "    neg_probs = analyze_sentiment(texts) if texts else []\n",
        "    neg_ratio = smoothed_neg_ratio(neg_probs, thr=0.5, alpha=1.0, beta=3.0)\n",
        "    label_news = judge_sentiment_label(neg_ratio)\n",
        "    red_hits = 0\n",
        "    for t in texts:\n",
        "        low = (t or \"\").lower()\n",
        "        red_hits += sum(1 for w in RED_KEYWORDS if w.lower() in low)\n",
        "    scored = sorted([(p, it) for it, p in zip(items, neg_probs)], key=lambda x: x[0], reverse=True) if items else []\n",
        "    top_neg = [s for s in scored[:3] if s[0] >= 0.5]\n",
        "    return neg_ratio, label_news, red_hits, top_neg\n",
        "\n",
        "def _combine_risk_label(fund_flag: int, label_news: str, news_count: int) -> Tuple[str,int]:\n",
        "    \"\"\"\n",
        "    Í∞ÑÎã® Ìï©ÏÇ∞ Í∑úÏπô:\n",
        "      - Îâ¥Ïä§ ÎùºÎ≤® Ï†êÏàò: Í∏çÏ†ï0 / Ï§ëÎ¶Ω1 / Î∂ÄÏ†ï2\n",
        "      - total_score = fund_flag + news_score (+ Îâ¥Ïä§ÏóÜÏùåÏù¥Î©¥ news_score=1Î°ú Í¥ÄÎßù Î∞òÏòÅ)\n",
        "      - ÏµúÏ¢ÖÎùºÎ≤®: total>=4 Î∂ÄÏ†ï, total>=2 Ï§ëÎ¶Ω, else Í∏çÏ†ï\n",
        "    \"\"\"\n",
        "    news_score = {\"Í∏çÏ†ï\":0, \"Ï§ëÎ¶Ω\":1, \"Î∂ÄÏ†ï\":2}.get(label_news, 1)\n",
        "    if news_count == 0:\n",
        "        news_score = 1  # Í¥ÄÎßù ÌéòÎÑêÌã∞\n",
        "    total = fund_flag + news_score\n",
        "    if total >= 4: return \"Î∂ÄÏ†ï\", total\n",
        "    if total >= 2: return \"Ï§ëÎ¶Ω\", total\n",
        "    return \"Í∏çÏ†ï\", total\n",
        "\n",
        "# --- Ï¥ùÌèâ: Í∑úÏπô & GPT ---\n",
        "def _overall_comment_fallback(combined: Dict[str,Any],\n",
        "                              news: Dict[str,Any],\n",
        "                              fundamental: Dict[str,Any],\n",
        "                              prior: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    ÏúÑÌóòÎèÑ %ÏôÄ Ïã†Ìò∏Ïóê Í∏∞Î∞òÌïú Í∑úÏπôÌòï Ï¥ùÌèâ. 1~2Î¨∏Ïû•.\n",
        "    \"\"\"\n",
        "    pct = float(combined.get(\"risk_pct\", 0.0))\n",
        "    label = combined.get(\"final_label\", \"Ï§ëÎ¶Ω\")\n",
        "    fund_flag = int(fundamental.get(\"flag\", 0) or 0)\n",
        "    red = int(news.get(\"red_hits\", 0) or 0)\n",
        "    ncnt = int(news.get(\"news_count\", 0) or 0)\n",
        "\n",
        "    # Î≤ÑÌÇ∑\n",
        "    if pct >= 75:\n",
        "        bucket = \"Îß§Ïö∞ ÎÜíÏùå(Í≤ΩÍ≥Ñ)\"\n",
        "        advice = \"Îã®Í∏∞ ÎÖ∏Ï∂ú Ï∂ïÏÜå¬∑Í¥ÄÎßù Í∂åÍ≥†\"\n",
        "    elif pct >= 50:\n",
        "        bucket = \"ÎÜíÏùå(Ï£ºÏùò)\"\n",
        "        advice = \"Î≥¥ÏàòÏ†Å Ï†ëÍ∑º Î∞è Î™®ÎãàÌÑ∞ÎßÅ Í∞ïÌôî\"\n",
        "    elif pct >= 25:\n",
        "        bucket = \"Î≥¥ÌÜµ\"\n",
        "        advice = \"Ï§ëÎ¶Ω Ïú†ÏßÄ, Ïù¥Î≤§Ìä∏ Ï≤¥ÌÅ¨\"\n",
        "    else:\n",
        "        bucket = \"ÎÇÆÏùå\"\n",
        "        advice = \"Í∏∞Î≥∏ Ïú†ÏßÄ, Ïù¥Ïäà Î∞úÏÉù Í∞êÏãú\"\n",
        "\n",
        "    # Î≥¥Ï†ï ÌûåÌä∏\n",
        "    hint = []\n",
        "    if ncnt == 0:\n",
        "        hint.append(\"ÏµúÍ∑º Îâ¥Ïä§ Î∂ÄÏû¨\")\n",
        "    if red > 0:\n",
        "        hint.append(f\"Î†àÎìúÌÇ§ÏõåÎìú {red}Í±¥\")\n",
        "    if fund_flag >= 2:\n",
        "        hint.append(f\"ÌéÄÎçîÎ©òÌÑ∏ ÌîåÎûòÍ∑∏ {fund_flag}Í±¥\")\n",
        "\n",
        "    tail = f\"({', '.join(hint)})\" if hint else \"\"\n",
        "    return f\"ÏúÑÌóòÎèÑ {pct:.1f}%({bucket})Î°ú {label} ÌåêÎã®. {advice}{(' ' + tail) if tail else ''}.\"\n",
        "\n",
        "def _gpt_overall_comment(company: Dict[str,str],\n",
        "                         fundamental: Dict[str,Any],\n",
        "                         news: Dict[str,Any],\n",
        "                         prior: Dict[str,Any],\n",
        "                         combined: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    JSON Ïª®ÌÖçÏä§Ìä∏ Í∏∞Î∞ò GPT Ï¥ùÌèâ. Ïã§Ìå® Ïãú Í∑úÏπôÌòï Ìè¥Î∞±.\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(OPENAI_API_KEY)\n",
        "    if not (OpenAI and api_key):\n",
        "        return _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        sys_prompt = (\n",
        "            \"ÎÑàÎäî ÌïúÍµ≠ Í∏∞ÏóÖ Î¶¨Ïä§ÌÅ¨Î•º Ï¢ÖÌï© Î∂ÑÏÑù ÌõÑ ÏöîÏïΩÌïòÎäî Ï†ÑÎ¨∏ Ïï†ÎÑêÎ¶¨Ïä§Ìä∏Îã§. \"\n",
        "            \"ÏûÖÎ†• JSONÏùÑ Î∞îÌÉïÏúºÎ°ú 1~2Î¨∏Ïû•, 40Ïûê Ïù¥ÎÇ¥Î°ú Ï¥ùÌèâÏùÑ ÏûëÏÑ±ÌïòÎùº. \"\n",
        "            \"Í≥ºÏû• ÌëúÌòÑÏùÄ ÌîºÌïòÍ≥†, ÏúÑÌóòÎèÑ%¬∑ÎùºÎ≤®¬∑ÌïµÏã¨ Í∑ºÍ±∞(1Í∞ú ÎÇ¥Ïô∏)¬∑Í∞ÑÎã® Í∂åÍ≥†Î•º Ìè¨Ìï®ÌïòÎùº.\"\n",
        "        )\n",
        "        payload = {\n",
        "            \"company\": company,\n",
        "            \"fundamental\": {\n",
        "                \"flag\": fundamental.get(\"flag\", 0),\n",
        "                \"reasons\": fundamental.get(\"reasons\", []),\n",
        "            },\n",
        "            \"news\": {\n",
        "                \"news_count\": news.get(\"news_count\", 0),\n",
        "                \"neg_ratio\": round(news.get(\"neg_ratio\", 0.0), 4),\n",
        "                \"label_news\": news.get(\"label_news\", \"Ï§ëÎ¶Ω\"),\n",
        "                \"red_hits\": news.get(\"red_hits\", 0),\n",
        "            },\n",
        "            \"prior\": {\n",
        "                \"is_listed\": prior.get(\"is_listed\"),\n",
        "                \"name_pattern\": prior.get(\"name_pattern\"),\n",
        "                \"parent_hint\": prior.get(\"parent_hint\"),\n",
        "                \"age_hint\": prior.get(\"age_hint\"),\n",
        "            },\n",
        "            \"combined\": {\n",
        "                \"final_label\": combined.get(\"final_label\", \"Ï§ëÎ¶Ω\"),\n",
        "                \"total_score\": combined.get(\"total_score\", 0),\n",
        "                \"risk_pct\": combined.get(\"risk_pct\", 0.0),\n",
        "            }\n",
        "        }\n",
        "        user_prompt = \"Îã§Ïùå JSONÏùÑ ÏöîÏïΩÌï¥ Ï¥ùÌèâÏùÑ ÏûëÏÑ±:\\n\\n\" + json.dumps(payload, ensure_ascii=False, indent=2)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
        "                      {\"role\":\"user\",\"content\":user_prompt}],\n",
        "            max_tokens=120,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        msg = (resp.choices[0].message.content or \"\").strip()\n",
        "        return msg or _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "    except Exception:\n",
        "        return _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "\n",
        "# --- Í∏∞Ï°¥ Ïù¥Ïú†(GPT) ---\n",
        "def _gpt_reason(company: Dict[str,str],\n",
        "                fundamental: Dict[str,Any],\n",
        "                news: Dict[str,Any],\n",
        "                prior: Dict[str,Any],\n",
        "                combined: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    Î≥ÄÏàòÎ™Ö ÏùºÏπò: company, fundamental.flag, fundamental.reasons,\n",
        "                 news.news_count, news.neg_ratio, news.red_hits, news.label_news,\n",
        "                 prior.is_listed, prior.name_pattern, prior.parent_hint, prior.age_hint,\n",
        "                 combined.final_label, combined.total_score, combined.risk_pct\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not (OpenAI and api_key):\n",
        "        return _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                    news.get(\"red_hits\",0), combined.get(\"final_label\",\"Ï§ëÎ¶Ω\"),\n",
        "                                    news.get(\"news_count\",0))\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        sys_prompt = (\n",
        "            \"ÎÑàÎäî ÌïúÍµ≠ Í∏∞ÏóÖÏùò ÎÑêÎ¶¨Ïä§Ìä∏Îã§. \"\n",
        "            \"Îã®Ï†ï ÎåÄÏã† Í∑ºÍ±∞ Í∏∞Î∞òÏúºÎ°ú ÏöîÏïΩÌïòÎêò, ÏµúÎåÄ 2Î¨∏Ïû•(Í∞Å 40Ïûê ÎÇ¥Ïô∏)ÏúºÎ°ú Í∞ÑÍ≤∞Ìûà Ïì∞Í≥†, \"\n",
        "            \"ÌïÑÏöîÏãú ‚ÄòÍ¥ÄÎßù‚Äô Í∂åÍ≥† ÌÜ§ÏùÑ ÏÇ¨Ïö©ÌïòÎùº.\"\n",
        "        )\n",
        "        user_payload = {\n",
        "            \"company\": company,\n",
        "            \"fundamental\": {\n",
        "                \"flag\": fundamental.get(\"flag\", 0),\n",
        "                \"reasons\": fundamental.get(\"reasons\", []),\n",
        "            },\n",
        "            \"news\": {\n",
        "                \"news_count\": news.get(\"news_count\", 0),\n",
        "                \"neg_ratio\": round(news.get(\"neg_ratio\", 0.0), 4),\n",
        "                \"label_news\": news.get(\"label_news\", \"Ï§ëÎ¶Ω\"),\n",
        "                \"red_hits\": news.get(\"red_hits\", 0),\n",
        "            },\n",
        "            \"prior\": {\n",
        "                \"is_listed\": prior.get(\"is_listed\"),\n",
        "                \"name_pattern\": prior.get(\"name_pattern\"),\n",
        "                \"parent_hint\": prior.get(\"parent_hint\"),\n",
        "                \"age_hint\": prior.get(\"age_hint\"),\n",
        "            },\n",
        "            \"combined\": {\n",
        "                \"final_label\": combined.get(\"final_label\", \"Ï§ëÎ¶Ω\"),\n",
        "                \"total_score\": combined.get(\"total_score\", 0),\n",
        "                \"risk_pct\": combined.get(\"risk_pct\", 0.0),\n",
        "            }\n",
        "        }\n",
        "        user_prompt = (\n",
        "            \"ÏïÑÎûò JSONÏùÑ Ï¢ÖÌï©Ìï¥ ÏµúÏ¢Ö ÎùºÎ≤®Ïùò Ïù¥Ïú†Î•º ÏöîÏïΩÌï¥Ï§ò. \"\n",
        "            \"ÌïµÏã¨ Í∑ºÍ±∞ 1~2Í∞úÎßå Ïñ∏Í∏âÌïòÍ≥† Í≥ºÏû• ÌëúÌòÑÏùÄ ÌîºÌïòÎùº.\\n\\n\"\n",
        "            + json.dumps(user_payload, ensure_ascii=False, indent=2)\n",
        "        )\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
        "                      {\"role\":\"user\",\"content\":user_prompt}],\n",
        "            max_tokens=120,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        msg = (resp.choices[0].message.content or \"\").strip()\n",
        "        return msg or _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                           news.get(\"red_hits\",0), combined.get(\"final_label\",\"Ï§ëÎ¶Ω\"),\n",
        "                                           news.get(\"news_count\",0))\n",
        "    except Exception:\n",
        "        return _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                    news.get(\"red_hits\",0), combined.get(\"final_label\",\"Ï§ëÎ¶Ω\"),\n",
        "                                    news.get(\"news_count\",0))\n",
        "\n",
        "# -----------------------------\n",
        "# 7) ÏÑúÎπÑÏä§ ÏóîÌä∏Î¶¨Ìè¨Ïù∏Ìä∏Îì§\n",
        "# -----------------------------\n",
        "def run_once(keyword: str, choice_idx: Optional[int] = None) -> dict:\n",
        "    \"\"\"\n",
        "    Î∞±ÏóîÎìúÏóêÏÑú Î∞îÎ°ú Ìò∏Ï∂ú Í∞ÄÎä•Ìïú Îã®Ïùº Ìï®Ïàò(ÌõÑÎ≥¥ ÏÑ†ÌÉùÎèÑ ÎÇ¥Î∂ÄÏóêÏÑú Ï≤òÎ¶¨).\n",
        "    main()ÏóêÏÑúÎäî Ïù¥ Ìï®ÏàòÎ•º ÏßÅÏ†ë Ïì∞ÏßÄ ÏïäÍ≥†, run_once_with_corpÎ•º ÏÇ¨Ïö©Ìï¥\n",
        "    ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ†ÌÉùÌïú ÌõÑÎ≥¥Î•º Ï†ïÌôïÌûà Î∞òÏòÅÌïòÎèÑÎ°ù ÌïúÎã§.\n",
        "    \"\"\"\n",
        "    corps = load_corp_list_from_dart(DART_API_KEY)\n",
        "    cand = find_corp_candidates(corps, keyword, limit=20)\n",
        "    if not cand:\n",
        "        return {\"matches\": [], \"message\": \"Ìï¥Îãπ ÌÇ§ÏõåÎìúÎ°ú Í∏∞ÏóÖÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\"}\n",
        "\n",
        "    corp = cand[choice_idx] if (choice_idx is not None and 0 <= choice_idx < len(cand)) else cand[0]\n",
        "    return run_once_with_corp(corp, cand=cand)\n",
        "\n",
        "def run_once_with_corp(corp: dict, cand: Optional[List[dict]] = None) -> dict:\n",
        "    \"\"\"\n",
        "    ÏÇ¨Ïö©ÏûêÍ∞Ä Í≥†Î•∏ 'corp' Í∞ùÏ≤¥Î•º Î∞îÎ°ú ÎÑ£Ïñ¥ Î∂ÑÏÑùÎßå ÏàòÌñâ.\n",
        "    \"\"\"\n",
        "    # Îâ¥Ïä§\n",
        "    items = fetch_news(corp[\"corp_name\"])\n",
        "    neg_ratio, label_news, red_hits, top_neg = _compute_news_metrics(items)\n",
        "    news = {\n",
        "        \"news_count\": len(items),\n",
        "        \"neg_ratio\": neg_ratio,\n",
        "        \"label_news\": label_news,\n",
        "        \"red_hits\": red_hits,\n",
        "    }\n",
        "\n",
        "    # ÌéÄÎçîÎ©òÌÑ∏\n",
        "    fundamental = score_fundamental(corp[\"corp_code\"])\n",
        "\n",
        "    # prior Ïª®ÌÖçÏä§Ìä∏\n",
        "    prior = _build_prior_context(corp, news_count=len(items))\n",
        "\n",
        "    # Ï¢ÖÌï© ÎùºÎ≤®\n",
        "    final_label, total_score = _combine_risk_label(fundamental.get(\"flag\",0), label_news, len(items))\n",
        "    # Ï†ïÍ∑úÌôî ÌçºÏÑºÌä∏\n",
        "    risk_pct = 0.0\n",
        "    try:\n",
        "        risk_pct = round((total_score / MAX_TOTAL_SCORE) * 100.0, 1) if MAX_TOTAL_SCORE > 0 else 0.0\n",
        "    except Exception:\n",
        "        risk_pct = 0.0\n",
        "    combined = {\"final_label\": final_label, \"total_score\": total_score, \"risk_pct\": risk_pct}\n",
        "\n",
        "    # Ïù¥Ïú†(GPT ÎòêÎäî Ìè¥Î∞±)\n",
        "    reason = _gpt_reason(\n",
        "        company={\"corp_name\": corp[\"corp_name\"], \"corp_code\": corp[\"corp_code\"], \"stock_code\": corp.get(\"stock_code\",\"\")},\n",
        "        fundamental=fundamental, news=news, prior=prior, combined=combined\n",
        "    )\n",
        "    # Ï¥ùÌèâ(GPT ÎòêÎäî Ìè¥Î∞±)\n",
        "    overall_comment = _gpt_overall_comment(\n",
        "        company={\"corp_name\": corp[\"corp_name\"], \"corp_code\": corp[\"corp_code\"], \"stock_code\": corp.get(\"stock_code\",\"\")},\n",
        "        fundamental=fundamental, news=news, prior=prior, combined=combined\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"company\": corp,\n",
        "        \"fundamental\": fundamental,\n",
        "        \"news\": news,\n",
        "        \"prior\": prior,\n",
        "        \"combined\": combined,\n",
        "        \"reason\": reason,\n",
        "        \"overall_comment\": overall_comment,\n",
        "        \"top_neg\": top_neg,\n",
        "        \"items\": items,\n",
        "        \"matches\": cand or [],\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# 8) CLI (ÏûÖÎ†•/ÌõÑÎ≥¥ÏÑ†ÌÉù/Ï∂úÎ†• Ï†ÑÎã¥)\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print(\"üîé Í≤ÄÏÉâÌï† Í∏∞ÏóÖÎ™Ö ÏùºÎ∂ÄÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî:\", flush=True)\n",
        "    try:\n",
        "        keyword = input(\"> \").strip()\n",
        "    except EOFError:\n",
        "        print(\"‚ùå ÌëúÏ§ÄÏûÖÎ†•Ïù¥ ÏóÜÏñ¥ ÎåÄÍ∏∞ Ï§ëÏù¥ÏóàÏùÑ Ïàò ÏûàÏäµÎãàÎã§. ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Ïã§ÌñâÌïòÍ±∞ÎÇò ÌååÏù¥ÌîÑÎùºÏù∏ ÏûÖÎ†•ÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not keyword:\n",
        "        print(\"‚ùå Í∏∞ÏóÖÎ™Ö ÌÇ§ÏõåÎìúÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # ÌõÑÎ≥¥ Ï°∞Ìöå\n",
        "    try:\n",
        "        corps = load_corp_list_from_dart(DART_API_KEY)\n",
        "        cand = find_corp_candidates(corps, keyword, limit=20)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå DART Í∏∞ÏóÖÎ™©Î°ù/ÌõÑÎ≥¥ Ï°∞Ìöå Ïã§Ìå®:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not cand:\n",
        "        print(\"‚ùå Ìï¥Îãπ ÌÇ§ÏõåÎìúÎ°ú Í∏∞ÏóÖÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    if len(cand) == 1:\n",
        "        choice_idx = 0\n",
        "        print(f\"‚úÖ ÏûêÎèô ÏÑ†ÌÉù: {summarize_company(cand[0])}\", flush=True)\n",
        "    else:\n",
        "        print(\"\\nÎã§Ïùå Ï§ë Í∏∞ÏóÖÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:\")\n",
        "        for i, c in enumerate(cand):\n",
        "            print(f\"[{i}] {summarize_company(c)}\")\n",
        "        while True:\n",
        "            try:\n",
        "                _in = input(\"Î≤àÌò∏ ÏûÖÎ†•: \").strip()\n",
        "                choice_idx = int(_in)\n",
        "                if 0 <= choice_idx < len(cand):\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"Ïú†Ìö®Ìïú Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\", flush=True)\n",
        "\n",
        "    corp = cand[choice_idx]\n",
        "\n",
        "    # ÏÑ†ÌÉùÌïú corpÎ°ú Î∞îÎ°ú Î∂ÑÏÑù ÏàòÌñâ\n",
        "    try:\n",
        "        result = run_once_with_corp(corp, cand=cand)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Ïã§Ìñâ Ïã§Ìå®:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not result.get(\"company\"):\n",
        "        print(result.get(\"message\",\"Í∏∞ÏóÖÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\"))\n",
        "        sys.exit(0)\n",
        "\n",
        "    # Í≤∞Í≥º Ï∂úÎ†•\n",
        "    corp = result[\"company\"]\n",
        "    news = result[\"news\"]\n",
        "    fundamental = result[\"fundamental\"]\n",
        "    combined = result[\"combined\"]\n",
        "\n",
        "    print(f\"\\nüì∞ ÏàòÏßëÎêú Îâ¥Ïä§: {news['news_count']}Í±¥ (ÏµúÍ∑º {DEFAULT_DAYS}Ïùº)\")\n",
        "    print(\"\\n===== Í≤∞Í≥º =====\")\n",
        "    print(\"Í∏∞ÏóÖ:\", summarize_company(corp))\n",
        "    print(f\"[Îâ¥Ïä§] Î∂ÄÏ†ïÎπÑÏú®(Ïä§Î¨¥Îî©): {news['neg_ratio']*100:.1f}% | Î†àÎìúÌÇ§ÏõåÎìú: {news['red_hits']}\")\n",
        "    print(f\"[ÌéÄÎçîÎ©òÌÑ∏] flag: {fundamental.get('flag',0)} | ÏÇ¨Ïú†: {', '.join(fundamental.get('reasons',[])) or 'ÏóÜÏùå'}\")\n",
        "    print(f\"[Ï¢ÖÌï©ÌåêÏ†ï] ÎùºÎ≤®: {combined['final_label']} | Ï¥ùÏ†ê: {combined['total_score']}/{MAX_TOTAL_SCORE} | ÏúÑÌóòÎèÑ(Ï†ïÍ∑úÌôî): {combined['risk_pct']:.1f}%\")\n",
        "    print(f\"Ïù¥Ïú†: {result['reason']}\")\n",
        "    print(f\"Ï¥ùÌèâ: {result['overall_comment']}\")\n",
        "\n",
        "    top_neg = result.get(\"top_neg\", [])\n",
        "    if top_neg:\n",
        "        print(\"\\n‚ö†Ô∏è Î∂ÄÏ†ï ÌôïÎ•† ÎÜíÏùÄ Í∏∞ÏÇ¨ Top 3\")\n",
        "        for p, it in top_neg:\n",
        "            dt = it.get(\"date\",\"\")[:19].replace(\"T\",\" \")\n",
        "            print(f\"- {p*100:5.1f}% | {dt} | {it.get('title','').strip()}\")\n",
        "\n",
        "    print(\"\\n(Î©îÎ™®) NAVER Ïã§Ìå® Ïãú Google RSS Î∞±ÏóÖ, FinBERT ÏßÄÏó∞/Ïã§Ìå® Ïãú Î∞±ÏóÖÎ£∞ Ï¶âÏãú ÏÇ¨Ïö©.\")\n",
        "    if NO_FINBERT:\n",
        "        print(\"(Î©îÎ™®) NO_FINBERT=1 ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú FinBERT ÎπÑÌôúÏÑ±Ìôî ÏÉÅÌÉúÏûÖÎãàÎã§.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwHK_jNBaSD",
        "outputId": "1ab59cc7-2f38-41ae-9eb6-bfab81cd4ab6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Í≤ÄÏÉâÌï† Í∏∞ÏóÖÎ™Ö ÏùºÎ∂ÄÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî:\n",
            "> Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏\n",
            "‚è≥ DART Í∏∞ÏóÖÎ™©Î°ù Î°úÎî© Ï§ë...\n",
            "‚úÖ Í∏∞ÏóÖÎ™©Î°ù 113,057Í±¥ Î°úÎìú ÏôÑÎ£å\n",
            "üîé ÌõÑÎ≥¥ Í≤ÄÏÉâ: 'Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏'\n",
            "‚úÖ ÌõÑÎ≥¥ 8Í±¥\n",
            "\n",
            "Îã§Ïùå Ï§ë Í∏∞ÏóÖÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:\n",
            "[0] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏Í≤åÏûÑÏ¶à (ÎπÑÏÉÅÏû•, corp_code 00934275)\n",
            "[1] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏Ïä§ÌÜ†Î∏å (ÎπÑÏÉÅÏû•, corp_code 01205107)\n",
            "[2] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÏïåÌîºÏßÄ (ÎπÑÏÉÅÏû•, corp_code 00961756)\n",
            "[3] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÌôÄÎî©Ïä§ (ÎπÑÏÉÅÏû•, corp_code 00868194)\n",
            "[4] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏Î©îÍ∞ÄÌè¨Ìä∏ (ÎπÑÏÉÅÏû•, corp_code 01015911)\n",
            "[5] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÏûêÏÇ∞Ïö¥Ïö© (ÎπÑÏÉÅÏû•, corp_code 01314104)\n",
            "[6] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏Ïù∏Î≤†Ïä§Ìä∏Î®ºÌä∏ (ÎπÑÏÉÅÏû•, corp_code 00432272)\n",
            "[7] Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÏóîÌÑ∞ÌÖåÏù∏Î®ºÌä∏ (ÎπÑÏÉÅÏû•, corp_code 00809049)\n",
            "Î≤àÌò∏ ÏûÖÎ†•: 3\n",
            "‚è≥ NAVER Îâ¥Ïä§ ÏàòÏßë: 'Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÌôÄÎî©Ïä§'\n",
            "‚úÖ NAVER 40Í±¥\n",
            "‚è≥ FinBERT Î°úÎî© ÏãúÎèÑ(ÏµúÎåÄ 10s)...\n",
            "‚úÖ FinBERT Î°úÎî© ÏôÑÎ£å\n",
            "\n",
            "üì∞ ÏàòÏßëÎêú Îâ¥Ïä§: 40Í±¥ (ÏµúÍ∑º 50Ïùº)\n",
            "\n",
            "===== Í≤∞Í≥º =====\n",
            "Í∏∞ÏóÖ: Ïä§ÎßàÏùºÍ≤åÏù¥Ìä∏ÌôÄÎî©Ïä§ (ÎπÑÏÉÅÏû•, corp_code 00868194)\n",
            "[Îâ¥Ïä§] Î∂ÄÏ†ïÎπÑÏú®(Ïä§Î¨¥Îî©): 2.3% | Î†àÎìúÌÇ§ÏõåÎìú: 0\n",
            "[ÌéÄÎçîÎ©òÌÑ∏] flag: 0 | ÏÇ¨Ïú†: ÏóÜÏùå\n",
            "[Ï¢ÖÌï©ÌåêÏ†ï] ÎùºÎ≤®: Í∏çÏ†ï | Ï¥ùÏ†ê: 0/8 | ÏúÑÌóòÎèÑ(Ï†ïÍ∑úÌôî): 0.0%\n",
            "Ïù¥Ïú†: ÏµúÍ∑º Î≥¥ÎèÑÏóêÏÑú Î∂ÄÏ†ï Ïã†Ìò∏Í∞Ä ÎìúÎ¨ºÏñ¥(2.3%¬∑Î†àÎìúÌÇ§ÏõåÎìú 0Í±¥) Ï†ÑÎ∞òÏ†ÅÏúºÎ°ú Í∏çÏ†ïÏûÖÎãàÎã§.\n",
            "Ï¥ùÌèâ: ÏúÑÌóòÎèÑ 0.0%(ÎÇÆÏùå)Î°ú Í∏çÏ†ï ÌåêÎã®. Í∏∞Î≥∏ Ïú†ÏßÄ, Ïù¥Ïäà Î∞úÏÉù Í∞êÏãú.\n",
            "\n",
            "(Î©îÎ™®) NAVER Ïã§Ìå® Ïãú Google RSS Î∞±ÏóÖ, FinBERT ÏßÄÏó∞/Ïã§Ìå® Ïãú Î∞±ÏóÖÎ£∞ Ï¶âÏãú ÏÇ¨Ïö©.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cG-mE2tzBZF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
