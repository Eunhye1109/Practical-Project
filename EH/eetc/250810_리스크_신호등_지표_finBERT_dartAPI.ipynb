{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR-wP_Jrcg1r",
        "outputId": "25db0f59-fc43-49bd-ac72-19fff5ba4ce1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/risk-light-model-v2.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 250810 0.3ver(ìµœì¢…)"
      ],
      "metadata": {
        "id": "HIw5aiWqCN37"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ejZlGyIPH_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, io, sys, json, zipfile, time, hashlib, threading, queue\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import xml.etree.ElementTree as ET\n",
        "import argparse\n",
        "\n",
        "# OpenAI (í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ë¡œ ì£¼ì…)\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    OpenAI = None  # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ í™˜ê²½ ëŒ€ë¹„\n",
        "\n",
        "# -----------------------------\n",
        "# ì„¤ì •\n",
        "# -----------------------------\n",
        "KST = timezone(timedelta(hours=9))\n",
        "HTTP_TIMEOUT = 15\n",
        "DEFAULT_DAYS = 50\n",
        "DEFAULT_MAX_ITEMS = 40\n",
        "USER_AGENT = \"CorpSentimentCLI/1.0 (+https://example.local)\"\n",
        "\n",
        "DART_API_KEY = os.getenv(\"DART_API_KEY\", \"\")\n",
        "NAVER_ID = os.getenv(\"NAVER_CLIENT_ID\", \"\")\n",
        "NAVER_SECRET = os.getenv(\"NAVER_CLIENT_SECRET\", \"\")\n",
        "NO_FINBERT = os.getenv(\"NO_FINBERT\", \"0\") == \"1\"  # í™˜ê²½ë³€ìˆ˜ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥\n",
        "FINBERT_MAX_LOAD_SEC = int(os.getenv(\"FINBERT_MAX_LOAD_SEC\", \"10\"))\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fQBlUfH3Bc9h"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ê³µí†µ ìœ í‹¸\n",
        "# -----------------------------\n",
        "def _sha1(text: str) -> str:\n",
        "    import hashlib\n",
        "    return hashlib.sha1((text or \"\").encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def to_kst(dt: datetime) -> datetime:\n",
        "    return (dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt).astimezone(KST)\n",
        "\n",
        "def parse_rfc2822_date(s: str) -> Optional[datetime]:\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        return parsedate_to_datetime(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def clip_days(dt: datetime, days: int = DEFAULT_DAYS) -> bool:\n",
        "    return (to_kst(datetime.utcnow()) - to_kst(dt)) <= timedelta(days=days)\n",
        "\n",
        "def _is_spc_like(name: str) -> bool:\n",
        "    low = (name or \"\").lower()\n",
        "    return any(p.lower() in low for p in SPC_PATTERNS)\n",
        "\n",
        "def _reason_from_metrics(corp_name: str, neg_ratio: float, red_hits: int, label: str, news_count: int) -> str:\n",
        "    \"\"\"OPENAI_API_KEY ì—†ê±°ë‚˜ GPT ì‹¤íŒ¨ ì‹œ í´ë°± í•œ ì¤„ ì„¤ëª….\"\"\"\n",
        "    if news_count == 0:\n",
        "        return f\"ìµœê·¼ ê¸°ì‚¬ ì‹ í˜¸ê°€ ì—†ì–´ ë¶ˆí™•ì‹¤ì„±ì´ ì»¤ {label} íŒë‹¨ì´ë©°, ì¶”ì´ë¥¼ ê´€ë§í•˜ëŠ” í¸ì´ ì•ˆì „í•©ë‹ˆë‹¤.\"\n",
        "    if label == \"ê¸ì •\":\n",
        "        return f\"ìµœê·¼ ë³´ë„ì—ì„œ ë¶€ì • ì‹ í˜¸ê°€ ë“œë¬¼ì–´({neg_ratio*100:.1f}%Â·ë ˆë“œí‚¤ì›Œë“œ {red_hits}ê±´) ì „ë°˜ì ìœ¼ë¡œ {label}ì…ë‹ˆë‹¤.\"\n",
        "    if label == \"ì¤‘ë¦½\":\n",
        "        return f\"ë³´ë„ ë‚´ ê¸Â·ë¶€ì • ì‹ í˜¸ê°€ í˜¼ì¬í•´({neg_ratio*100:.1f}%Â·ë ˆë“œí‚¤ì›Œë“œ {red_hits}ê±´) {label}ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.\"\n",
        "    return f\"ë¶€ì • ì‹ í˜¸ ë¹„ì¤‘ì´ ë†’ì•„({neg_ratio*100:.1f}%Â·ë ˆë“œí‚¤ì›Œë“œ {red_hits}ê±´) {label}ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "# -----------------------------\n",
        "# 1) DART corpCode ë¡œë”©/ê²€ìƒ‰\n",
        "# -----------------------------\n",
        "def load_corp_list_from_dart(DART_API_KEY: str) -> List[Dict[str, str]]:\n",
        "    if not DART_API_KEY:\n",
        "        raise RuntimeError(\"âŒ DART_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "    print(\"â³ DART ê¸°ì—…ëª©ë¡ ë¡œë”© ì¤‘...\", flush=True)\n",
        "    api_key = DART_API_KEY  # ì˜¤íƒ€/ìŠ¤ì½”í”„ ì•ˆì „ ë³´ì •\n",
        "    url = f\"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}\"\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    res = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)\n",
        "    res.raise_for_status()\n",
        "    zf = zipfile.ZipFile(io.BytesIO(res.content))\n",
        "    with zf.open(\"CORPCODE.xml\") as f:\n",
        "        tree = ET.parse(f)\n",
        "    root = tree.getroot()\n",
        "    out = []\n",
        "    for el in root.findall(\"list\"):\n",
        "        out.append({\n",
        "            \"corp_code\": (el.findtext(\"corp_code\") or \"\").strip(),\n",
        "            \"corp_name\": (el.findtext(\"corp_name\") or \"\").strip(),\n",
        "            \"stock_code\": (el.findtext(\"stock_code\") or \"\").strip(),\n",
        "            \"modify_date\": (el.findtext(\"modify_date\") or \"\").strip(),\n",
        "        })\n",
        "    print(f\"âœ… ê¸°ì—…ëª©ë¡ {len(out):,}ê±´ ë¡œë“œ ì™„ë£Œ\", flush=True)\n",
        "    return out\n",
        "\n",
        "def find_corp_candidates(corps: List[Dict[str, str]], keyword: str, limit: int = 20) -> List[Dict[str, str]]:\n",
        "    kw = keyword.strip().lower()\n",
        "    print(f\"ğŸ” í›„ë³´ ê²€ìƒ‰: '{keyword}'\", flush=True)\n",
        "    cand = [c for c in corps if kw in c[\"corp_name\"].lower()]\n",
        "    def score(c):\n",
        "        name = c[\"corp_name\"]; sc = 0\n",
        "        if name == keyword: sc += 100\n",
        "        if name.startswith(keyword): sc += 50\n",
        "        if c.get(\"stock_code\"): sc += 10\n",
        "        sc += max(0, 20 - abs(len(name) - len(keyword)))\n",
        "        return -sc\n",
        "    cand.sort(key=score)\n",
        "    print(f\"âœ… í›„ë³´ {len(cand)}ê±´\", flush=True)\n",
        "    return cand[:limit]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) ë‰´ìŠ¤ ìˆ˜ì§‘\n",
        "# -----------------------------\n",
        "def fetch_news_naver(query: str, display: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    if not (NAVER_ID and NAVER_SECRET):\n",
        "        raise RuntimeError(\"NAVER í‚¤ ì—†ìŒ\")\n",
        "    print(f\"â³ NAVER ë‰´ìŠ¤ ìˆ˜ì§‘: '{query}'\", flush=True)\n",
        "    headers = {\n",
        "        \"X-Naver-Client-Id\": NAVER_ID,\n",
        "        \"X-Naver-Client-Secret\": NAVER_SECRET,\n",
        "        \"User-Agent\": USER_AGENT\n",
        "    }\n",
        "    params = {\"query\": query, \"display\": min(100, display), \"start\": 1, \"sort\": \"date\"}\n",
        "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
        "    r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    items = []\n",
        "    for it in data.get(\"items\", []):\n",
        "        dt = parse_rfc2822_date(it.get(\"pubDate\",\"\")) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt, DEFAULT_DAYS):\n",
        "            continue\n",
        "        title = (it.get(\"title\") or \"\").replace(\"<b>\",\"\").replace(\"</b>\",\"\")\n",
        "        desc  = (it.get(\"description\") or \"\").replace(\"<b>\",\"\").replace(\"</b>\",\"\")\n",
        "        link  = it.get(\"link\") or it.get(\"originallink\") or \"\"\n",
        "        items.append({\"title\": title, \"description\": desc, \"link\": link, \"date\": dt.isoformat()})\n",
        "        if len(items) >= DEFAULT_MAX_ITEMS:\n",
        "            break\n",
        "    print(f\"âœ… NAVER {len(items)}ê±´\", flush=True)\n",
        "    return items\n",
        "\n",
        "def fetch_news_google_rss(query: str, max_items: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    print(f\"â³ Google RSS ìˆ˜ì§‘(ë°±ì—…): '{query}'\", flush=True)\n",
        "    q = requests.utils.quote(query)\n",
        "    url = f\"https://news.google.com/rss/search?q={q}+when:{DEFAULT_DAYS}d&hl=ko&gl=KR&ceid=KR:ko\"\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    r = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    root = ET.fromstring(r.text)\n",
        "    ch = root.find(\"channel\")\n",
        "    out = []\n",
        "    if ch is None: return out\n",
        "    for item in ch.findall(\"item\"):\n",
        "        title = item.findtext(\"title\") or \"\"\n",
        "        desc  = item.findtext(\"description\") or \"\"\n",
        "        link  = item.findtext(\"link\") or \"\"\n",
        "        pub   = item.findtext(\"{http://purl.org/dc/elements/1.1/}date\") or item.findtext(\"pubDate\") or \"\"\n",
        "        dt    = parse_rfc2822_date(pub) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt, DEFAULT_DAYS):\n",
        "            continue\n",
        "        out.append({\"title\": title, \"description\": desc, \"link\": link, \"date\": dt.isoformat()})\n",
        "        if len(out) >= max_items:\n",
        "            break\n",
        "    print(f\"âœ… Google RSS {len(out)}ê±´\", flush=True)\n",
        "    return out\n",
        "\n",
        "def fetch_news(query: str) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        items = fetch_news_naver(query, display=DEFAULT_MAX_ITEMS)\n",
        "        if items:\n",
        "            return items\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ NAVER ì‹¤íŒ¨: {e}\", flush=True)\n",
        "    try:\n",
        "        return fetch_news_google_rss(query, max_items=DEFAULT_MAX_ITEMS)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Google RSS ì‹¤íŒ¨: {e}\", flush=True)\n",
        "        return []\n",
        "\n",
        "# ---------------------------\n",
        "# 3) í€ë”ë©˜í„¸ ë¦¬ìŠ¤í¬ ì ìˆ˜ (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€)\n",
        "# ---------------------------\n",
        "def score_fundamental(corp_code: str, prefer_year: int | None = None) -> dict:\n",
        "    import requests\n",
        "    from datetime import datetime\n",
        "    BASE = \"https://opendart.fss.or.kr\"\n",
        "    REPRT_PRIORITY = [\"11014\", \"11012\", \"11013\"]  # 3Q > 2Q > 1Q\n",
        "    API_KEY = os.getenv(\"DART_API_KEY\", \"\")\n",
        "    if not API_KEY:\n",
        "        return {\"year\": prefer_year or datetime.now().year, \"reprt_code\": \"11011\", \"flag\": 0, \"reasons\": []}\n",
        "\n",
        "    session = requests.Session()\n",
        "    def _j(path, **kw):\n",
        "        p = {\"crtfc_key\": API_KEY}; p.update(kw)\n",
        "        try:\n",
        "            r = session.get(f\"{BASE}{path}\", params=p, timeout=HTTP_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            if str(data.get(\"status\", \"000\")) != \"000\":\n",
        "                return []\n",
        "            return data.get(\"list\", []) or []\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    y0 = prefer_year or datetime.now().year\n",
        "    chosen = None\n",
        "    for y in [y0, y0 - 1]:\n",
        "        for rc in REPRT_PRIORITY:\n",
        "            if any([\n",
        "                _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/mrhlSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/hyslrChgSttus.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "                _j(\"/api/cprndNrdmpBlce.json\", corp_code=corp_code, bsns_year=str(y), reprt_code=rc),\n",
        "            ]):\n",
        "                chosen = (y, rc); break\n",
        "        if chosen: break\n",
        "    if not chosen:\n",
        "        return {\"year\": y0, \"reprt_code\": \"11011\", \"flag\": 0, \"reasons\": []}\n",
        "    year, rc = chosen\n",
        "\n",
        "    flag = 0; reasons = []\n",
        "\n",
        "    irds = _j(\"/api/irdsSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    if irds: flag += 1; reasons.append(\"ê°ì ê³µì‹œ\")\n",
        "\n",
        "    hyslr = _j(\"/api/hyslrChgSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    if hyslr: flag += 1; reasons.append(\"ìµœëŒ€ì£¼ì£¼ ë³€ë™\")\n",
        "\n",
        "    mr = _j(\"/api/mrhlSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    hold = None\n",
        "    for r in mr:\n",
        "        if (r.get(\"se\") or \"\").strip() == \"ì†Œì•¡ì£¼ì£¼\":\n",
        "            try:\n",
        "                s = str(r.get(\"hold_stock_rate\")).replace(\",\",\"\").strip()\n",
        "                hold = float(s) if s not in (\"\",\"-\") else None\n",
        "            except Exception:\n",
        "                hold = None\n",
        "    if hold is not None and hold >= 70:\n",
        "        flag += 1; reasons.append(f\"ì†Œì•¡ì£¼ì£¼ {hold:.1f}%\")\n",
        "\n",
        "    cur = _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    prv = _j(\"/api/empSttus.json\", corp_code=corp_code, bsns_year=str(year - 1), reprt_code=rc)\n",
        "    def _emp_total(rows):\n",
        "        if not rows: return None\n",
        "        s=0; seen=False\n",
        "        for r in rows:\n",
        "            try:\n",
        "                v = float(str(r.get(\"sm\")).replace(\",\",\"\").strip())\n",
        "                s += v; seen=True\n",
        "            except Exception:\n",
        "                pass\n",
        "        return s if seen else None\n",
        "    tcur, tprv = _emp_total(cur), _emp_total(prv)\n",
        "    if tcur and tprv and tprv > 0:\n",
        "        yoy = (tcur - tprv) / tprv * 100\n",
        "        if yoy <= -10:\n",
        "            flag += 1; reasons.append(f\"ì§ì›ìˆ˜ YoY {yoy:.1f}%\")\n",
        "\n",
        "    bd = _j(\"/api/cprndNrdmpBlce.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    def _tof2(x):\n",
        "        try: s=str(x).replace(\",\",\"\").strip(); return float(s) if s not in (\"\",\"-\") else None\n",
        "        except: return None\n",
        "    if bd:\n",
        "        sm = _tof2(bd[0].get(\"sm\")); s1 = _tof2(bd[0].get(\"yy1_below\"))\n",
        "        if sm and s1 and sm > 0 and s1/sm > 0.5:\n",
        "            flag += 1; reasons.append(f\"1ë…„ë‚´ ìƒí™˜ {s1/sm*100:.1f}%\")\n",
        "\n",
        "    ot = _j(\"/api/otrCprInvstmntSttus.json\", corp_code=corp_code, bsns_year=str(year), reprt_code=rc)\n",
        "    def _tof3(x):\n",
        "        try: s=str(x).replace(\",\",\"\").strip(); return float(s) if s not in (\"\",\"-\") else None\n",
        "        except: return None\n",
        "    if any(((_tof3(r.get(\"recent_bsns_year_fnnr_sttus_thstrm_ntpf\")) or 0) < 0) for r in ot):\n",
        "        flag += 1; reasons.append(\"ì ì í”¼íˆ¬ìë²•ì¸ ì¡´ì¬\")\n",
        "\n",
        "    return {\"year\": year, \"reprt_code\": rc, \"flag\": flag, \"reasons\": reasons}\n",
        "\n",
        "# -----------------------------\n",
        "# 4) FinBERT (íƒ€ì„ì•„ì›ƒ/ì˜µì…˜) + ë°±ì—…ë£°\n",
        "# -----------------------------\n",
        "_FINBERT = {\"tok\": None, \"mdl\": None, \"device\": \"cpu\"}\n",
        "_SENT_CACHE: Dict[str, float] = {}\n",
        "\n",
        "def _load_finbert_worker(out_q: \"queue.Queue\"):\n",
        "    try:\n",
        "        import torch\n",
        "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "        name = \"yiyanghkust/finbert-tone\"\n",
        "        tok = AutoTokenizer.from_pretrained(name)\n",
        "        mdl = AutoModelForSequenceClassification.from_pretrained(name)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        mdl = mdl.to(device)\n",
        "        out_q.put((tok, mdl, device, None))\n",
        "    except Exception as e:\n",
        "        out_q.put((None, None, None, e))\n",
        "\n",
        "def _ensure_finbert_loaded(timeout_sec: int = FINBERT_MAX_LOAD_SEC) -> bool:\n",
        "    if _FINBERT[\"tok\"] is not None: return True\n",
        "    if NO_FINBERT: return False\n",
        "    print(f\"â³ FinBERT ë¡œë”© ì‹œë„(ìµœëŒ€ {timeout_sec}s)...\", flush=True)\n",
        "    out_q: \"queue.Queue\" = queue.Queue(maxsize=1)\n",
        "    t = threading.Thread(target=_load_finbert_worker, args=(out_q,), daemon=True)\n",
        "    t.start()\n",
        "    try:\n",
        "        tok, mdl, device, err = out_q.get(timeout=timeout_sec)\n",
        "    except queue.Empty:\n",
        "        print(\"âš ï¸ FinBERT ë¡œë”© íƒ€ì„ì•„ì›ƒ â†’ ë°±ì—…ë£°ë¡œ ì „í™˜\", flush=True)\n",
        "        return False\n",
        "    if err is not None or tok is None or mdl is None:\n",
        "        print(f\"âš ï¸ FinBERT ë¡œë”© ì‹¤íŒ¨ â†’ ë°±ì—…ë£°ë¡œ ì „í™˜ ({err})\", flush=True)\n",
        "        return False\n",
        "    _FINBERT[\"tok\"], _FINBERT[\"mdl\"], _FINBERT[\"device\"] = tok, mdl, device\n",
        "    print(\"âœ… FinBERT ë¡œë”© ì™„ë£Œ\", flush=True)\n",
        "    return True\n",
        "\n",
        "def analyze_sentiment(texts: List[str]) -> List[float]:\n",
        "    if not texts:\n",
        "        return []\n",
        "    keys = [_sha1(t) for t in texts]\n",
        "    out: List[Optional[float]] = [None] * len(texts)\n",
        "    pending = []\n",
        "    for i, (k, t) in enumerate(zip(keys, texts)):\n",
        "        if k in _SENT_CACHE:\n",
        "            out[i] = _SENT_CACHE[k]\n",
        "        else:\n",
        "            pending.append(i)\n",
        "\n",
        "    use_model = _ensure_finbert_loaded()\n",
        "    if use_model and pending:\n",
        "        import torch\n",
        "        tok, mdl, device = _FINBERT[\"tok\"], _FINBERT[\"mdl\"], _FINBERT[\"device\"]\n",
        "        bs = 16\n",
        "        for s in range(0, len(pending), bs):\n",
        "            idxs = pending[s:s+bs]\n",
        "            batch = [(texts[i] or \"\")[:1024] for i in idxs]\n",
        "            inputs = tok(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = mdl(**inputs).logits\n",
        "            probs = torch.softmax(logits, dim=1)[:,2].detach().cpu().numpy().tolist()\n",
        "            for i, p in zip(idxs, probs):\n",
        "                out[i] = float(p); _SENT_CACHE[keys[i]] = out[i]\n",
        "\n",
        "    # ë°±ì—…ë£°\n",
        "    for i in range(len(out)):\n",
        "        if out[i] is None:\n",
        "            low = (texts[i] or \"\").lower()\n",
        "            hits = sum(1 for w in BACKUP_NEG_LEXICON if w in low)\n",
        "            out[i] = min(1.0, hits/3.0)\n",
        "            _SENT_CACHE[keys[i]] = out[i]\n",
        "\n",
        "    return [float(x or 0.0) for x in out]\n",
        "\n",
        "# -----------------------------\n",
        "# 5) ë² ì´ì§€ì•ˆ ìŠ¤ë¬´ë”© ë° ë¼ë²¨\n",
        "# -----------------------------\n",
        "def smoothed_neg_ratio(neg_probs: List[float], thr: float=0.5, alpha: float=1.0, beta: float=3.0) -> float:\n",
        "    if not neg_probs:\n",
        "        return alpha / (alpha + beta)\n",
        "    n = len(neg_probs)\n",
        "    hits = sum(1 for p in neg_probs if p >= thr)\n",
        "    return (hits + alpha) / (n + alpha + beta)\n",
        "\n",
        "def judge_sentiment_label(neg_ratio: float) -> str:\n",
        "    if neg_ratio >= 0.60: return \"ë¶€ì •\"\n",
        "    if neg_ratio >= 0.30: return \"ì¤‘ë¦½\"\n",
        "    return \"ê¸ì •\"\n",
        "\n",
        "def summarize_company(c: Dict[str, str]) -> str:\n",
        "    listed = \"ìƒì¥\" if c.get(\"stock_code\") else \"ë¹„ìƒì¥\"\n",
        "    sx = f\"{c['corp_name']} ({listed}\"\n",
        "    if c.get(\"stock_code\"):\n",
        "        sx += f\", ì¢…ëª©ì½”ë“œ {c['stock_code']}\"\n",
        "    sx += f\", corp_code {c['corp_code']})\"\n",
        "    return sx\n",
        "\n",
        "# -----------------------------\n",
        "# 6) ì¢…í•© í‰ê°€(ë‰´ìŠ¤+í€ë”ë©˜í„¸+ê°œí™©) í—¬í¼\n",
        "# -----------------------------\n",
        "def _build_prior_context(corp: Dict[str,str], news_count: int) -> dict:\n",
        "    name = corp.get(\"corp_name\",\"\")\n",
        "    return {\n",
        "        \"corp_name\": name,\n",
        "        \"corp_code\": corp.get(\"corp_code\",\"\"),\n",
        "        \"is_listed\": \"ìƒì¥\" if corp.get(\"stock_code\") else \"ë¹„ìƒì¥\",\n",
        "        \"name_pattern\": \"SPC ì˜ì‹¬\" if _is_spc_like(name) else \"ì¼ë°˜ë²•ì¸\",\n",
        "        \"parent_hint\": \"ì¿ íŒ¡ ê´€ë ¨ ì‚¬ëª…\" if \"ì¿ íŒ¡\" in name else \"ì •ë³´ì—†ìŒ\",\n",
        "        \"age_hint\": \"ì •ë³´ì—†ìŒ\",  # (ì¶”í›„ ì—°í˜/ì„¤ë¦½ì—°ë„ ë¶™ì´ë©´ ì±„ì›€)\n",
        "        \"news_count\": news_count,\n",
        "    }\n",
        "\n",
        "def _compute_news_metrics(items: List[Dict[str,Any]]) -> Tuple[float, str, int, List[Tuple[float,Dict[str,Any]]]]:\n",
        "    texts = [f\"{it.get('title','')} {it.get('description','')}\".strip() for it in items]\n",
        "    neg_probs = analyze_sentiment(texts) if texts else []\n",
        "    neg_ratio = smoothed_neg_ratio(neg_probs, thr=0.5, alpha=1.0, beta=3.0)\n",
        "    label_news = judge_sentiment_label(neg_ratio)\n",
        "    red_hits = 0\n",
        "    for t in texts:\n",
        "        low = (t or \"\").lower()\n",
        "        red_hits += sum(1 for w in RED_KEYWORDS if w.lower() in low)\n",
        "    scored = sorted([(p, it) for it, p in zip(items, neg_probs)], key=lambda x: x[0], reverse=True) if items else []\n",
        "    top_neg = [s for s in scored[:3] if s[0] >= 0.5]\n",
        "    return neg_ratio, label_news, red_hits, top_neg\n",
        "\n",
        "def _combine_risk_label(fund_flag: int, label_news: str, news_count: int) -> Tuple[str,int]:\n",
        "    \"\"\"\n",
        "    ê°„ë‹¨ í•©ì‚° ê·œì¹™:\n",
        "      - ë‰´ìŠ¤ ë¼ë²¨ ì ìˆ˜: ê¸ì •0 / ì¤‘ë¦½1 / ë¶€ì •2\n",
        "      - total_score = fund_flag + news_score (+ ë‰´ìŠ¤ì—†ìŒì´ë©´ news_score=1ë¡œ ê´€ë§ ë°˜ì˜)\n",
        "      - ìµœì¢…ë¼ë²¨: total>=4 ë¶€ì •, total>=2 ì¤‘ë¦½, else ê¸ì •\n",
        "    \"\"\"\n",
        "    news_score = {\"ê¸ì •\":0, \"ì¤‘ë¦½\":1, \"ë¶€ì •\":2}.get(label_news, 1)\n",
        "    if news_count == 0:\n",
        "        news_score = 1  # ê´€ë§ í˜ë„í‹°\n",
        "    total = fund_flag + news_score\n",
        "    if total >= 4: return \"ë¶€ì •\", total\n",
        "    if total >= 2: return \"ì¤‘ë¦½\", total\n",
        "    return \"ê¸ì •\", total\n",
        "\n",
        "# --- ì´í‰: ê·œì¹™ & GPT ---\n",
        "def _overall_comment_fallback(combined: Dict[str,Any],\n",
        "                              news: Dict[str,Any],\n",
        "                              fundamental: Dict[str,Any],\n",
        "                              prior: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    ìœ„í—˜ë„ %ì™€ ì‹ í˜¸ì— ê¸°ë°˜í•œ ê·œì¹™í˜• ì´í‰. 1~2ë¬¸ì¥.\n",
        "    \"\"\"\n",
        "    pct = float(combined.get(\"risk_pct\", 0.0))\n",
        "    label = combined.get(\"final_label\", \"ì¤‘ë¦½\")\n",
        "    fund_flag = int(fundamental.get(\"flag\", 0) or 0)\n",
        "    red = int(news.get(\"red_hits\", 0) or 0)\n",
        "    ncnt = int(news.get(\"news_count\", 0) or 0)\n",
        "\n",
        "    # ë²„í‚·\n",
        "    if pct >= 75:\n",
        "        bucket = \"ë§¤ìš° ë†’ìŒ(ê²½ê³„)\"\n",
        "        advice = \"ë‹¨ê¸° ë…¸ì¶œ ì¶•ì†ŒÂ·ê´€ë§ ê¶Œê³ \"\n",
        "    elif pct >= 50:\n",
        "        bucket = \"ë†’ìŒ(ì£¼ì˜)\"\n",
        "        advice = \"ë³´ìˆ˜ì  ì ‘ê·¼ ë° ëª¨ë‹ˆí„°ë§ ê°•í™”\"\n",
        "    elif pct >= 25:\n",
        "        bucket = \"ë³´í†µ\"\n",
        "        advice = \"ì¤‘ë¦½ ìœ ì§€, ì´ë²¤íŠ¸ ì²´í¬\"\n",
        "    else:\n",
        "        bucket = \"ë‚®ìŒ\"\n",
        "        advice = \"ê¸°ë³¸ ìœ ì§€, ì´ìŠˆ ë°œìƒ ê°ì‹œ\"\n",
        "\n",
        "    # ë³´ì • íŒíŠ¸\n",
        "    hint = []\n",
        "    if ncnt == 0:\n",
        "        hint.append(\"ìµœê·¼ ë‰´ìŠ¤ ë¶€ì¬\")\n",
        "    if red > 0:\n",
        "        hint.append(f\"ë ˆë“œí‚¤ì›Œë“œ {red}ê±´\")\n",
        "    if fund_flag >= 2:\n",
        "        hint.append(f\"í€ë”ë©˜í„¸ í”Œë˜ê·¸ {fund_flag}ê±´\")\n",
        "\n",
        "    tail = f\"({', '.join(hint)})\" if hint else \"\"\n",
        "    return f\"ìœ„í—˜ë„ {pct:.1f}%({bucket})ë¡œ {label} íŒë‹¨. {advice}{(' ' + tail) if tail else ''}.\"\n",
        "\n",
        "def _gpt_overall_comment(company: Dict[str,str],\n",
        "                         fundamental: Dict[str,Any],\n",
        "                         news: Dict[str,Any],\n",
        "                         prior: Dict[str,Any],\n",
        "                         combined: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    JSON ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ GPT ì´í‰. ì‹¤íŒ¨ ì‹œ ê·œì¹™í˜• í´ë°±.\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(OPENAI_API_KEY)\n",
        "    if not (OpenAI and api_key):\n",
        "        return _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        sys_prompt = (\n",
        "            \"ë„ˆëŠ” í•œêµ­ ê¸°ì—… ë¦¬ìŠ¤í¬ë¥¼ ì¢…í•© ë¶„ì„ í›„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤. \"\n",
        "            \"ì…ë ¥ JSONì„ ë°”íƒ•ìœ¼ë¡œ 1~2ë¬¸ì¥, 40ì ì´ë‚´ë¡œ ì´í‰ì„ ì‘ì„±í•˜ë¼. \"\n",
        "            \"ê³¼ì¥ í‘œí˜„ì€ í”¼í•˜ê³ , ìœ„í—˜ë„%Â·ë¼ë²¨Â·í•µì‹¬ ê·¼ê±°(1ê°œ ë‚´ì™¸)Â·ê°„ë‹¨ ê¶Œê³ ë¥¼ í¬í•¨í•˜ë¼.\"\n",
        "        )\n",
        "        payload = {\n",
        "            \"company\": company,\n",
        "            \"fundamental\": {\n",
        "                \"flag\": fundamental.get(\"flag\", 0),\n",
        "                \"reasons\": fundamental.get(\"reasons\", []),\n",
        "            },\n",
        "            \"news\": {\n",
        "                \"news_count\": news.get(\"news_count\", 0),\n",
        "                \"neg_ratio\": round(news.get(\"neg_ratio\", 0.0), 4),\n",
        "                \"label_news\": news.get(\"label_news\", \"ì¤‘ë¦½\"),\n",
        "                \"red_hits\": news.get(\"red_hits\", 0),\n",
        "            },\n",
        "            \"prior\": {\n",
        "                \"is_listed\": prior.get(\"is_listed\"),\n",
        "                \"name_pattern\": prior.get(\"name_pattern\"),\n",
        "                \"parent_hint\": prior.get(\"parent_hint\"),\n",
        "                \"age_hint\": prior.get(\"age_hint\"),\n",
        "            },\n",
        "            \"combined\": {\n",
        "                \"final_label\": combined.get(\"final_label\", \"ì¤‘ë¦½\"),\n",
        "                \"total_score\": combined.get(\"total_score\", 0),\n",
        "                \"risk_pct\": combined.get(\"risk_pct\", 0.0),\n",
        "            }\n",
        "        }\n",
        "        user_prompt = \"ë‹¤ìŒ JSONì„ ìš”ì•½í•´ ì´í‰ì„ ì‘ì„±:\\n\\n\" + json.dumps(payload, ensure_ascii=False, indent=2)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
        "                      {\"role\":\"user\",\"content\":user_prompt}],\n",
        "            max_tokens=120,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        msg = (resp.choices[0].message.content or \"\").strip()\n",
        "        return msg or _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "    except Exception:\n",
        "        return _overall_comment_fallback(combined, news, fundamental, prior)\n",
        "\n",
        "# --- ê¸°ì¡´ ì´ìœ (GPT) ---\n",
        "def _gpt_reason(company: Dict[str,str],\n",
        "                fundamental: Dict[str,Any],\n",
        "                news: Dict[str,Any],\n",
        "                prior: Dict[str,Any],\n",
        "                combined: Dict[str,Any]) -> str:\n",
        "    \"\"\"\n",
        "    ë³€ìˆ˜ëª… ì¼ì¹˜: company, fundamental.flag, fundamental.reasons,\n",
        "                 news.news_count, news.neg_ratio, news.red_hits, news.label_news,\n",
        "                 prior.is_listed, prior.name_pattern, prior.parent_hint, prior.age_hint,\n",
        "                 combined.final_label, combined.total_score, combined.risk_pct\n",
        "    \"\"\"\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not (OpenAI and api_key):\n",
        "        return _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                    news.get(\"red_hits\",0), combined.get(\"final_label\",\"ì¤‘ë¦½\"),\n",
        "                                    news.get(\"news_count\",0))\n",
        "    try:\n",
        "        client = OpenAI(api_key=api_key)\n",
        "        sys_prompt = (\n",
        "            \"ë„ˆëŠ” í•œêµ­ ê¸°ì—…ì˜ ë„ë¦¬ìŠ¤íŠ¸ë‹¤. \"\n",
        "            \"ë‹¨ì • ëŒ€ì‹  ê·¼ê±° ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ë˜, ìµœëŒ€ 2ë¬¸ì¥(ê° 40ì ë‚´ì™¸)ìœ¼ë¡œ ê°„ê²°íˆ ì“°ê³ , \"\n",
        "            \"í•„ìš”ì‹œ â€˜ê´€ë§â€™ ê¶Œê³  í†¤ì„ ì‚¬ìš©í•˜ë¼.\"\n",
        "        )\n",
        "        user_payload = {\n",
        "            \"company\": company,\n",
        "            \"fundamental\": {\n",
        "                \"flag\": fundamental.get(\"flag\", 0),\n",
        "                \"reasons\": fundamental.get(\"reasons\", []),\n",
        "            },\n",
        "            \"news\": {\n",
        "                \"news_count\": news.get(\"news_count\", 0),\n",
        "                \"neg_ratio\": round(news.get(\"neg_ratio\", 0.0), 4),\n",
        "                \"label_news\": news.get(\"label_news\", \"ì¤‘ë¦½\"),\n",
        "                \"red_hits\": news.get(\"red_hits\", 0),\n",
        "            },\n",
        "            \"prior\": {\n",
        "                \"is_listed\": prior.get(\"is_listed\"),\n",
        "                \"name_pattern\": prior.get(\"name_pattern\"),\n",
        "                \"parent_hint\": prior.get(\"parent_hint\"),\n",
        "                \"age_hint\": prior.get(\"age_hint\"),\n",
        "            },\n",
        "            \"combined\": {\n",
        "                \"final_label\": combined.get(\"final_label\", \"ì¤‘ë¦½\"),\n",
        "                \"total_score\": combined.get(\"total_score\", 0),\n",
        "                \"risk_pct\": combined.get(\"risk_pct\", 0.0),\n",
        "            }\n",
        "        }\n",
        "        user_prompt = (\n",
        "            \"ì•„ë˜ JSONì„ ì¢…í•©í•´ ìµœì¢… ë¼ë²¨ì˜ ì´ìœ ë¥¼ ìš”ì•½í•´ì¤˜. \"\n",
        "            \"í•µì‹¬ ê·¼ê±° 1~2ê°œë§Œ ì–¸ê¸‰í•˜ê³  ê³¼ì¥ í‘œí˜„ì€ í”¼í•˜ë¼.\\n\\n\"\n",
        "            + json.dumps(user_payload, ensure_ascii=False, indent=2)\n",
        "        )\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys_prompt},\n",
        "                      {\"role\":\"user\",\"content\":user_prompt}],\n",
        "            max_tokens=120,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        msg = (resp.choices[0].message.content or \"\").strip()\n",
        "        return msg or _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                           news.get(\"red_hits\",0), combined.get(\"final_label\",\"ì¤‘ë¦½\"),\n",
        "                                           news.get(\"news_count\",0))\n",
        "    except Exception:\n",
        "        return _reason_from_metrics(company.get(\"corp_name\",\"\"), news.get(\"neg_ratio\",0.25),\n",
        "                                    news.get(\"red_hits\",0), combined.get(\"final_label\",\"ì¤‘ë¦½\"),\n",
        "                                    news.get(\"news_count\",0))\n",
        "\n",
        "# -----------------------------\n",
        "# 7) ì„œë¹„ìŠ¤ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ë“¤\n",
        "# -----------------------------\n",
        "def run_once(keyword: str, choice_idx: Optional[int] = None) -> dict:\n",
        "    \"\"\"\n",
        "    ë°±ì—”ë“œì—ì„œ ë°”ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•œ ë‹¨ì¼ í•¨ìˆ˜(í›„ë³´ ì„ íƒë„ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬).\n",
        "    main()ì—ì„œëŠ” ì´ í•¨ìˆ˜ë¥¼ ì§ì ‘ ì“°ì§€ ì•Šê³ , run_once_with_corpë¥¼ ì‚¬ìš©í•´\n",
        "    ì‚¬ìš©ìê°€ ì„ íƒí•œ í›„ë³´ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ë„ë¡ í•œë‹¤.\n",
        "    \"\"\"\n",
        "    corps = load_corp_list_from_dart(DART_API_KEY)\n",
        "    cand = find_corp_candidates(corps, keyword, limit=20)\n",
        "    if not cand:\n",
        "        return {\"matches\": [], \"message\": \"í•´ë‹¹ í‚¤ì›Œë“œë¡œ ê¸°ì—…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"}\n",
        "\n",
        "    corp = cand[choice_idx] if (choice_idx is not None and 0 <= choice_idx < len(cand)) else cand[0]\n",
        "    return run_once_with_corp(corp, cand=cand)\n",
        "\n",
        "def run_once_with_corp(corp: dict, cand: Optional[List[dict]] = None) -> dict:\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìê°€ ê³ ë¥¸ 'corp' ê°ì²´ë¥¼ ë°”ë¡œ ë„£ì–´ ë¶„ì„ë§Œ ìˆ˜í–‰.\n",
        "    \"\"\"\n",
        "    # ë‰´ìŠ¤\n",
        "    items = fetch_news(corp[\"corp_name\"])\n",
        "    neg_ratio, label_news, red_hits, top_neg = _compute_news_metrics(items)\n",
        "    news = {\n",
        "        \"news_count\": len(items),\n",
        "        \"neg_ratio\": neg_ratio,\n",
        "        \"label_news\": label_news,\n",
        "        \"red_hits\": red_hits,\n",
        "    }\n",
        "\n",
        "    # í€ë”ë©˜í„¸\n",
        "    fundamental = score_fundamental(corp[\"corp_code\"])\n",
        "\n",
        "    # prior ì»¨í…ìŠ¤íŠ¸\n",
        "    prior = _build_prior_context(corp, news_count=len(items))\n",
        "\n",
        "    # ì¢…í•© ë¼ë²¨\n",
        "    final_label, total_score = _combine_risk_label(fundamental.get(\"flag\",0), label_news, len(items))\n",
        "    # ì •ê·œí™” í¼ì„¼íŠ¸\n",
        "    risk_pct = 0.0\n",
        "    try:\n",
        "        risk_pct = round((total_score / MAX_TOTAL_SCORE) * 100.0, 1) if MAX_TOTAL_SCORE > 0 else 0.0\n",
        "    except Exception:\n",
        "        risk_pct = 0.0\n",
        "    combined = {\"final_label\": final_label, \"total_score\": total_score, \"risk_pct\": risk_pct}\n",
        "\n",
        "    # ì´ìœ (GPT ë˜ëŠ” í´ë°±)\n",
        "    reason = _gpt_reason(\n",
        "        company={\"corp_name\": corp[\"corp_name\"], \"corp_code\": corp[\"corp_code\"], \"stock_code\": corp.get(\"stock_code\",\"\")},\n",
        "        fundamental=fundamental, news=news, prior=prior, combined=combined\n",
        "    )\n",
        "    # ì´í‰(GPT ë˜ëŠ” í´ë°±)\n",
        "    overall_comment = _gpt_overall_comment(\n",
        "        company={\"corp_name\": corp[\"corp_name\"], \"corp_code\": corp[\"corp_code\"], \"stock_code\": corp.get(\"stock_code\",\"\")},\n",
        "        fundamental=fundamental, news=news, prior=prior, combined=combined\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"company\": corp,\n",
        "        \"fundamental\": fundamental,\n",
        "        \"news\": news,\n",
        "        \"prior\": prior,\n",
        "        \"combined\": combined,\n",
        "        \"reason\": reason,\n",
        "        \"overall_comment\": overall_comment,\n",
        "        \"top_neg\": top_neg,\n",
        "        \"items\": items,\n",
        "        \"matches\": cand or [],\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# 8) CLI (ì…ë ¥/í›„ë³´ì„ íƒ/ì¶œë ¥ ì „ë‹´)\n",
        "# -----------------------------\n",
        "def main():\n",
        "    print(\"ğŸ” ê²€ìƒ‰í•  ê¸°ì—…ëª… ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\", flush=True)\n",
        "    try:\n",
        "        keyword = input(\"> \").strip()\n",
        "    except EOFError:\n",
        "        print(\"âŒ í‘œì¤€ì…ë ¥ì´ ì—†ì–´ ëŒ€ê¸° ì¤‘ì´ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ íŒŒì´í”„ë¼ì¸ ì…ë ¥ì„ ì œê³µí•˜ì„¸ìš”.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not keyword:\n",
        "        print(\"âŒ ê¸°ì—…ëª… í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # í›„ë³´ ì¡°íšŒ\n",
        "    try:\n",
        "        corps = load_corp_list_from_dart(DART_API_KEY)\n",
        "        cand = find_corp_candidates(corps, keyword, limit=20)\n",
        "    except Exception as e:\n",
        "        print(\"âŒ DART ê¸°ì—…ëª©ë¡/í›„ë³´ ì¡°íšŒ ì‹¤íŒ¨:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not cand:\n",
        "        print(\"âŒ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ê¸°ì—…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    if len(cand) == 1:\n",
        "        choice_idx = 0\n",
        "        print(f\"âœ… ìë™ ì„ íƒ: {summarize_company(cand[0])}\", flush=True)\n",
        "    else:\n",
        "        print(\"\\në‹¤ìŒ ì¤‘ ê¸°ì—…ì„ ì„ íƒí•˜ì„¸ìš”:\")\n",
        "        for i, c in enumerate(cand):\n",
        "            print(f\"[{i}] {summarize_company(c)}\")\n",
        "        while True:\n",
        "            try:\n",
        "                _in = input(\"ë²ˆí˜¸ ì…ë ¥: \").strip()\n",
        "                choice_idx = int(_in)\n",
        "                if 0 <= choice_idx < len(cand):\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"ìœ íš¨í•œ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\", flush=True)\n",
        "\n",
        "    corp = cand[choice_idx]\n",
        "\n",
        "    # ì„ íƒí•œ corpë¡œ ë°”ë¡œ ë¶„ì„ ìˆ˜í–‰\n",
        "    try:\n",
        "        result = run_once_with_corp(corp, cand=cand)\n",
        "    except Exception as e:\n",
        "        print(\"âŒ ì‹¤í–‰ ì‹¤íŒ¨:\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if not result.get(\"company\"):\n",
        "        print(result.get(\"message\",\"ê¸°ì—…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"))\n",
        "        sys.exit(0)\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    corp = result[\"company\"]\n",
        "    news = result[\"news\"]\n",
        "    fundamental = result[\"fundamental\"]\n",
        "    combined = result[\"combined\"]\n",
        "\n",
        "    print(f\"\\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {news['news_count']}ê±´ (ìµœê·¼ {DEFAULT_DAYS}ì¼)\")\n",
        "    print(\"\\n===== ê²°ê³¼ =====\")\n",
        "    print(\"ê¸°ì—…:\", summarize_company(corp))\n",
        "    print(f\"[ë‰´ìŠ¤] ë¶€ì •ë¹„ìœ¨(ìŠ¤ë¬´ë”©): {news['neg_ratio']*100:.1f}% | ë ˆë“œí‚¤ì›Œë“œ: {news['red_hits']}\")\n",
        "    print(f\"[í€ë”ë©˜í„¸] flag: {fundamental.get('flag',0)} | ì‚¬ìœ : {', '.join(fundamental.get('reasons',[])) or 'ì—†ìŒ'}\")\n",
        "    print(f\"[ì¢…í•©íŒì •] ë¼ë²¨: {combined['final_label']} | ì´ì : {combined['total_score']}/{MAX_TOTAL_SCORE} | ìœ„í—˜ë„(ì •ê·œí™”): {combined['risk_pct']:.1f}%\")\n",
        "    print(f\"ì´ìœ : {result['reason']}\")\n",
        "    print(f\"ì´í‰: {result['overall_comment']}\")\n",
        "\n",
        "    top_neg = result.get(\"top_neg\", [])\n",
        "    if top_neg:\n",
        "        print(\"\\nâš ï¸ ë¶€ì • í™•ë¥  ë†’ì€ ê¸°ì‚¬ Top 3\")\n",
        "        for p, it in top_neg:\n",
        "            dt = it.get(\"date\",\"\")[:19].replace(\"T\",\" \")\n",
        "            print(f\"- {p*100:5.1f}% | {dt} | {it.get('title','').strip()}\")\n",
        "\n",
        "    print(\"\\n(ë©”ëª¨) NAVER ì‹¤íŒ¨ ì‹œ Google RSS ë°±ì—…, FinBERT ì§€ì—°/ì‹¤íŒ¨ ì‹œ ë°±ì—…ë£° ì¦‰ì‹œ ì‚¬ìš©.\")\n",
        "    if NO_FINBERT:\n",
        "        print(\"(ë©”ëª¨) NO_FINBERT=1 í™˜ê²½ë³€ìˆ˜ë¡œ FinBERT ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwHK_jNBaSD",
        "outputId": "1ab59cc7-2f38-41ae-9eb6-bfab81cd4ab6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ê²€ìƒ‰í•  ê¸°ì—…ëª… ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\n",
            "> ìŠ¤ë§ˆì¼ê²Œì´íŠ¸\n",
            "â³ DART ê¸°ì—…ëª©ë¡ ë¡œë”© ì¤‘...\n",
            "âœ… ê¸°ì—…ëª©ë¡ 113,057ê±´ ë¡œë“œ ì™„ë£Œ\n",
            "ğŸ” í›„ë³´ ê²€ìƒ‰: 'ìŠ¤ë§ˆì¼ê²Œì´íŠ¸'\n",
            "âœ… í›„ë³´ 8ê±´\n",
            "\n",
            "ë‹¤ìŒ ì¤‘ ê¸°ì—…ì„ ì„ íƒí•˜ì„¸ìš”:\n",
            "[0] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ê²Œì„ì¦ˆ (ë¹„ìƒì¥, corp_code 00934275)\n",
            "[1] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ìŠ¤í† ë¸Œ (ë¹„ìƒì¥, corp_code 01205107)\n",
            "[2] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ì•Œí”¼ì§€ (ë¹„ìƒì¥, corp_code 00961756)\n",
            "[3] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸í™€ë”©ìŠ¤ (ë¹„ìƒì¥, corp_code 00868194)\n",
            "[4] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ë©”ê°€í¬íŠ¸ (ë¹„ìƒì¥, corp_code 01015911)\n",
            "[5] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ìì‚°ìš´ìš© (ë¹„ìƒì¥, corp_code 01314104)\n",
            "[6] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸ (ë¹„ìƒì¥, corp_code 00432272)\n",
            "[7] ìŠ¤ë§ˆì¼ê²Œì´íŠ¸ì—”í„°í…Œì¸ë¨¼íŠ¸ (ë¹„ìƒì¥, corp_code 00809049)\n",
            "ë²ˆí˜¸ ì…ë ¥: 3\n",
            "â³ NAVER ë‰´ìŠ¤ ìˆ˜ì§‘: 'ìŠ¤ë§ˆì¼ê²Œì´íŠ¸í™€ë”©ìŠ¤'\n",
            "âœ… NAVER 40ê±´\n",
            "â³ FinBERT ë¡œë”© ì‹œë„(ìµœëŒ€ 10s)...\n",
            "âœ… FinBERT ë¡œë”© ì™„ë£Œ\n",
            "\n",
            "ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: 40ê±´ (ìµœê·¼ 50ì¼)\n",
            "\n",
            "===== ê²°ê³¼ =====\n",
            "ê¸°ì—…: ìŠ¤ë§ˆì¼ê²Œì´íŠ¸í™€ë”©ìŠ¤ (ë¹„ìƒì¥, corp_code 00868194)\n",
            "[ë‰´ìŠ¤] ë¶€ì •ë¹„ìœ¨(ìŠ¤ë¬´ë”©): 2.3% | ë ˆë“œí‚¤ì›Œë“œ: 0\n",
            "[í€ë”ë©˜í„¸] flag: 0 | ì‚¬ìœ : ì—†ìŒ\n",
            "[ì¢…í•©íŒì •] ë¼ë²¨: ê¸ì • | ì´ì : 0/8 | ìœ„í—˜ë„(ì •ê·œí™”): 0.0%\n",
            "ì´ìœ : ìµœê·¼ ë³´ë„ì—ì„œ ë¶€ì • ì‹ í˜¸ê°€ ë“œë¬¼ì–´(2.3%Â·ë ˆë“œí‚¤ì›Œë“œ 0ê±´) ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì…ë‹ˆë‹¤.\n",
            "ì´í‰: ìœ„í—˜ë„ 0.0%(ë‚®ìŒ)ë¡œ ê¸ì • íŒë‹¨. ê¸°ë³¸ ìœ ì§€, ì´ìŠˆ ë°œìƒ ê°ì‹œ.\n",
            "\n",
            "(ë©”ëª¨) NAVER ì‹¤íŒ¨ ì‹œ Google RSS ë°±ì—…, FinBERT ì§€ì—°/ì‹¤íŒ¨ ì‹œ ë°±ì—…ë£° ì¦‰ì‹œ ì‚¬ìš©.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cG-mE2tzBZF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
