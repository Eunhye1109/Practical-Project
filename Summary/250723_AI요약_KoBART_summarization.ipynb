{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1YC3_qbMtvBPZuZbNSyJNmL2GZr-hhHqM",
      "authorship_tag": "ABX9TyMhhwTZbeihPlvIWebyxkZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eunhye1109/Practical-Project/blob/EH/250723_AI%EC%9A%94%EC%95%BD_KoBART_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6BDhfm0W9T2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 0. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q transformers\n",
        "!pip install -q PyMuPDF\n",
        "\n",
        "# âœ… 1. ì„í¬íŠ¸\n",
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF_fTNmK-RxH",
        "outputId": "29134e23-f995-4559-8214-9e2d222336a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì²˜ìŒ ì‹¤í–‰ ì‹œ)\n",
        "!pip install sentence-transformers transformers pdfplumber --quiet\n",
        "!pip install -q sentence-transformers transformers\n",
        "\n",
        "!pip install sentence-transformers transformers --quiet\n"
      ],
      "metadata": {
        "id": "_47uKUB57cHi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 0. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install -q sentence-transformers transformers PyMuPDF\n",
        "\n",
        "# âœ… 1. ì„í¬íŠ¸\n",
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "hk7AJ71A_ngm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 2. KoBART ìš”ì•½ ëª¨ë¸ ë¡œë”©\n",
        "tokenizer_kobart = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
        "model_kobart = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\")\n",
        "\n",
        "def summarize_kobart(text, max_length=60, min_length=20):\n",
        "    inputs = tokenizer_kobart([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    summary_ids = model_kobart.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer_kobart.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# âœ… 3. ê¸°ì—…ëª… ì…ë ¥\n",
        "company = input(\"ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "\n",
        "# âœ… 4. ê²½ë¡œ ì„¤ì • ë° PDF ê²€ìƒ‰\n",
        "pdf_dir = \"/content/drive/MyDrive/2. ì·¨ì—… ë¶€ì—…/7. SWê°œë°œìê³¼ì •/3. ì‹¤ì „í”„ë¡œì íŠ¸/[ì‹¤ì „] ì¬ë¬´ ê´€ë ¨ ë°ì´í„°_í˜¸ì„± íŒ€/PDF ì‚¬ì—… ë³´ê³ ì„œ\"\n",
        "\n",
        "if not os.path.exists(pdf_dir):\n",
        "    print(\"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    files = os.listdir(pdf_dir)\n",
        "    matched = [f for f in files if company in f and f.endswith(\".pdf\")]\n",
        "\n",
        "    if not matched:\n",
        "        print(f\"âŒ '{company}' ê´€ë ¨ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        pdf_file = os.path.join(pdf_dir, matched[0])\n",
        "        print(f\"âœ… ì°¾ì€ íŒŒì¼: {pdf_file}\")\n",
        "\n",
        "        # âœ… 5. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "        with fitz.open(pdf_file) as doc:\n",
        "            raw_text = \"\"\n",
        "            for page in doc:\n",
        "                raw_text += page.get_text()\n",
        "\n",
        "        # âœ… 6. ì „ì²˜ë¦¬ ë° ë¬¸ì¥ ë‚˜ëˆ„ê¸°\n",
        "        text = re.sub(r'\\[\\d+\\]', '', raw_text)\n",
        "        text = re.sub(r'\\n+', ' ', text)\n",
        "        sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "\n",
        "        def is_valid_sentence(s):\n",
        "            if len(s.strip()) < 10: return False\n",
        "            digits = sum(c.isdigit() for c in s)\n",
        "            if digits / max(len(s), 1) > 0.2: return False\n",
        "            if len(re.findall(r'[ê°€-í£a-zA-Z]', s)) < 5: return False\n",
        "            return True\n",
        "\n",
        "        filtered_sentences = [s.strip() for s in sentences if is_valid_sentence(s)]\n",
        "        print(f\"ğŸ“š ì •ì œëœ ë¬¸ì¥ ìˆ˜: {len(filtered_sentences)}\")\n",
        "\n",
        "        # âœ… 7. SBERT ì„ë² ë”© ë° ì£¼ì œ ì •ì˜\n",
        "        model_sbert = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "        sentence_embeddings = model_sbert.encode(filtered_sentences, convert_to_tensor=True)\n",
        "\n",
        "        topics = {\n",
        "            \"ğŸ“Š ì¬ë¬´ ìš”ì•½\": \"ì¬ë¬´ ì„±ê³¼ ë° ìˆ˜ìµì— ëŒ€í•œ ìš”ì•½\",\n",
        "            \"ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\": \"ë¶€ì±„ ë° ì¬ë¬´ ë¦¬ìŠ¤í¬\",\n",
        "            \"ğŸš€ ì‚¬ì—… ì „ëµ ë° ì„±ì¥\": \"ì„±ì¥ ì „ëµê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íš\",\n",
        "            \"ğŸ”¬ ê¸°ìˆ  ë° R&D\": \"ê¸°ìˆ  íˆ¬ì ë° ì—°êµ¬ê°œë°œ\",\n",
        "            \"ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\": \"í•´ì™¸ ì‹œì¥ ë° ê¸€ë¡œë²Œ ì „ëµ\",\n",
        "            \"ğŸ¢ ì¡°ì§ ë° ì¸ë ¥\": \"ì¡°ì§ êµ¬ì„± ë° ì¸ë ¥ ìš´ì˜\"\n",
        "        }\n",
        "        topic_embeddings = model_sbert.encode(list(topics.values()), convert_to_tensor=True)\n",
        "\n",
        "        # âœ… 8. í† í”½ë³„ ìš”ì•½ ìˆ˜í–‰\n",
        "        summary_result = {}\n",
        "        for i, topic in enumerate(topics.keys()):\n",
        "            cosine_scores = util.pytorch_cos_sim(topic_embeddings[i], sentence_embeddings)[0]\n",
        "            top_k = torch.topk(cosine_scores, k=min(5, len(filtered_sentences)))\n",
        "            top_sentences = [filtered_sentences[idx] for idx in top_k.indices]\n",
        "            joined_text = \" \".join(top_sentences)\n",
        "            summary = summarize_kobart(joined_text)\n",
        "            summary_result[topic] = summary.strip()\n",
        "\n",
        "        # âœ… 9. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥\n",
        "        final_output = \"\"\n",
        "        for sec, s in summary_result.items():\n",
        "            final_output += f\"\\n\\n## {sec}\\n- {s.strip()}\\n\"\n",
        "\n",
        "        output_path = pdf_file.replace(\".pdf\", \"_ìš”ì•½ê²°ê³¼.txt\")\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(final_output.strip())\n",
        "\n",
        "        print(\"âœ… ìš”ì•½ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜:\", output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qObQYOaD5BUq",
        "outputId": "3c982199-9b1d-46b4-ffe4-a04ac5a3f2a0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ì¼ë™ì œì•½\n",
            "âŒ 'ì¼ë™ì œì•½' ê´€ë ¨ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "# íšŒì‚¬ëª… ì •ë¦¬\n",
        "company = input(\"ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "company = unicodedata.normalize(\"NFC\", company)\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "pdf_dir = \"/content/drive/MyDrive/2. ì·¨ì—… ë¶€ì—…/7. SWê°œë°œìê³¼ì •/3. ì‹¤ì „í”„ë¡œì íŠ¸/[ì‹¤ì „] ì¬ë¬´ ê´€ë ¨ ë°ì´í„°_í˜¸ì„± íŒ€/PDF ì‚¬ì—… ë³´ê³ ì„œ\"\n",
        "\n",
        "if not os.path.exists(pdf_dir):\n",
        "    print(\"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    files = os.listdir(pdf_dir)\n",
        "\n",
        "    # íŒŒì¼ëª… ì •ê·œí™” í›„ ë¹„êµ\n",
        "    matched = [f for f in files if company in unicodedata.normalize(\"NFC\", f) and f.endswith(\".pdf\")]\n",
        "\n",
        "    if not matched:\n",
        "        print(f\"âŒ '{company}' ê´€ë ¨ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(f\"âœ… ì°¾ì€ íŒŒì¼: {matched[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq2kEX9JEIAY",
        "outputId": "a8815248-9b41-4fc3-91dd-9a8423254e05"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ì¼ë™ì œì•½\n",
            "âœ… ì°¾ì€ íŒŒì¼: á„‹á…µá†¯á„ƒá…©á†¼á„Œá…¦á„‹á…£á†¨ á„‰á…¡á„‹á…¥á†¸á„‡á…©á„€á…©á„‰á…¥ (2024.12).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWdW2ysUIUZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ìµœì¢… ìš°ë¦¬ë§ ìš”ì•½ ê¸°ëŠ¥"
      ],
      "metadata": {
        "id": "xzVrvBJTIUv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì½”ë©ì—ì„œ í•„ìš” ì‹œ ì‹¤í–‰)\n",
        "!pip install -q sentence-transformers transformers PyMuPDF\n",
        "\n",
        "# âœ… 1. ì„í¬íŠ¸\n",
        "import os, re, unicodedata\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# âœ… 2. KoBART ìš”ì•½ ëª¨ë¸ ë¡œë”©\n",
        "tokenizer_kobart = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\", use_fast=False)\n",
        "model_kobart = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\")\n",
        "\n",
        "def summarize_kobart(text, max_length=120, min_length=30):\n",
        "    inputs = tokenizer_kobart([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    summary_ids = model_kobart.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer_kobart.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# âœ… 3. SBERT ì„ë² ë”© ëª¨ë¸\n",
        "model_sbert = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# âœ… 4. ì‚¬ìš©ì ì…ë ¥\n",
        "company = input(\"ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "company = unicodedata.normalize(\"NFC\", company)\n",
        "\n",
        "# âœ… 5. PDF íŒŒì¼ ì½ê¸° (ì •í™•í•œ ê²½ë¡œë¡œ ìˆ˜ì •)\n",
        "pdf_dir = \"/content/drive/MyDrive/2. ì·¨ì—… ë¶€ì—…/7. SWê°œë°œìê³¼ì •/3. ì‹¤ì „í”„ë¡œì íŠ¸/[ì‹¤ì „] ì¬ë¬´ ê´€ë ¨ ë°ì´í„°_í˜¸ì„± íŒ€/PDF ì‚¬ì—… ë³´ê³ ì„œ\"\n",
        "assert os.path.exists(pdf_dir), \"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "\n",
        "files = os.listdir(pdf_dir)\n",
        "matched = [f for f in files\n",
        "           if company in unicodedata.normalize(\"NFC\", f) and f.lower().endswith(\".pdf\")]\n",
        "\n",
        "if not matched:\n",
        "    raise FileNotFoundError(f\"âŒ '{company}' ê´€ë ¨ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "pdf_path = os.path.join(pdf_dir, matched[0])\n",
        "print(\"âœ… ì°¾ì€ PDF:\", pdf_path)\n",
        "\n",
        "# âœ… 6. PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "doc = fitz.open(pdf_path)\n",
        "raw_text = \"\"\n",
        "for page in doc:\n",
        "    raw_text += page.get_text()\n",
        "doc.close()\n",
        "\n",
        "# âœ… 7. ì „ì²˜ë¦¬: ë¬¸ì¥ ë¶„ë¦¬ ë° í•„í„°ë§\n",
        "text = re.sub(r'\\[\\d+\\]', '', raw_text)\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "\n",
        "def is_valid(s):\n",
        "    s = s.strip()\n",
        "    if len(s) < 10: return False\n",
        "    if sum(c.isdigit() for c in s) / max(len(s),1) > 0.2: return False\n",
        "    if len(re.findall(r'[ê°€-í£a-zA-Z]', s)) < 5: return False\n",
        "    return True\n",
        "\n",
        "filtered = [s for s in sentences if is_valid(s)]\n",
        "print(\"ğŸ“š ìœ íš¨ ë¬¸ì¥ ìˆ˜:\", len(filtered))\n",
        "\n",
        "# âœ… 8. ì„ë² ë”© ê³„ì‚°\n",
        "sent_embed = model_sbert.encode(filtered, convert_to_tensor=True)\n",
        "\n",
        "topics = {\n",
        "    # \"ğŸ“Š ì¬ë¬´ ìš”ì•½\": \"ì¬ë¬´ ì„±ê³¼ ë° ìˆ˜ìµ\",\n",
        "    # \"ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\": \"ë¶€ì±„ ë° ë¦¬ìŠ¤í¬\",\n",
        "    # \"ğŸš€ ì‚¬ì—… ì „ëµ\": \"ì‚¬ì—… ì „ëµ ë° ì„±ì¥ ê³„íš\",\n",
        "    # \"ğŸ”¬ ê¸°ìˆ  ë° R&D\": \"ê¸°ìˆ  íˆ¬ì ë° ì—°êµ¬ê°œë°œ\",\n",
        "    # \"ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\": \"ê¸€ë¡œë²Œ ì§„ì¶œ ë° í•´ì™¸ ì‹œì¥\",\n",
        "    \"ğŸ“Š ì¬ë¬´ ìš”ì•½\": \"ìµœê·¼ 3ê°œë…„ ì†ìµê³„ì‚°ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ, ë‹¹ê¸°ìˆœì´ìµ ë“±ì˜ ì£¼ìš” ì¬ë¬´ì„±ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\": \"ê¸°ì—…ì˜ ì£¼ìš” ë¶€ì±„ í˜„í™©, ìœ ë™ì„± ë° ì§€ê¸‰ëŠ¥ë ¥ ë¦¬ìŠ¤í¬, ì±„ë¬´ë¶ˆì´í–‰ ê°€ëŠ¥ì„±, ì§€ê¸‰ë³´ì¦ ë° ë‹´ë³´ì œê³µ ë“±ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸš€ ì‚¬ì—… ì „ëµ\": \"ì‹ ê·œ ì‚¬ì—… ì¶”ì§„, íˆ¬ìê³„íš, ë§¤ì¶œ ì¦ëŒ€ë¥¼ ìœ„í•œ ì „ëµ, ì‚¬ì—… ë‹¤ê°í™” ë°©ì•ˆ ë° í–¥í›„ ì¤‘ì¥ê¸° ì„±ì¥ê³„íšì— ê´€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸ”¬ ê¸°ìˆ  ë° R&D\": \"ì—°êµ¬ê°œë°œ íˆ¬ì í˜„í™©, ì£¼ìš” ì‹ ì•½ ë° ê¸°ìˆ  íŒŒì´í”„ë¼ì¸ ì§„í–‰ìƒí™©, ê¸°ìˆ  ì œíœ´ ë° ì—°êµ¬ì„±ê³¼ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\": \"í•´ì™¸ì‹œì¥ ë§¤ì¶œ ë¹„ì¤‘, ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆì‹­ ì²´ê²°, ìˆ˜ì¶œ í™•ëŒ€, í˜„ì§€ë²•ì¸ ì„¤ë¦½ ë“± ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ ë° ì„±ê³¼ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "\n",
        "}\n",
        "\n",
        "topic_embed = model_sbert.encode(list(topics.values()), convert_to_tensor=True)\n",
        "\n",
        "# âœ… 9. ì£¼ì œë³„ ë¬¸ì¥ ì¶”ì¶œ & ìš”ì•½\n",
        "result = {}\n",
        "for i, title in enumerate(topics.keys()):\n",
        "    sims = util.pytorch_cos_sim(topic_embed[i], sent_embed)[0]\n",
        "    topk = torch.topk(sims, k=min(5, len(filtered)))\n",
        "    sel = [filtered[idx] for idx in topk.indices]\n",
        "    joined = \" \".join(sel)\n",
        "    result[title] = summarize_kobart(joined)\n",
        "\n",
        "# âœ… 10. ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\n\\n======= ìš”ì•½ ê²°ê³¼ =======\")\n",
        "for sec, summ in result.items():\n",
        "    print(f\"\\n## {sec}\\n{summ}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBGrl26yET9k",
        "outputId": "a5c81043-f029-467b-f3d6-210f1973ce7c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ì•Œí”¼ë°”ì´ì˜¤\n",
            "âœ… ì°¾ì€ PDF: /content/drive/MyDrive/2. ì·¨ì—… ë¶€ì—…/7. SWê°œë°œìê³¼ì •/3. ì‹¤ì „í”„ë¡œì íŠ¸/[ì‹¤ì „] ì¬ë¬´ ê´€ë ¨ ë°ì´í„°_í˜¸ì„± íŒ€/PDF ì‚¬ì—… ë³´ê³ ì„œ/á„‹á…¡á†¯á„‘á…µá„‡á…¡á„‹á…µá„‹á…© á„‰á…¡á„‹á…¥á†¸á„‡á…©á„€á…©á„‰á…¥ (2024.12).pdf\n",
            "ğŸ“š ìœ íš¨ ë¬¸ì¥ ìˆ˜: 698\n",
            "\n",
            "\n",
            "======= ìš”ì•½ ê²°ê³¼ =======\n",
            "\n",
            "## ğŸ“Š ì¬ë¬´ ìš”ì•½\n",
            "ì¶œì ëª©ì  ì¶œìíšŒì‚¬ìˆ˜ ì´ ì¶œìê¸ˆì•¡ ìƒì¥ ë¹„ìƒì¥ ê³„ ê¸°ì´ˆ ì¥ë¶€ ê°€ì•¡ ì¦ê°€(ê°ì†Œ) ê¸°ë§ ì¥ë¶€ ê°€ì•¡ ì·¨ë“ (ì²˜ë¶„) í‰ê°€ ì†ìµ ê²½ì˜ì°¸ì—¬ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\n",
            "ë‹¹ê¸°ë²•ì¸ì„¸ ìì‚°ê³¼ ë¶€ì±„ëŠ” ë²•ì ìœ¼ë¡œ ìƒê³„í•  ìˆ˜ ìˆëŠ” ê¶Œ ë¦¬ë¥¼ íšŒì‚¬ê°€ ë³´ìœ í•˜ê³  ìˆê³ , ìˆœì•¡ìœ¼ë¡œ ê²°ì œí•  ì˜ë„ê°€ ìˆê±°ë‚˜ ìì‚°ì„ ì‹¤í˜„í•˜ëŠ” ë™ì‹œì— ë¶€ì±„ë¥¼ ê²°ì œí•˜ë ¤ëŠ” ì˜ë„ê°€ ìˆëŠ” ê²½ìš°ì— ìƒê³„í•œë‹¤.\n",
            "\n",
            "## ğŸš€ ì‚¬ì—… ì „ëµ\n",
            "ì •ë¶€ì˜ ì •ì±…ê¸°ì¡° ê°€ ì‹œì¥í™œì„±í™”ë¥¼ ìœ„í•œ ê¸°ì¡°ê°€ ì§€ì†ë˜ê³  ìˆìœ¼ë©° ì•ìœ¼ë¡œ ë°”ì´ì˜¤ì˜ì•½í’ˆì˜ ì„±ì¥, ì™¸ìì‚¬ì™€ì˜ ì „ëµì  ì œíœ´, ê°œë³„ ì¢…ëª©ë“¤ì˜ ì—°êµ¬ì„±ê³¼ë‚˜ ìˆ˜ì¶œê³„ì•½ ë“±ì´ ì§€ì†ì ìœ¼ë¡œ ë¶€ê°ë˜ë©° í–¥í›„ ê²½ì œì„±ì¥ì„ ì´ëŒ ì£¼ë ¥ì‚°ì—…ìœ¼ë¡œ ì„±ì¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.\n",
            "\n",
            "## ğŸ”¬ ê¸°ìˆ  ë° R&D\n",
            "ë°”ì´ì˜¤ì˜ì•½í’ˆì˜ ì„±ì¥, ì™¸ìì‚¬ì™€ì˜ ì „ëµì  ì œíœ´, ê°œë³„ ì¢…ëª©ë“¤ì˜ ì—°êµ¬ì„±ê³¼ë‚˜ ìˆ˜ì¶œê³„ì•½ ë“±ì´ ì§€ì†ì ìœ¼ë¡œ ë¶€ê°ë˜ë©° í–¥í›„ ê²½ì œì„±ì¥ì„ ì´ëŒ ì£¼ë ¥ì‚°ì—…ìœ¼ë¡œ ì„±ì¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.\n",
            "\n",
            "## ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\n",
            "ì œì•½ì‹œì¥ì€ ì•ìœ¼ë¡œ ë°”ì´ì˜¤ì˜ì•½í’ˆì˜ ì„±ì¥, ì™¸ìì‚¬ì™€ì˜ ì „ëµì  ì œíœ´, ê°œë³„ ì¢…ëª©ë“¤ì˜ ì—°êµ¬ì„±ê³¼ë‚˜ ìˆ˜ì¶œê³„ì•½ ë“±ì´ ì§€ì†ì ìœ¼ë¡œ ë¶€ê°ë˜ë©° í–¥í›„ ê²½ì œì„±ì¥ì„ ì´ëŒ ì£¼ë ¥ì‚°ì—…ìœ¼ë¡œ ì„±ì¥í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì‰¬ìš´ ë§ ìš”ì•½ ë²„ì „"
      ],
      "metadata": {
        "id": "bg9anMN9Kppx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì½”ë©ì—ì„œ í•„ìš” ì‹œ ì‹¤í–‰)\n",
        "!pip install -q sentence-transformers transformers PyMuPDF\n",
        "\n",
        "# âœ… 1. ì„í¬íŠ¸\n",
        "import os, re, unicodedata\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# âœ… 2. KoBART ìš”ì•½ ëª¨ë¸ ë¡œë”©\n",
        "tokenizer_kobart = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\", use_fast=False)\n",
        "model_kobart = AutoModelForSeq2SeqLM.from_pretrained(\"digit82/kobart-summarization\")\n",
        "\n",
        "def summarize_kobart(text, max_length=120, min_length=30):\n",
        "    inputs = tokenizer_kobart([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    summary_ids = model_kobart.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer_kobart.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# âœ… 3. ì‰¬ìš´ ë¬¸ì¥ í›„ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def simplify_korean(text):\n",
        "    text = text.replace(\"í•´ë‹¹\", \"ì´\")\n",
        "    text = text.replace(\"ì˜í•˜ì—¬\", \"ë¡œ ì¸í•´\")\n",
        "    text = text.replace(\"í•˜ê³ ì í•œë‹¤\", \"í•˜ë ¤ê³  í•œë‹¤\")\n",
        "    text = text.replace(\"ìˆìŠµë‹ˆë‹¤\", \"ìˆì–´ìš”\")\n",
        "    text = text.replace(\"ë˜ì—ˆìŠµë‹ˆë‹¤\", \"ëì–´ìš”\")\n",
        "    text = text.replace(\"í•˜ê³  ìˆìŠµë‹ˆë‹¤\", \"í•˜ê³  ìˆì–´ìš”\")\n",
        "    text = text.replace(\"ìˆ˜ ìˆìŠµë‹ˆë‹¤\", \"ìˆ˜ ìˆì–´ìš”\")\n",
        "    text = text.replace(\"ìˆ˜ ì—†ìŠµë‹ˆë‹¤\", \"í•  ìˆ˜ ì—†ì–´ìš”\")\n",
        "    text = text.replace(\"ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤\", \"ì œê³µ ì¤‘ì´ì—ìš”\")\n",
        "    return text\n",
        "\n",
        "# âœ… 4. SBERT ì„ë² ë”© ëª¨ë¸\n",
        "model_sbert = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# âœ… 5. ì‚¬ìš©ì ì…ë ¥\n",
        "company = input(\"ìš”ì•½í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
        "company = unicodedata.normalize(\"NFC\", company)\n",
        "\n",
        "# âœ… 6. PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "pdf_dir = \"/content/drive/MyDrive/2. ì·¨ì—… ë¶€ì—…/7. SWê°œë°œìê³¼ì •/3. ì‹¤ì „í”„ë¡œì íŠ¸/[ì‹¤ì „] ì¬ë¬´ ê´€ë ¨ ë°ì´í„°_í˜¸ì„± íŒ€/PDF ì‚¬ì—… ë³´ê³ ì„œ\"\n",
        "assert os.path.exists(pdf_dir), \"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "files = os.listdir(pdf_dir)\n",
        "matched = [f for f in files\n",
        "           if company in unicodedata.normalize(\"NFC\", f) and f.lower().endswith(\".pdf\")]\n",
        "\n",
        "if not matched:\n",
        "    raise FileNotFoundError(f\"âŒ '{company}' ê´€ë ¨ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "pdf_path = os.path.join(pdf_dir, matched[0])\n",
        "print(\"âœ… ì°¾ì€ PDF:\", pdf_path)\n",
        "\n",
        "# âœ… 7. PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "doc = fitz.open(pdf_path)\n",
        "raw_text = \"\"\n",
        "for page in doc:\n",
        "    raw_text += page.get_text()\n",
        "doc.close()\n",
        "\n",
        "# âœ… 8. ì „ì²˜ë¦¬: ë¬¸ì¥ ë¶„ë¦¬ ë° í•„í„°ë§\n",
        "text = re.sub(r'\\[\\d+\\]', '', raw_text)\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
        "\n",
        "def is_valid(s):\n",
        "    s = s.strip()\n",
        "    if len(s) < 10: return False\n",
        "    if sum(c.isdigit() for c in s) / max(len(s),1) > 0.2: return False\n",
        "    if len(re.findall(r'[ê°€-í£a-zA-Z]', s)) < 5: return False\n",
        "    return True\n",
        "\n",
        "filtered = [s for s in sentences if is_valid(s)]\n",
        "print(\"ğŸ“š ìœ íš¨ ë¬¸ì¥ ìˆ˜:\", len(filtered))\n",
        "\n",
        "# âœ… 9. ì„ë² ë”© ê³„ì‚°\n",
        "sent_embed = model_sbert.encode(filtered, convert_to_tensor=True)\n",
        "\n",
        "topics = {\n",
        "    \"ğŸ“Š ì¬ë¬´ ìš”ì•½\": \"ìµœê·¼ 3ê°œë…„ ì†ìµê³„ì‚°ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œì•¡, ì˜ì—…ì´ìµ, ë‹¹ê¸°ìˆœì´ìµ ë“±ì˜ ì£¼ìš” ì¬ë¬´ì„±ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\": \"ê¸°ì—…ì˜ ì£¼ìš” ë¶€ì±„ í˜„í™©, ìœ ë™ì„± ë° ì§€ê¸‰ëŠ¥ë ¥ ë¦¬ìŠ¤í¬, ì±„ë¬´ë¶ˆì´í–‰ ê°€ëŠ¥ì„±, ì§€ê¸‰ë³´ì¦ ë° ë‹´ë³´ì œê³µ ë“±ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸš€ ì‚¬ì—… ì „ëµ\": \"ì‹ ê·œ ì‚¬ì—… ì¶”ì§„, íˆ¬ìê³„íš, ë§¤ì¶œ ì¦ëŒ€ë¥¼ ìœ„í•œ ì „ëµ, ì‚¬ì—… ë‹¤ê°í™” ë°©ì•ˆ ë° í–¥í›„ ì¤‘ì¥ê¸° ì„±ì¥ê³„íšì— ê´€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸ”¬ ê¸°ìˆ  ë° R&D\": \"ì—°êµ¬ê°œë°œ íˆ¬ì í˜„í™©, ì£¼ìš” ì‹ ì•½ ë° ê¸°ìˆ  íŒŒì´í”„ë¼ì¸ ì§„í–‰ìƒí™©, ê¸°ìˆ  ì œíœ´ ë° ì—°êµ¬ì„±ê³¼ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤.\",\n",
        "    \"ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\": \"í•´ì™¸ì‹œì¥ ë§¤ì¶œ ë¹„ì¤‘, ê¸€ë¡œë²Œ íŒŒíŠ¸ë„ˆì‹­ ì²´ê²°, ìˆ˜ì¶œ í™•ëŒ€, í˜„ì§€ë²•ì¸ ì„¤ë¦½ ë“± ê¸€ë¡œë²Œ ì§„ì¶œ ì „ëµ ë° ì„±ê³¼ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\",\n",
        "}\n",
        "\n",
        "topic_embed = model_sbert.encode(list(topics.values()), convert_to_tensor=True)\n",
        "\n",
        "# âœ… 10. ì£¼ì œë³„ ë¬¸ì¥ ì¶”ì¶œ & ìš”ì•½ + ì‰¬ìš´ ë§ ë³€í™˜\n",
        "result = {}\n",
        "for i, title in enumerate(topics.keys()):\n",
        "    sims = util.pytorch_cos_sim(topic_embed[i], sent_embed)[0]\n",
        "    topk = torch.topk(sims, k=min(5, len(filtered)))\n",
        "    sel = [filtered[idx] for idx in topk.indices]\n",
        "    joined = \" \".join(sel)\n",
        "    summarized = summarize_kobart(joined)\n",
        "    simplified = simplify_korean(summarized)\n",
        "    result[title] = simplified\n",
        "\n",
        "# âœ… 11. ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\n\\n======= ìš”ì•½ ê²°ê³¼ (ì‰¬ìš´ ë§) =======\")\n",
        "for sec, summ in result.items():\n",
        "    print(f\"\\n## {sec}\")\n",
        "    for line in summ.split(\". \"):\n",
        "        print(f\"- {line.strip()}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwDYUb5PEcTv",
        "outputId": "6b18817a-1585-4b88-af95-2d1399862608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels will be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RuNY4F5_Sim",
        "outputId": "2429abd1-eb1a-42e7-8b69-ef9294931ff0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======= ìš”ì•½ ê²°ê³¼ (ì‰¬ìš´ ë§) =======\n",
            "\n",
            "## ğŸ“Š ì¬ë¬´ ìš”ì•½\n",
            "- (4) ë‹¹ê¸°ë§ ë° ì „ê¸°ë§ í˜„ì¬ íˆ¬ìë¶€ë™ì‚°ì˜ ê³µì •ê°€ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©° (4) ë‹¹ê¸°ë§ ë° ì „ê¸°ë§ í˜„ì¬ íˆ¬ìë¶€ë™ì‚°ì˜ ê³µì •ê°€ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤..\n",
            "\n",
            "## ğŸ›¡ï¸ ë¦¬ìŠ¤í¬ ë° ë¶€ì±„\n",
            "- ë‹¹ê¸°ë²•ì¸ì„¸ìì‚°ê³¼ ë¶€ì±„ëŠ” ë²•ì ìœ¼ë¡œ ìƒê³„í•  ìˆ˜ ìˆëŠ” ê¶Œë¦¬ë¥¼ íšŒì‚¬ê°€ ë³´ìœ í•˜ê³  ìˆê³ , ìˆœì•¡ìœ¼ë¡œ ê²°ì œí•  ì˜ë„ê°€ ìˆê±°ë‚˜ ìì‚°ì„ ì‹¤í˜„í•˜ëŠ” ë™ì‹œì— ë¶€ì±„ë¥¼ ê²°ì œí•˜ë ¤ëŠ” ì˜ë„ê°€ ìˆëŠ” ê²½ìš°ì— ìƒê³„í•˜ì—¬ ì¬ë¬´ìƒíƒœí‘œì— ìˆœì•¡ìœ¼ë¡œ í‘œì‹œí•˜ê³  ìˆë‹¤..\n",
            "\n",
            "## ğŸš€ ì‚¬ì—… ì „ëµ\n",
            "- ë°°ë‹¹ì ˆì°¨ ê°œì„ ë°©ì•ˆ ì´í–‰ ê´€ë ¨ í–¥í›„ ê³„íš ë‹¹ì‚¬ëŠ” ë°°ë‹¹ ì ˆì°¨ ê°œì„ ë°©ì•ˆì— ëŒ€í•´ íƒ€ìƒì¥ë¦¬ì¸ ì‚¬ ë° ë‹¹ì‚¬ ì‚¬ì—…í˜„í™© ë“±ì„ ê³ ë ¤í•˜ì—¬ ë„ì…ì„ ê²€í† í•˜ê² ë‹¤..\n",
            "\n",
            "## ğŸ”¬ ê¸°ìˆ  ë° R&D\n",
            "- ë°°ë‹¹ì ˆì°¨ ê°œì„ ë°©ì•ˆ ì´í–‰ ê´€ë ¨ í–¥í›„ ê³„íš ë‹¹ì‚¬ëŠ” ë°°ë‹¹ ì ˆì°¨ ê°œì„ ë°©ì•ˆì— ëŒ€í•´ íƒ€ìƒì¥ë¦¬ì¸ ì‚¬ ë° ë‹¹ì‚¬ ì‚¬ì—…í˜„í™© ë“±ì„ ê³ ë ¤í•˜ì—¬ ë„ì…ì„ ê²€í† í•˜ê² ë‹¤..\n",
            "\n",
            "## ğŸŒ ê¸€ë¡œë²Œ ì§„ì¶œ\n",
            "- 100.0 ì‚¼ì„±ë¬¼ì‚°(ì£¼) Vista Contracting and Investment Global Pte.ëŠ” 100.0 ì‚¼ì„±ë¬¼ì‚°(ì£¼) Vista Contracting and Investment Global Pte.ì˜ ì£¼ìš” ë§¤ì¶œì²˜ ë° ë§¤ì¶œì•¡ì€ ì•„ë˜ì™€ ê°™ìœ¼ë©° ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™, ì£¼ìš”ê³„ì•½ ë° ì—°êµ¬ê°œë°œí™œë™.............................................................................................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69wZBzGELdHo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
