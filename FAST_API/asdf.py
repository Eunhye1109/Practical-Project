# #| êµ¬ì„± ìš”ì†Œ            | ì„¤ëª…                                                                                |
# #| ---------------- | --------------------------------------------------------------------------------- |
# #| **1. ê¸°ì—… ê²€ìƒ‰**     | `load_corp_list()` + `find_corp_by_keyword()`ë¥¼ í†µí•´ í‚¤ì›Œë“œ í¬í•¨ ê¸°ì—… ê²€ìƒ‰                    |
# #| **2. ê¸°ì—… í”„ë¡œí•„ ìˆ˜ì§‘** | `collect_company_profile()`                                                       |
# #| **3. ë‰´ìŠ¤ ìˆ˜ì§‘**     | `fetch_news_articles()` â€“ NAVER API í™œìš©                                            |
# #| **4. ê°ì„± ë¶„ì„**     | `analyze_sentiment()` â€“ FinBERT í™œìš©                                                |
# #| **5. GPT ìš”ì•½ ìƒì„±** | `gpt_summary()` â€“ íˆ¬ìì ìœ í˜•ì— ë”°ë¼ ë¶„ì„ í¬ì¸íŠ¸ ë¶„ê¸°                                            |
# #| **6. ë‰´ìŠ¤ ìš”ì•½ ìƒì„±**  | `summarize_news_articles()` â€“ ê°ì„± ë¶„ì„ ê²°ê³¼ í¬í•¨ GPT ìš”ì•½                                  |
# #| **7. ì €ì¥ ì²˜ë¦¬**     | `save_reports()` â€“ ìš”ì•½ í…ìŠ¤íŠ¸, JSON, Markdown ì €ì¥                                      |
# #| **8. ê²°ê³¼ ë°˜í™˜**     | ê²½ë¡œ í¬í•¨ ë”•ì…”ë„ˆë¦¬ ë¦¬í„´ (`corp_name`, `summary_path`, `json_path`, `md_path`, `news_items`) |




# pip install --upgrade openai
# !pip install transformers
# import requests
# import urllib.parse
# import zipfile
# import io
# import json
# import time
# import os
# import xml.etree.ElementTree as ET
# import pandas as pd
# import openai
# import torch
# from openai import OpenAI
# from transformers import AutoTokenizer, AutoModelForSequenceClassification



# # ğŸ” ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ ì…ë ¥
# NAVER_CLIENT_ID = "í‚¤ ì•ˆì•Œë´ì¤Œ"
# NAVER_CLIENT_SECRET = "ì•ˆì•Œë´ì¥¼"
# DART_API_KEY = "ì•ˆì•Œë´ì¤Œ!!!!!!!"
# OPENAI_API_KEY = "ì²˜ìŒë¶€í„° ê°œë°œ ì˜í–ˆë˜ ê¹€í˜¸ì„±ì”¨ ì•„ì£¼ ë¶€ëŸ½ë‹¤"  # ì—¬ê¸°ì— OpenAI í‚¤ ì…ë ¥
# client = OpenAI(api_key=OPENAI_API_KEY)

#  # âœ… ë“œë¼ì´ë¸Œ ì„í¬íŠ¸
# from google.colab import drive
# drive.mount('/content/drive')

# # âœ… FinBERT ë¡œë“œ
# tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
# model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")

# # âœ… DART ê¸°ì—… ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
# def load_corp_list(api_key: str) -> pd.DataFrame:
#     url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}"
#     res = requests.get(url)
#     if res.status_code != 200 or b'PK' not in res.content[:2]:
#         raise Exception("âŒ DART APIì—ì„œ corpCode.zipì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#     z = zipfile.ZipFile(io.BytesIO(res.content))
#     xml_content = z.read(z.namelist()[0])
#     root = ET.fromstring(xml_content)
#     return pd.DataFrame([{
#         "corp_code": item.findtext('corp_code'),
#         "corp_name": item.findtext('corp_name'),
#         "stock_code": item.findtext('stock_code'),
#         "modify_date": item.findtext('modify_date')
#     } for item in root.findall('list')])

# # âœ… ê¸°ì—…ëª… ê²€ìƒ‰
# def find_corp_by_keyword(df: pd.DataFrame, keyword: str):
#     return df[df["corp_name"].str.contains(keyword, case=False, na=False)]

# # âœ… DART ê¸°ì—… í”„ë¡œí•„ ìˆ˜ì§‘
# def collect_company_profile(corp_code: str, api_key: str):
#     url = "https://opendart.fss.or.kr/api/company.json"
#     params = {"crtfc_key": api_key, "corp_code": corp_code}
#     res = requests.get(url, params=params).json()
#     if res.get("status") != "000":
#         return {}
#     return {
#         "ê¸°ì—…ëª…": res.get("corp_name"),
#         "ëŒ€í‘œìëª…": res.get("ceo_nm"),
#         "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸": res.get("bizr_no"),
#         "ì£¼ì†Œ": res.get("adres"),
#         "í™ˆí˜ì´ì§€": res.get("hm_url"),
#         "ì—…ì¢…ì½”ë“œ": res.get("induty_code"),
#         "ì„¤ë¦½ì¼ì": res.get("est_dt"),
#         "ìƒì¥ì—¬ë¶€": "ìƒì¥" if res.get("stock_code") else "ë¹„ìƒì¥",
#         "ì§€ì£¼íšŒì‚¬": res.get("hm_ownr_corp_nm")
#     }

# # âœ… ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ (ë³¸ë¬¸ + ë§í¬)
# def fetch_news_articles(keyword: str, max_count: int = 5):
#     url = "https://openapi.naver.com/v1/search/news.json"
#     headers = {
#         "X-Naver-Client-Id": NAVER_CLIENT_ID,
#         "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
#     }
#     params = {"query": keyword, "display": max_count, "sort": "date"}
#     res = requests.get(url, headers=headers, params=params)
#     if res.status_code != 200:
#         return []
#     return [{
#         "title": item["title"],
#         "description": item["description"],
#         "link": item["link"]
#     } for item in res.json().get("items", [])]

# # âœ… ê°ì„± ë¶„ì„ (FinBERT)
# def analyze_sentiment(texts: list) -> list:
#     results = []
#     for text in texts:
#         inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
#         with torch.no_grad():
#             logits = model(**inputs).logits
#             probs = torch.nn.functional.softmax(logits, dim=1)[0]
#             label = torch.argmax(probs).item()
#             sentiment = ["Negative", "Neutral", "Positive"][label]
#         results.append(sentiment)
#     return results

# # âœ… GPT ìš”ì•½ (ê¸°ì—… ê°œìš” ê¸°ë°˜)
# def gpt_summary(profile: dict, investor_type: str):
#     guide = {
#         "ì•ˆì •í˜•": "ì¥ê¸°ì  ì¬ë¬´ ì•ˆì •ì„±ê³¼ ë°°ë‹¹ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
#         "ê³µê²©í˜•": "ë‹¨ê¸° ê³ ìˆ˜ìµ ë˜ëŠ” ê³ ì„±ì¥ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.",
#         "ìœµí•©í˜•": "ìˆ˜ìµì„±ê³¼ ì•ˆì •ì„± ì‚¬ì´ ê· í˜•ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”."
#     }
#     prompt = f"""
# ë‹¤ìŒì€ í•œ ê¸°ì—…ì˜ ê°œìš”ì…ë‹ˆë‹¤. ì‚°ì—… íŠ¹ì§•, ì£¼ìš” ì‚¬ì—…ì˜ì—­, ê·œëª¨, R&D íˆ¬ì, ë¦¬ìŠ¤í¬ ë“±ì„ ìš”ì•½í•˜ê³ ,
# '{investor_type}' íˆ¬ììì—ê²Œ ì í•©í•œ ì „ëµì„ ì œì‹œí•˜ì„¸ìš”.

# [ê¸°ì—… ê°œìš”]
# - ê¸°ì—…ëª…: {profile.get("ê¸°ì—…ëª…")}
# - ëŒ€í‘œìëª…: {profile.get("ëŒ€í‘œìëª…")}
# - ì—…ì¢…ì½”ë“œ: {profile.get("ì—…ì¢…ì½”ë“œ")}
# - ìƒì¥ì—¬ë¶€: {profile.get("ìƒì¥ì—¬ë¶€")}
# - ì„¤ë¦½ì¼ì: {profile.get("ì„¤ë¦½ì¼ì")}
# - ì£¼ì†Œ: {profile.get("ì£¼ì†Œ")}
# - ì§€ì£¼íšŒì‚¬: {profile.get("ì§€ì£¼íšŒì‚¬") or 'ì—†ìŒ'}

# [íˆ¬ìì ìœ í˜•: {investor_type}]
# {guide[investor_type]}
# """
#     response = client.chat.completions.create(
#         model="gpt-4",
#         messages=[
#             {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.3
#     )
#     return response.choices[0].message.content.strip()

# # âœ… GPT ë‰´ìŠ¤ ìš”ì•½

# def summarize_news_articles(news_items: list) -> str:
#     if not news_items:
#         return "ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

#     sentiments = analyze_sentiment([item["description"] for item in news_items])
#     joined = "\n".join(
#         f"- {item['title']} ({sentiment})\n  {item['link']}"
#         for item, sentiment in zip(news_items, sentiments)
#     )

#     prompt = f"""
# ë‹¤ìŒì€ íŠ¹ì • ê¸°ì—…ê³¼ ê´€ë ¨ëœ ìµœê·¼ ë‰´ìŠ¤ ì œëª©, ë§í¬, ê°ì„± ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.
# ì´ ê¸°ì—…ì˜ íˆ¬ì ê´€ì ì—ì„œ í•µì‹¬ ë‰´ìŠ¤ íë¦„ê³¼ ê¸Â·ë¶€ì • ì‹œì‚¬ì ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.

# [ë‰´ìŠ¤ ëª©ë¡]
# {joined}
# """
#     response = client.chat.completions.create(
#         model="gpt-4",
#         messages=[
#             {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
#             {"role": "user", "content": prompt}
#         ],
#         temperature=0.3
#     )
#     return response.choices[0].message.content.strip()

# # âœ… ì €ì¥ í•¨ìˆ˜ (ìš”ì•½, JSON, ë§ˆí¬ë‹¤ìš´)
# def save_reports(profile: dict, summary: str, news_items: list = None):
#     os.makedirs("summaries", exist_ok=True)
#     os.makedirs("reports", exist_ok=True)
#     base = profile["ê¸°ì—…ëª…"].replace("/", "_")

#       # âœ… ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë„ í¬í•¨)
#     with open(f"summaries/{base}_ìš”ì•½.txt", "w", encoding="utf-8") as f:
#         f.write(summary)
#         if news_items:
#             f.write("\n\n## ğŸ“° ë‰´ìŠ¤ ëª©ë¡ ë° ê°ì„± ë¶„ì„\n")
#             for item in news_items:
#                 f.write(f"- {item['title']} ({item.get('sentiment', 'Unknown')})\n  {item['link']}\n")

#     with open(f"summaries/react_items_{base}.json", "w", encoding="utf-8") as f:
#         json.dump({
#             "name": profile.get("ê¸°ì—…ëª…"),
#             "src": "https://res.cloudinary.com/zuzu-homepage/image/upload/v1739446680/p4rn9qxymke0lda94pzk.jpg",
#             "itemList": [
#                 {"type": "text", "data": [summary]},
#                 {"type": "text", "data": [profile.get("ìƒì¥ì—¬ë¶€")]},
#                 {"type": "text", "data": ["ì •ë³´ ì—†ìŒ"]},
#                 {"type": "text", "data": ["ì •ë³´ ì—†ìŒ"]},
#                 {"type": "tag", "data": ["ì‚°ì—…", "ê¸°ìˆ ", "ì„œë¹„ìŠ¤"]},
#                 {"type": "tag", "data": ["ì„±ì¥ì„±", "ì‹œì¥ì§€ë°°ë ¥"]}
#             ]
#         }, f, ensure_ascii=False, indent=2)

#     with open(f"reports/{base}_ìƒì„¸ë³´ê³ ì„œ.md", "w", encoding="utf-8") as f:
#         f.write(f"# \U0001f4c4 {profile['ê¸°ì—…ëª…']} ìƒì„¸ ë³´ê³ ì„œ\n\n")
#         f.write("## \U0001f3e2 ê¸°ì—… ê°œìš”\n")
#         for key, value in profile.items():
#             f.write(f"- **{key}**: {value or 'ì •ë³´ ì—†ìŒ'}\n")
#         f.write("\n## \U0001f4d8 ê¸°ì—… ìƒì„¸\n")
#         f.write(summary)

# # âœ… ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

# def generate_report(keyword: str, investor_type: str = "ìœµí•©í˜•") -> dict:
#     corp_df = load_corp_list(DART_API_KEY)
#     matches = find_corp_by_keyword(corp_df, keyword)
#     if matches.empty:
#         raise ValueError("âŒ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")

#     row = matches.iloc[0]
#     profile = collect_company_profile(row["corp_code"], DART_API_KEY)

#     news_items = fetch_news_articles(keyword)
#     news_summary = summarize_news_articles(news_items)

#     summary = ""
#     if profile:
#         summary = gpt_summary(profile, investor_type)
#     summary += f"\n\n## \U0001f4f0 ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ (ê°ì„± ë¶„ì„ í¬í•¨)\n{news_summary}"

#     save_reports(profile or {"ê¸°ì—…ëª…": keyword}, summary, news_items=news_items)

#     corp_name = profile.get("ê¸°ì—…ëª…") if profile else keyword
#     return {
#         "corp_name": corp_name,
#         "summary_path": f"summaries/{corp_name.replace('/', '_')}_ìš”ì•½.txt",
#         "json_path": f"summaries/react_items_{corp_name.replace('/', '_')}.json",
#         "md_path": f"reports/{corp_name.replace('/', '_')}_ìƒì„¸ë³´ê³ ì„œ.md",
#         "news_items": news_items  # ë”•ì…”ë„ˆë¦¬í™” ëª©ì  í¬í•¨ (ë§í¬ + ê°ì„± ê°€ëŠ¥)
#     }

# # âœ… CLI or Colab ì‹¤í–‰ìš©
# if __name__ == "__main__":
#     keyword = input("\U0001f50d ê²€ìƒ‰í•  ê¸°ì—… í‚¤ì›Œë“œ ì…ë ¥: ").strip()
#     investor_type = input("\U0001f464 íˆ¬ìì ìœ í˜• ì„ íƒ (ì•ˆì •í˜• / ê³µê²©í˜• / ìœµí•©í˜•): ").strip()
#     if investor_type not in ["ì•ˆì •í˜•", "ê³µê²©í˜•", "ìœµí•©í˜•"]:
#         print("âŒ íˆ¬ìì ìœ í˜•ì€ 'ì•ˆì •í˜•', 'ê³µê²©í˜•', 'ìœµí•©í˜•' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
#     else:
#         result = generate_report(keyword, investor_type)
#         print("\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
#         print(json.dumps(result, ensure_ascii=False, indent=2))
