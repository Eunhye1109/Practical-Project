import os
import json
import requests
import zipfile
import io
import xml.etree.ElementTree as ET
import pandas as pd
from openai import OpenAI
import os, zipfile, io, xml.etree.ElementTree as ET, pandas as pd
import dart_fss as dart
import re

# ğŸ”‘ API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ê¶Œì¥)
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "YOUR_NAVER_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "YOUR_NAVER_SECRET")
DART_API_KEY = os.getenv("DART_API_KEY", "YOUR_DART_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)


# =========================
# ğŸ§± Top100 í•˜ë“œì½”ë”© (ì˜ˆì‹œ ìŠ¤ëƒ…ìƒ·)
#  - í•„ìš” ì‹œ ì•„ë˜ ëª©ë¡ì„ ì—¬ëŸ¬ë¶„ ì¡°ì§ ê¸°ì¤€ìœ¼ë¡œ 100ê°œ ì±„ìš°ì„¸ìš”.
#  - ì½”ë“œê°€ ì •í™•íˆ 100ê°œì¼ í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì¶©ë¶„íˆ ë§ì„ìˆ˜ë¡ ë§¤ì¹­ í’ˆì§ˆâ†‘
# =========================
TOP100 = [
    ("005930","ì‚¼ì„±ì „ì"), ("000660","SKí•˜ì´ë‹‰ìŠ¤"), ("373220","LGì—ë„ˆì§€ì†”ë£¨ì…˜"),
    ("207940","ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤"), ("005380","í˜„ëŒ€ì°¨"), ("000270","ê¸°ì•„"),
    ("051910","LGí™”í•™"), ("068270","ì…€íŠ¸ë¦¬ì˜¨"), ("006400","ì‚¼ì„±SDI"),
    ("035420","NAVER"), ("035720","ì¹´ì¹´ì˜¤"), ("028260","ì‚¼ì„±ë¬¼ì‚°"),
    ("066570","LGì „ì"), ("003550","LG"), ("005490","POSCOí™€ë”©ìŠ¤"),
    ("012330","í˜„ëŒ€ëª¨ë¹„ìŠ¤"), ("051900","LGìƒí™œê±´ê°•"), ("055550","ì‹ í•œì§€ì£¼"),
    ("105560","KBê¸ˆìœµ"), ("034730","SK"), ("259960","í¬ë˜í”„í†¤"),
    ("096770","SKì´ë…¸ë² ì´ì…˜"), ("010950","S-Oil"), ("003670","í¬ìŠ¤ì½”ì¼€ë¯¸ì¹¼"),
    ("011200","HMM"), ("000810","ì‚¼ì„±í™”ì¬"), ("086790","í•˜ë‚˜ê¸ˆìœµì§€ì£¼"),
    ("316140","ìš°ë¦¬ê¸ˆìœµì§€ì£¼"), ("086520","ì—ì½”í”„ë¡œ"), ("066970","ì—˜ì•¤ì—í”„"),
    ("011170","ë¡¯ë°ì¼€ë¯¸ì¹¼"), ("010130","ê³ ë ¤ì•„ì—°"), ("251270","ë„·ë§ˆë¸”"),
    ("035510","ì‹ ì„¸ê³„"), ("034020","ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°"), ("006800","ë¯¸ë˜ì—ì…‹ì¦ê¶Œ"),
    ("005940","NHíˆ¬ìì¦ê¶Œ"), ("326030","SKë°”ì´ì˜¤íŒœ"), ("018260","ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤"),
    ("032830","ì‚¼ì„±ìƒëª…"), ("180640","í•œì§„ì¹¼"), ("003490","ëŒ€í•œí•­ê³µ"),
    ("272210","í•œí™”ì‹œìŠ¤í…œ"), ("003490","ëŒ€í•œí•­ê³µ"), ("047810","í•œêµ­í•­ê³µìš°ì£¼"),
    ("017670","SKí…”ë ˆì½¤"), ("030200","KT"), ("035600","KGì´ë‹ˆì‹œìŠ¤"),
    ("069620","ëŒ€ì›…ì œì•½"), ("128940","í•œë¯¸ì•½í’ˆ"), ("000720","í˜„ëŒ€ê±´ì„¤"),
    ("006360","GSê±´ì„¤"), ("161390","í•œêµ­íƒ€ì´ì–´ì•¤í…Œí¬ë†€ë¡œì§€"), ("033780","KT&G"),
    ("139480","ì´ë§ˆíŠ¸"), ("204320","ë§Œë„"), ("267250","HDí˜„ëŒ€"),
    ("010620","HDí˜„ëŒ€ì¼ë ‰íŠ¸ë¦­"), ("009540","í•œêµ­ì¡°ì„ í•´ì–‘"), ("009150","ì‚¼ì„±ì „ê¸°"),
    ("000990","DBí•˜ì´í…"), ("021240","ì½”ì›¨ì´"), ("002790","ì•„ëª¨ë ˆG"),
    ("090430","ì•„ëª¨ë ˆí¼ì‹œí”½"), ("028050","ì‚¼ì„±ì—”ì§€ë‹ˆì–´ë§"), ("024110","ê¸°ì—…ì€í–‰"),
    ("003410","ìŒìš©C&E"), ("032640","LGìœ í”ŒëŸ¬ìŠ¤"), ("090370","ë©”íƒ€ë©ìŠ¤"),
    ("096770","SKì´ë…¸ë² ì´ì…˜"), ("003550","LG"), ("011780","ê¸ˆí˜¸ì„ìœ í™”í•™"),
    ("017800","í˜„ëŒ€ì—˜ë¦¬ë² ì´í„°"), ("006650","ëŒ€í•œìœ í™”"), ("011070","LGì´ë…¸í…"),
    ("005930","ì‚¼ì„±ì „ììš°"), ("093370","í›„ì„±"), ("316140","ìš°ë¦¬ê¸ˆìœµì§€ì£¼"),
    # â¬†ï¸ ì˜ˆì‹œ: ì‹¤ì œë¡œëŠ” 100ê°œ ì±„ì›Œ ì“°ì„¸ìš”.
]

# =========================
# âš™ï¸ ê³µí†µ ìœ í‹¸
# =========================
SPV_PAT = re.compile(r"(?:ìœ ë™í™”|ì „ë¬¸íšŒì‚¬|íŠ¹ìˆ˜ëª©ì |SPC|ì œ\d+ì°¨|ìœ í•œíšŒì‚¬)$")  # ë¹„ìº¡ì²˜ ê·¸ë£¹
HANGUL_BLOCK = re.compile(r"[ê°€-í£]+")

def _top_news_tokens(news_titles, k=6):
    txt = " ".join(news_titles or [])
    toks = re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", txt)
    stop = {"ê´€ë ¨","ë³´ë„","ê¸°ì‚¬","ì‚¬ì—…","íšŒì‚¬","ì œê³µ","ì¶œì‹œ","ëŒ€í‘œ","ê³„ì•½","ì§„ì¶œ","ê²½ì˜","ë¶„ê¸°","ì‹¤ì "}
    freq = {}
    for t in toks:
        if t in stop: 
            continue
        freq[t] = freq.get(t, 0) + 1
    return [w for w,_ in sorted(freq.items(), key=lambda x: -x[1])[:k]]

def _parse_json_strict(text: str):
    m = re.search(r"```json\s*(.*?)\s*```", text, flags=re.S)
    payload = m.group(1) if m else text
    return json.loads(payload)

def _clamp_int(x, lo=0, hi=100):
    try:
        v = int(round(float(x)))
        return max(lo, min(hi, v))
    except Exception:
        return None

def brand_root(name: str, min_len: int = 2, max_len: int = 3) -> str:
    if not name:
        return ""
    m = HANGUL_BLOCK.search(name)
    if m:
        block = m.group(0)
        return block[:max_len] if len(block) >= max_len else block[:min_len]
    alnum = re.findall(r"[A-Za-z0-9]+", name)
    if alnum:
        return alnum[0][:max_len]
    return name[:min_len]

def exclude_same_group_candidates(cand_df: pd.DataFrame, base_name: str, base_holder: str = None) -> pd.DataFrame:
    """ë™ì¼ ê³„ì—´/ë¸Œëœë“œ(íšŒì‚¬ëª… ì‹œì‘ ë£¨íŠ¸) ì œê±°"""
    root = brand_root(base_name)
    out = cand_df.copy()
    if 'Name' in out.columns:
        name_col = 'Name'
    else:
        name_col = 'corp_name'
    if root:
        out = out[~out[name_col].str.contains(rf"^{re.escape(root)}", na=False)]
    if base_holder and base_holder != "ì—†ìŒ":
        holder_root = brand_root(base_holder)
        if holder_root:
            out = out[~out[name_col].str.contains(rf"^{re.escape(holder_root)}", na=False)]
    return out

def _one_line_reason(profile, news_keys, product_hint=None, holder_hint=None):
    p_name = product_hint or "í•µì‹¬ ì œí’ˆ/ë¸Œëœë“œ"
    hold   = holder_hint or (("ì§€ì£¼: " + profile.get("ì§€ì£¼íšŒì‚¬")) if profile.get("ì§€ì£¼íšŒì‚¬") and profile.get("ì§€ì£¼íšŒì‚¬")!="ì—†ìŒ" else "ë™ì¼ ì—…ì¢…")
    news   = f"\"{news_keys[0]}\"" if news_keys else "ì¶”ì •: í•´ì™¸ ìœ í†µ"
    return f"{p_name} í¬ì§€ì…”ë‹ ìœ ì‚¬, {hold} ê·¼ê±°, ë‰´ìŠ¤ ì¸ìš© {news}"

# =========================
# ğŸ—‚ DART corpCode (ìºì‹œ)
# =========================
def load_corp_list(api_key, cache_path="corpCode.xml") -> pd.DataFrame:
    if os.path.exists(cache_path):
        tree = ET.parse(cache_path)
        root = tree.getroot()
    else:
        url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={api_key}"
        res = requests.get(url)
        if res.status_code != 200 or res.content[:2] != b'PK':
            raise Exception("âŒ corpCode.zip ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        z = zipfile.ZipFile(io.BytesIO(res.content))
        xml_content = z.read(z.namelist()[0])
        with open(cache_path, "wb") as f:
            f.write(xml_content)
        root = ET.fromstring(xml_content)
    return pd.DataFrame([{
        "corp_code": item.findtext('corp_code'),
        "corp_name": item.findtext('corp_name'),
        "stock_code": item.findtext('stock_code')
    } for item in root.findall('list')])

# =========================
# ğŸ” ê¸°ì¤€ê¸°ì—… ê²€ìƒ‰ & ê°œí™©/ë‰´ìŠ¤
# =========================
def clean_name(name: str) -> str:
    return re.sub(r'\(ì£¼\)|ì£¼ì‹íšŒì‚¬', '', str(name)).strip()

def find_corp_by_keyword(df: pd.DataFrame, keyword: str) -> pd.DataFrame:
    kw = clean_name(keyword)
    return df[df["corp_name"].str.contains(kw, case=False, na=False)]

def pick_best_match(matches_df: pd.DataFrame, keyword: str) -> pd.Series:
    kw = clean_name(keyword)
    cand = matches_df.copy()
    cand["__score__"] = 0
    cand.loc[cand["corp_name"].str.startswith(kw, na=False), "__score__"] += 3
    cand.loc[cand["stock_code"].notnull(), "__score__"] += 2
    cand.loc[~cand["corp_name"].str.contains(SPV_PAT, na=False), "__score__"] += 2
    cand.loc[cand["corp_name"].str.contains(kw, case=False, na=False), "__score__"] += 1
    cand = cand.sort_values(["__score__", "corp_name"], ascending=[False, True])
    return cand.iloc[0]

def collect_company_profile(corp_code: str, api_key: str) -> dict:
    url = "https://opendart.fss.or.kr/api/company.json"
    params = {"crtfc_key": api_key, "corp_code": corp_code}
    res = requests.get(url, params=params).json()
    if res.get("status") != "000":
        return {}
    return {
        "ê¸°ì—…ëª…": res.get("corp_name"),
        "ëŒ€í‘œìëª…": res.get("ceo_nm"),
        "ì—…ì¢…": res.get("induty_code"),
        "ìƒì¥ì—¬ë¶€": "ìƒì¥" if res.get("stock_code") else "ë¹„ìƒì¥",
        "ì„¤ë¦½ì¼ì": res.get("est_dt"),
        "ì£¼ì†Œ": res.get("adres"),
        "ì§€ì£¼íšŒì‚¬": res.get("hm_ownr_corp_nm") or "ì—†ìŒ",
        "corp_code": corp_code
    }

def fetch_news_titles(keyword: str, max_count: int = 7) -> list:
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": keyword, "display": max_count, "sort": "date"}
    res = requests.get(url, headers=headers, params=params)
    if res.status_code != 200:
        return []
    items = res.json().get("items", [])
    return [item.get("title","") for item in items]

# =========================
# ğŸ§¾ Top100 â†’ DataFrame
# =========================
def get_top100_dataframe() -> pd.DataFrame:
    df = pd.DataFrame(TOP100, columns=["Symbol","Name"])
    # ì¤‘ë³µ ì œê±°/ì •ë ¬(ì•ˆì „)
    df = df.drop_duplicates(subset=["Symbol","Name"]).reset_index(drop=True)
    return df

# =========================
# ğŸ¤– GPT: Top100 ì¤‘ ìœ ì‚¬ 2ê°œ + ì ìˆ˜ + í•œ ì¤„ ì´ìœ 
# =========================
def suggest_similar_from_top100(base_profile: dict, base_news_titles: list, top100_df: pd.DataFrame):
    news_keys = _top_news_tokens(base_news_titles, k=6)
    candidates = [{"name": n} for n in top100_df["Name"].tolist()]

    base_root = brand_root(base_profile.get("ê¸°ì—…ëª…",""))
    holder = base_profile.get("ì§€ì£¼íšŒì‚¬","")

    prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ê¸°ì—… ì¬ë¬´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ê¸°ì¤€ ê¸°ì—… ê°œìš”ì™€ ë‰´ìŠ¤ í•µì‹¬ì–´, ê·¸ë¦¬ê³  ìƒì¥ì‚¬ í›„ë³´ ëª©ë¡ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
í›„ë³´ ì¤‘ ê¸°ì¤€ ê¸°ì—…ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ìƒì¥ì‚¬ 2ê°œë¥¼ ê³ ë¥´ì„¸ìš”.

[ê¸°ì¤€ ê¸°ì—… ê°œìš”(JSON)]
{json.dumps(base_profile, ensure_ascii=False)}

[ë‰´ìŠ¤ í•µì‹¬ì–´]
{news_keys}

[í›„ë³´]
{json.dumps(candidates, ensure_ascii=False)}

ì œì•½:
- "similarity"ëŠ” 0~100 ì •ìˆ˜.
- "reason"ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥(12~30ì ê¶Œì¥, ìµœëŒ€ 25ë‹¨ì–´), ë§ˆì¹¨í‘œë¡œ ëë§ºê¸°.
- ë°˜ë“œì‹œ í¬í•¨: (1) êµ¬ì²´ ëª…ì‚¬(ë¸Œëœë“œÂ·ì œí’ˆÂ·ì±„ë„Â·ê³ ê°êµ° ì¤‘ í•˜ë‚˜), (2) ê°œí™© ê·¼ê±°(ì—…ì¢…Â·ì§€ì£¼íšŒì‚¬Â·ì§€ì—­ ë“±), (3) [ë‰´ìŠ¤ í•µì‹¬ì–´] 1ê°œë¥¼ í°ë”°ì˜´í‘œë¡œ ì¸ìš©.
- ê¸ˆì§€: 'í‚¤ì›Œë“œ í¬í•¨', 'ìœ ì‚¬í•œ ì—…ì¢…' ê°™ì€ í¬ê´„ì /ë©”íƒ€ í‘œí˜„.
- ë™ì¼ ê³„ì—´/ë™ì¼ ë¸Œëœë“œ ë£¨íŠ¸ ê¸ˆì§€: ê¸°ì—…ëª…ì´ "{base_root}"ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ì§€ì£¼ "{holder}"ì™€ ë™ì¼ ê³„ì—´ë¡œ ì¶”ì •ë˜ëŠ” í›„ë³´ëŠ” ì œì™¸.

ì¶œë ¥(ë°˜ë“œì‹œ ì´ JSON ë°°ì—´ë§Œ, ì½”ë“œíœìŠ¤ ê¸ˆì§€):
[
  {{ "name": "ê¸°ì—…A", "similarity": 87, "reason": "ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥" }},
  {{ "name": "ê¸°ì—…B", "similarity": 81, "reason": "ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥" }}
]
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}],
        temperature=0
    )
    try:
        data = _parse_json_strict(resp.choices[0].message.content)
    except Exception:
        data = []

    # ê²€ì¦/ë³´ì •
    cand_names = set(top100_df["Name"].tolist())
    out = []
    if isinstance(data, list):
        for it in data:
            nm = str(it.get("name","")).strip()
            if nm in cand_names:
                out.append({
                    "name": nm,
                    "similarity": _clamp_int(it.get("similarity"), 0, 100) or 68,
                    "reason": (it.get("reason") or _one_line_reason(base_profile, news_keys)).strip()
                })

    # ë¶€ì¡±í•˜ë©´ ìƒìœ„ í›„ë³´ë¡œ ì±„ìš°ê¸°
    need = 2 - len(out)
    if need > 0:
        for nm in top100_df["Name"].tolist():
            if nm in {o["name"] for o in out}:
                continue
            out.append({
                "name": nm,
                "similarity": 66 if len(out)==1 else 64,
                "reason": _one_line_reason(base_profile, news_keys)
            })
            if len(out) >= 2: break
    return out[:2]

# =========================
# ğŸš€ íŒŒì´í”„ë¼ì¸
# =========================
def run_similarity_against_top100_hardcoded(keyword: str, candidate_limit: int = 30):
    # 1) Top100 (í•˜ë“œì½”ë”©)
    top100 = get_top100_dataframe()

    # 2) DART corp list â†’ ê¸°ì¤€ ê¸°ì—… ë§¤í•‘
    corp_df = load_corp_list(DART_API_KEY)
    matches = find_corp_by_keyword(corp_df, keyword)
    if matches.empty:
        raise ValueError("âŒ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ì„ DARTì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    matches = matches[~matches["corp_name"].str.contains(SPV_PAT, na=False)]  # SPV ì œê±°
    # ìƒì¥ ìš°ì„ 
    if matches["stock_code"].notnull().any():
        base_row = matches[matches["stock_code"].notnull()].iloc[0]
    else:
        base_row = matches.iloc[0]

    # 3) ê¸°ì¤€ê¸°ì—… ê°œí™© + ë‰´ìŠ¤
    base_profile = collect_company_profile(base_row["corp_code"], DART_API_KEY) or {
        "ê¸°ì—…ëª…": base_row["corp_name"],
        "ì§€ì£¼íšŒì‚¬": "ì—†ìŒ",
        "corp_code": base_row["corp_code"]
    }
    base_news = fetch_news_titles(base_profile["ê¸°ì—…ëª…"], max_count=7)

    # 4) í›„ë³´ ì „ì²˜ë¦¬: SPV/ë™ì¼ê³„ì—´ ì œê±° + ìƒìœ„ N
    cand_df = top100.copy()
    cand_df = cand_df[~cand_df["Name"].str.contains(SPV_PAT, na=False)]
    cand_df = exclude_same_group_candidates(cand_df, base_name=base_profile.get("ê¸°ì—…ëª…",""), base_holder=base_profile.get("ì§€ì£¼íšŒì‚¬", None))
    cand_df = cand_df.head(candidate_limit)  # í•„ìš” ì‹œ 30 ë“±ìœ¼ë¡œ ì¡°ì •

    # 5) GPT ìœ ì‚¬ 2ê°œ + ì´ìœ 
    similar = suggest_similar_from_top100(base_profile, base_news, cand_df)

    return {
        "ê¸°ì¤€ê¸°ì—…": base_profile.get("ê¸°ì—…ëª…", keyword),
        "Top100_ì‚¬ìš©": True,
        "í›„ë³´_ê²€í† ê°œìˆ˜": len(cand_df),
        "ìœ ì‚¬ìƒì¥ì‚¬": similar
    }

# =========================
# ğŸ–¥ï¸ CLI
# =========================
if __name__ == "__main__":
    keyword = input("ğŸ” ê¸°ì¤€ ê¸°ì—…ëª…: ").strip()
    result = run_similarity_against_top100_hardcoded(keyword, candidate_limit=30)
    print(json.dumps(result, ensure_ascii=False, indent=2))


