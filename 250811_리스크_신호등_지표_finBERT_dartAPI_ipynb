{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eunhye1109/Practical-Project/blob/EH/250811_%EB%A6%AC%EC%8A%A4%ED%81%AC_%EC%8B%A0%ED%98%B8%EB%93%B1_%EC%A7%80%ED%91%9C_finBERT_dartAPI_ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9dQw0wrxvWcw"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# RiskScanner Lite: ë‰´ìŠ¤ ê¸°ë°˜ ê°„ì´ ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ëŸ¬ (ì§§ê³  ì•ˆì „í•˜ê²Œ)\n",
        "\n",
        "import os, requests, io, json\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "\n",
        "# ===== ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ì—†ìœ¼ë©´ ìë™ í´ë°±) =====\n",
        "NAVER_ID = os.getenv(\"NAVER_CLIENT_ID\", \"\")\n",
        "NAVER_SECRET = os.getenv(\"NAVER_CLIENT_SECRET\", \"\")\n",
        "USER_AGENT = \"RiskScannerLite/1.0\"\n",
        "HTTP_TIMEOUT = 10\n",
        "DEFAULT_DAYS = 30\n",
        "DEFAULT_MAX_ITEMS = 40\n",
        "MAX_TOTAL_SCORE = 6  # fund_flag(0~) + news_score(0,1,2)ë¥¼ 0~6 êµ¬ê°„ìœ¼ë¡œ ì •ê·œí™”\n",
        "KST = timezone(timedelta(hours=9))\n",
        "try:\n",
        "    # ë³„ë„ íŒŒì¼ì— í‚¤ê°€ ìˆë‹¤ë©´(ì˜ˆ: config_secret.py), ê°€ì¥ ìš°ì„  ì‚¬ìš©\n",
        "    import config_secret as _cfg\n",
        "    OPENAI_API_KEY = getattr(_cfg, \"OPENAI_API_KEY\", None) or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "except Exception:\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "except Exception:\n",
        "    OpenAI = None\n",
        "\n",
        "# ===== ì•ˆì „ ê¸°ë³¸ê°’ =====\n",
        "BACKUP_NEG_LEXICON = [\n",
        "    \"íš¡ë ¹\",\"ë°°ì„\",\"ë¶„ì‹\",\"ì ì\",\"ìë³¸ì ì‹\",\"ìƒì¥íì§€\",\"ì†Œì†¡\",\"ê²½ê³ \",\"ë¦¬ì½œ\",\"ì§•ê³„\",\"ê³¼ì§•ê¸ˆ\",\"ë²Œê¸ˆ\",\n",
        "    \"íŒŒì‚°\",\"ë¶€ë„\",\"ìœ ìƒì¦ì\",\"ê°ì\",\"ìœ ì¶œ\",\"í•´í‚¹\",\"ì‚¬ê³ \",\"í™”ì¬\",\"ì‚¬ë§\",\"êµ¬ì†\",\"ì¡°ì‚¬\",\"ì••ìˆ˜ìˆ˜ìƒ‰\",\n",
        "    \"ê±°ë˜ì •ì§€\",\"ì—°ì²´\",\"ì±„ë¬´ë¶ˆì´í–‰\",\"ì˜ì—…ì •ì§€\",\"ìœ„ë°˜\",\"ë¶€ì •\",\"ë¦¬ìŠ¤í¬\"\n",
        "]\n",
        "RED_KEYWORDS = [\n",
        "    \"íš¡ë ¹\",\"ë°°ì„\",\"ë¶„ì‹íšŒê³„\",\"ìë³¸ì ì‹\",\"ìƒì¥íì§€\",\"ê±°ë˜ì •ì§€\",\"ëŒ€ê·œëª¨ ì ì\",\"ì˜ì—…ì†ì‹¤\",\"ì†Œì†¡\",\"ê³¼ì§•ê¸ˆ\",\n",
        "    \"ì œì¬\",\"ë¦¬ì½œ\",\"ì••ìˆ˜ìˆ˜ìƒ‰\",\"ìœ ì¶œ\",\"í•´í‚¹\",\"íŒŒì‚°\",\"ë¶€ë„\",\"ì±„ë¬´ë¶ˆì´í–‰\",\"êµ¬ì†\"\n",
        "]\n",
        "\n",
        "# ===== ìœ í‹¸ =====\n",
        "def to_kst(dt: datetime) -> datetime:\n",
        "    return (dt.replace(tzinfo=timezone.utc) if dt.tzinfo is None else dt).astimezone(KST)\n",
        "\n",
        "def parse_rfc2822_date(s: str) -> Optional[datetime]:\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        return parsedate_to_datetime(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def clip_days(dt: datetime, days: int = DEFAULT_DAYS) -> bool:\n",
        "    return (to_kst(datetime.utcnow()) - to_kst(dt)) <= timedelta(days=days)\n",
        "\n",
        "def _strip_basic_tags(s: str) -> str:\n",
        "    return (s or \"\").replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n",
        "\n",
        "# ===== ë‰´ìŠ¤ ìˆ˜ì§‘(NAVERâ†’Google RSS í´ë°±) =====\n",
        "def fetch_news_naver(query: str, display: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    if not (NAVER_ID and NAVER_SECRET):\n",
        "        raise RuntimeError(\"NAVER í‚¤ ì—†ìŒ\")\n",
        "    headers = {\n",
        "        \"X-Naver-Client-Id\": NAVER_ID,\n",
        "        \"X-Naver-Client-Secret\": NAVER_SECRET,\n",
        "        \"User-Agent\": USER_AGENT\n",
        "    }\n",
        "    params = {\"query\": query, \"display\": min(100, display), \"start\": 1, \"sort\": \"date\"}\n",
        "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
        "    r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    out = []\n",
        "    for it in data.get(\"items\", []):\n",
        "        dt = parse_rfc2822_date(it.get(\"pubDate\",\"\")) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt):\n",
        "            continue\n",
        "        out.append({\n",
        "            \"title\": _strip_basic_tags(it.get(\"title\") or \"\"),\n",
        "            \"description\": _strip_basic_tags(it.get(\"description\") or \"\"),\n",
        "            \"link\": it.get(\"link\") or it.get(\"originallink\") or \"\",\n",
        "            \"date\": dt.isoformat(),\n",
        "        })\n",
        "        if len(out) >= DEFAULT_MAX_ITEMS:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "def fetch_news_google_rss(query: str, max_items: int = DEFAULT_MAX_ITEMS) -> List[Dict[str, Any]]:\n",
        "    q = requests.utils.quote(query)\n",
        "    url = f\"https://news.google.com/rss/search?q={q}+when:{DEFAULT_DAYS}d&hl=ko&gl=KR&ceid=KR:ko\"\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    r = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    root = ET.fromstring(r.text)\n",
        "    ch = root.find(\"channel\")\n",
        "    out = []\n",
        "    if ch is None: return out\n",
        "    for item in ch.findall(\"item\"):\n",
        "        pub = item.findtext(\"{http://purl.org/dc/elements/1.1/}date\") or item.findtext(\"pubDate\") or \"\"\n",
        "        dt = parse_rfc2822_date(pub) or to_kst(datetime.utcnow())\n",
        "        if not clip_days(dt):\n",
        "            continue\n",
        "        out.append({\n",
        "            \"title\": item.findtext(\"title\") or \"\",\n",
        "            \"description\": item.findtext(\"description\") or \"\",\n",
        "            \"link\": item.findtext(\"link\") or \"\",\n",
        "            \"date\": dt.isoformat(),\n",
        "        })\n",
        "        if len(out) >= max_items:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "def fetch_news(query: str) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        items = fetch_news_naver(query)\n",
        "        if items: return items\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        return fetch_news_google_rss(query)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "# ===== ê°„ì´ ê°ì„±(ë°±ì—… ì‚¬ì „ ê¸°ë°˜) =====\n",
        "def analyze_sentiment_backup(texts: List[str]) -> List[float]:\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        low = (t or \"\").lower()\n",
        "        hits = sum(1 for w in BACKUP_NEG_LEXICON if w in low)\n",
        "        out.append(min(1.0, hits/3.0))  # íˆíŠ¸ ë§ì„ìˆ˜ë¡ ë¶€ì • í™•ë¥ â†‘\n",
        "    return out\n",
        "\n",
        "\n",
        "def smoothed_neg_ratio(neg_probs: List[float], thr: float=0.5, a: float=1.0, b: float=3.0) -> float:\n",
        "    if not neg_probs: return a / (a + b)\n",
        "    n = len(neg_probs); hits = sum(1 for p in neg_probs if p >= thr)\n",
        "    return (hits + a) / (n + a + b)\n",
        "\n",
        "def judge_label(neg_ratio: float) -> str:\n",
        "    return \"ë¶€ì •\" if neg_ratio >= 0.60 else (\"ì¤‘ë¦½\" if neg_ratio >= 0.30 else \"ê¸ì •\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vb_r2hYwtGC",
        "outputId": "c91b85d5-c1f0-4bb2-a1a8-c218713255ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ê²€ìƒ‰í•  ê¸°ì—…ëª… ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\n",
            "> í•œí™”\n",
            "\n",
            "ğŸŸ¨ ì–‘í˜¸ - ë…¸ë€ìƒ‰ (ì ì¬ì  ìœ„í—˜ë„ 36.7%)\n",
            "[ë¦¬ìŠ¤í¬ ì¡°ì–¸] ì¹˜ëª…ì  ì´ë²¤íŠ¸(ìë³¸ì ì‹/ë¶€ë„/íŒŒì‚°/ì±„ë¬´ë¶ˆì´í–‰), ì™„í™”ë  ì—¬ì§€ í˜¹ì€ ê°€ëŠ¥ì„± ìˆìŒ\n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import Text\n",
        "# ===== í€ë”ë©˜í„¸(ê°„ì´) ì¶”ì •: ë‰´ìŠ¤ì—ì„œ í€ë”ë©˜í„¸ì„± ì´ë²¤íŠ¸ ì¶”ì¶œ â†’ flag/reasons =====\n",
        "def infer_fundamentals_from_news(texts: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    ì™¸ë¶€ ì¬ë¬´ API ì—†ì´ë„ ë‰´ìŠ¤ì—ì„œ í€ë”ë©˜í„¸ ì„±ê²©ì˜ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•´ ê°€ì .\n",
        "    ëŒ€ëµì  ê¸°ì¤€ (ìµœëŒ€ 4ì  ê¶Œì¥):\n",
        "      - ì¹˜ëª…ì : 'ìë³¸ì ì‹','ë¶€ë„','íŒŒì‚°','ì±„ë¬´ë¶ˆì´í–‰' â†’ +3\n",
        "      - ê°•í•¨:   'ê°ì','ìƒì¥íì§€','ê±°ë˜ì •ì§€' â†’ +2\n",
        "      - ì¤‘ê°„:   'ìœ ìƒì¦ì','ëŒ€ê·œëª¨ ì ì','ì˜ì—…ì†ì‹¤' â†’ +1\n",
        "    ì»¨í…ìŠ¤íŠ¸ê°€ ë¶ˆí™•ì‹¤í•˜ë©´ ê°€ì ì„ í•œ ë‹¨ê³„ í•˜í–¥(ë³´ìˆ˜í™”).\n",
        "    \"\"\"\n",
        "    text = \" \".join((t or \"\").lower() for t in texts)\n",
        "    reasons: List[str] = []\n",
        "\n",
        "    def hit(word: str) -> bool: return word in text\n",
        "    def any_hit(words: List[str]) -> bool: return any(hit(w) for w in words)\n",
        "\n",
        "    deadly = [\"ìë³¸ì ì‹\",\"ë¶€ë„\",\"íŒŒì‚°\",\"ì±„ë¬´ë¶ˆì´í–‰\"]\n",
        "    strong = [\"ê°ì\",\"ìƒì¥íì§€\",\"ê±°ë˜ì •ì§€\"]\n",
        "    medium = [\"ìœ ìƒì¦ì\",\"ëŒ€ê·œëª¨ ì ì\",\"ì˜ì—…ì†ì‹¤\"]\n",
        "\n",
        "    # ì»¨í…ìŠ¤íŠ¸ ë™ì‚¬(í™•ì •/ê³µì‹œ/íŒê²°/ê°œì‹œ/ë°œìƒ/ì™„ë£Œ/ì§„í–‰/ì„ë°• ë“±)\n",
        "    context_confirm = [\"í™•ì •\",\"ê³µì‹œ\",\"íŒê²°\",\"ê°œì‹œ\",\"ë°œìƒ\",\"ì„ë°•\",\"ì™„ë£Œ\",\"ì§„í–‰\"]\n",
        "    # ì™„í™” í‘œí˜„(ê°€ëŠ¥ì„±/ê²€í† /ìš°ë ¤/ê´€ì¸¡/ì „ë§/ì¶”ì •/í•´ì„/ì†Œì‹/ì„¤ ë“±)\n",
        "    hedging_words = [\"ê°€ëŠ¥ì„±\",\"ê²€í† \",\"ë…¼ì˜\",\"ìš°ë ¤\",\"ê´€ì¸¡\",\"ì „ë§\",\"ì¶”ì •\",\"í•´ì„\",\"ì†Œì‹\",\"ì„¤\"]\n",
        "\n",
        "    raw = 0\n",
        "    if any_hit(deadly):\n",
        "        raw += 3; reasons.append(\"ì¹˜ëª…ì  ì´ë²¤íŠ¸(ìë³¸ì ì‹/ë¶€ë„/íŒŒì‚°/ì±„ë¬´ë¶ˆì´í–‰)\")\n",
        "    if any_hit(strong):\n",
        "        raw += 2; reasons.append(\"ê°•í•œ ì´ë²¤íŠ¸(ê°ì/ìƒì¥íì§€/ê±°ë˜ì •ì§€)\")\n",
        "    if any_hit(medium):\n",
        "        raw += 1; reasons.append(\"ì¤‘ê°„ ì´ë²¤íŠ¸(ìœ ìƒì¦ì/ëŒ€ê·œëª¨ ì ì/ì˜ì—…ì†ì‹¤)\")\n",
        "\n",
        "    # ì»¨í…ìŠ¤íŠ¸/ì™„í™” ë³´ì •\n",
        "    if raw > 0 and not any_hit(context_confirm):\n",
        "        raw -= 1\n",
        "        reasons.append(\"í™•ì •ì´ë¼ ë‹¨ì–¸í•  ìˆ˜ëŠ” ì—†ìŒ\")\n",
        "    if raw > 0 and any_hit(hedging_words):\n",
        "        raw -= 1\n",
        "        reasons.append(\"ì™„í™”ë  ì—¬ì§€ í˜¹ì€ ê°€ëŠ¥ì„± ìˆìŒ\")\n",
        "\n",
        "    score = max(0, min(4, raw))\n",
        "    # ì¤‘ë³µ ì œê±°\n",
        "    uniq = []\n",
        "    for r in reasons:\n",
        "        if r not in uniq:\n",
        "            uniq.append(r)\n",
        "    return {\"flag\": score, \"reasons\": uniq}\n",
        "\n",
        "# ===== ìœ„í—˜ë„ ë³´ì • (ì—°ì†ê°’ ë°˜ì˜, 5% ê³ ì • íƒˆì¶œ) =====\n",
        "def calibrate_risk(total_score: int,\n",
        "                   max_total: int,\n",
        "                   news_count: int,\n",
        "                   neg_ratio: float,\n",
        "                   red_hits: int = 0) -> float:\n",
        "    \"\"\"\n",
        "    êµ¬ì„±:\n",
        "    - ê·œì¹™ ì ìˆ˜ ê¸°ë°˜: base = 100 * (total_score / max_total)\n",
        "    - ì—°ì†í˜• ë³´ì •: news_cont = 15 * neg_ratio  (0~15%)\n",
        "    - ë ˆë“œí‚¤ì›Œë“œ ë³´ì •: red_boost = min(10, 1.5 * red_hits) (0~10%)\n",
        "    - ë°”ë‹¥ì¹˜: ë‰´ìŠ¤ ìˆìœ¼ë©´ 5%, ì—†ìœ¼ë©´ 2%\n",
        "    â†’ ì´í•©ì—ì„œ ë°”ë‹¥ì¹˜ë§Œ ë³´ì¥(ê°•ì œ 5% ê³ ì • ì—†ìŒ)\n",
        "    \"\"\"\n",
        "    if max_total <= 0:\n",
        "        raw = 15.0 * max(0.0, min(1.0, neg_ratio)) + min(10.0, 1.5 * max(0, red_hits))\n",
        "    else:\n",
        "        base = 100.0 * (float(total_score) / float(max_total))\n",
        "        news_cont = 15.0 * max(0.0, min(1.0, neg_ratio))\n",
        "        red_boost = min(10.0, 1.5 * max(0, red_hits))\n",
        "        raw = base + news_cont + red_boost\n",
        "\n",
        "    floor = 5.0 if news_count > 0 else 2.0\n",
        "    return round(max(raw, floor), 1)\n",
        "\n",
        "# ===== GPT í•œì¤„ìš”ì•½ =====\n",
        "GPT_SYSTEM_PROMPT = (\n",
        "    \"ë„ˆëŠ” í•œêµ­ ê¸°ì—… ë¦¬ìŠ¤í¬ë¥¼ ìš”ì•½í•˜ëŠ” ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼. \"\n",
        "    \"ì…ë ¥ JSON(ë‰´ìŠ¤/ë ˆë“œí‚¤ì›Œë“œ/í€ë”ë©˜í„¸/ë¼ë²¨/ìœ„í—˜ë„/ì£¼ìš” ì œëª©)ì„ ë³´ê³  1~2ë¬¸ì¥ìœ¼ë¡œ, \"\n",
        "    \"ìµœëŒ€ 120ì ì´ë‚´ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ ì¹œê·¼í•˜ê²Œ, ì¤‘í•™ìƒ ìˆ˜ì¤€ì˜ ì‰¬ìš´ ë§ë¡œ ìš”ì•½í•˜ë¼. \"\n",
        "    \"ìš”êµ¬ì‚¬í•­: \"\n",
        "    \"1) ì²« ë¬¸ì¥ì— ë ˆë“œí‚¤ì›Œë“œ ê°œìˆ˜ì™€ í•µì‹¬ ë¶€ì • í‚¤ì›Œë“œë¥¼ ê´„í˜¸ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ë¼. \"\n",
        "    \"   ì˜ˆ: 'ë ˆë“œí‚¤ì›Œë“œ 1ê±´(ìë³¸ì ì‹/ë¶€ë„/íŒŒì‚°/ì±„ë¬´ë¶ˆì´í–‰)â€¦' \"\n",
        "    \"2) ê°€ëŠ¥í•˜ë©´ ê¸°ì‚¬ ì œëª©ì—ì„œ êµ¬ì²´ ì‚¬ë¡€ 1ê°œë¥¼ ì§§ê²Œ ë„£ì–´ë¼. ì˜ˆ: 'ì•„ë™ ë‚™ìƒ ì‚¬ê³ ' \"\n",
        "    \"3) ë‘˜ì§¸ ë¬¸ì¥ì— ì™„í™”/ê²€í† /ê°€ëŠ¥ì„± ë“±ì˜ í‘œí˜„ì´ ê¸°ì‚¬ì— ë³´ì´ë©´ 'í•˜ì§€ë§Œ â€¦ ê°€ëŠ¥ì„±ë„ ìˆë„¤ìš”!'ì²˜ëŸ¼ \"\n",
        "    \"4) ì¶œë ¥ì€ ì˜¤ì§ ìš”ì•½ ë¬¸ì¥ë§Œ, ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬/ë¶ˆë¦¿/ì´ëª¨ì§€ëŠ” ì“°ì§€ ë§ ê²ƒ.\"\n",
        ")\n",
        "\n",
        "\n",
        "def build_gpt_user_prompt(company: dict, news: dict, fundamental: dict, combined: dict, top_titles: List[str]) -> str:\n",
        "    payload = {\n",
        "        \"company\": {\"name\": company.get(\"corp_name\", \"\")},\n",
        "        \"news\": {\n",
        "            \"count\": news.get(\"news_count\", 0),\n",
        "            \"neg_ratio\": round(news.get(\"neg_ratio\", 0.0), 3),\n",
        "            \"label\": news.get(\"label_news\", \"\"),\n",
        "            \"red_hits\": news.get(\"red_hits\", 0),\n",
        "            \"top_titles\": top_titles[:3],\n",
        "        },\n",
        "        \"fundamental\": {\n",
        "            \"flag\": fundamental.get(\"flag\", 0),\n",
        "            \"reasons\": (fundamental.get(\"reasons\") or [])[:3],\n",
        "        },\n",
        "        \"combined\": {\n",
        "            \"final_label\": combined.get(\"final_label\", \"\"),\n",
        "            \"risk_pct\": combined.get(\"risk_pct\", 0.0),\n",
        "            \"total_score\": combined.get(\"total_score\", 0),\n",
        "        },\n",
        "        \"instruction\": \"ë¶€ì • ì§•í›„ë¥¼ ìš”ì¸ ë‹¨ìœ„ë¡œ ë¬¶ì–´ í•µì‹¬ 1~2ê°œë§Œ ë‚˜ì—´í•˜ê³  ëì— 'ê´€ë§' ë˜ëŠ” 'ì£¼ì˜'ë¡œ ë§ˆë¬´ë¦¬.\"\n",
        "    }\n",
        "    return \"ë‹¤ìŒ JSONì„ í•œ ì¤„ë¡œ ìš”ì•½:\\n\\n\" + json.dumps(payload, ensure_ascii=False, indent=2)\n",
        "\n",
        "def gpt_one_liner(company, news, fundamental, combined, top_titles) -> str:\n",
        "    # í´ë°±(í‚¤ ì—†ìŒ ë˜ëŠ” SDK ì—†ìŒ)\n",
        "    def _fallback():\n",
        "        bits = []\n",
        "        if news.get(\"red_hits\", 0) > 0:\n",
        "            bits.append(f\"ë ˆë“œí‚¤ì›Œë“œ {news['red_hits']}ê±´\")\n",
        "        label = news.get(\"label_news\", \"\")\n",
        "        if label == \"ë¶€ì •\":\n",
        "            bits.append(\"ë¶€ì • ê¸°ì‚¬ ë‹¤ìˆ˜\")\n",
        "        if fundamental.get(\"flag\", 0) >= 2:\n",
        "            bits.append(\"í€ë”ë©˜í„¸ ê²½ê³ \")\n",
        "        if not bits:\n",
        "            bits.append(\"íŠ¹ì´ ë¶€ì • ì‹ í˜¸ ì œí•œ\")\n",
        "        tail = \"ê´€ë§\" if (combined.get(\"final_label\") or label) != \"ê¸ì •\" else \"ì£¼ì˜\"\n",
        "        return \"Â·\".join(bits[:2]) + f\", {tail}\"\n",
        "\n",
        "    if not (OPENAI_API_KEY and OpenAI):\n",
        "        return _fallback()\n",
        "\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    try:\n",
        "        sys_prompt = GPT_SYSTEM_PROMPT\n",
        "        user_prompt = build_gpt_user_prompt(company, news, fundamental, combined, top_titles)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"system\", \"content\": sys_prompt},\n",
        "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
        "            max_tokens=80,\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        txt = (resp.choices[0].message.content or \"\").strip()\n",
        "        return (txt.splitlines()[0] if txt else _fallback())[:120]\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ GPT í•œì¤„ìš”ì•½ ì˜¤ë¥˜: {e}\")\n",
        "        return _fallback()\n",
        "\n",
        "#ìµœì¢… ë¼ë²¨ í•¨ìˆ˜ ì¶”ê°€\n",
        "def decide_final_label(total_score: int) -> str:\n",
        "    \"\"\"\n",
        "    í•©ì‚°ì ìˆ˜ ê¸°ë°˜ ìµœì¢… ë¼ë²¨:\n",
        "    - 0~1: ì•ˆì •\n",
        "    - 2~3: ì–‘í˜¸\n",
        "    - 4~ : ì£¼ì˜\n",
        "    \"\"\"\n",
        "    if total_score >= 4:\n",
        "        return \"ì£¼ì˜\"\n",
        "    if total_score >= 2:\n",
        "        return \"ì–‘í˜¸\"\n",
        "    return \"ì•ˆì •\"\n",
        "\n",
        "\n",
        "# ===== í‰ê°€ íŒŒì´í”„ë¼ì¸ =====\n",
        "def evaluate_company(name: str) -> Dict[str, Any]:\n",
        "    # 1) ë‰´ìŠ¤ ìˆ˜ì§‘\n",
        "    items = fetch_news(name)\n",
        "\n",
        "    # 2) ê°„ì´ ê°ì„±/ë ˆë“œí‚¤ì›Œë“œ\n",
        "    texts = [f\"{it.get('title','')} {it.get('description','')}\".strip() for it in items]\n",
        "    neg_probs = analyze_sentiment_backup(texts) if texts else []\n",
        "    neg_ratio = smoothed_neg_ratio(neg_probs)\n",
        "    label_news = judge_label(neg_ratio)\n",
        "    red_hits = 0\n",
        "    for t in texts:\n",
        "        low = (t or \"\").lower()\n",
        "        red_hits += sum(1 for w in RED_KEYWORDS if w.lower() in low)\n",
        "\n",
        "    # 3) í€ë”ë©˜í„¸(ê°„ì´) ì¶”ì • â†’ flag/reasons\n",
        "    fundamental = infer_fundamentals_from_news(texts)\n",
        "    fund_flag = int(fundamental.get(\"flag\", 0) or 0)\n",
        "\n",
        "    # ğŸ”¼ ë ˆë“œí‚¤ì›Œë“œê°€ ë§ìœ¼ë©´ í€ë”ë©˜í„¸ ê°€ì (ìµœëŒ€ 1ì ) â€” ì‹¤ë¬´ì  í”„ë¡ì‹œ\n",
        "    if red_hits >= 3:\n",
        "        fund_flag = min(4, fund_flag + 1)\n",
        "        reasons = fundamental.get(\"reasons\") or []\n",
        "        reasons.append(\"ë¶€ì • í‚¤ì›Œë“œ ë‹¤ìˆ˜(3ê±´ ì´ìƒ)\")\n",
        "        fundamental[\"reasons\"] = reasons\n",
        "        fundamental[\"flag\"] = fund_flag\n",
        "\n",
        "    # 4) ì¢…í•© ì ìˆ˜ ë° ìœ„í—˜ë„ ë³´ì • â† red_hitsë¥¼ ê°™ì´ ë°˜ì˜\n",
        "    news_score = {\"ê¸ì •\": 0, \"ì¤‘ë¦½\": 1, \"ë¶€ì •\": 2}[label_news]\n",
        "    total_score = fund_flag + news_score\n",
        "    risk_pct = calibrate_risk(total_score, MAX_TOTAL_SCORE, len(items), neg_ratio, red_hits=red_hits)\n",
        "\n",
        "    # ğŸ”½ risk_pct ê³„ì‚° í›„ì— ìµœì¢… ë¼ë²¨ ê²°ì •\n",
        "    final_label = decide_final_label(total_score)\n",
        "\n",
        "    # ìœ„í—˜ë„ ê°€ë“œë ˆì¼\n",
        "    if risk_pct >= 60.0:\n",
        "      final_label = \"ë¶€ì •\"\n",
        "    elif risk_pct >= 35.0 and final_label == \"ê¸ì •\":\n",
        "      final_label = \"ì¤‘ë¦½\"\n",
        "\n",
        "# 5) ì»´í¬ë„ŒíŠ¸ dictë“¤\n",
        "    company = {\"corp_name\": name, \"corp_code\": \"\", \"stock_code\": \"\"}\n",
        "    news = {\n",
        "        \"news_count\": len(items),\n",
        "        \"neg_ratio\": neg_ratio,\n",
        "        \"label_news\": label_news,\n",
        "        \"red_hits\": red_hits\n",
        "    }\n",
        "\n",
        "    # 6) GPT í•œ ì¤„ ìš”ì•½\n",
        "    top_titles = [it.get(\"title\", \"\") for it in items][:3]\n",
        "    combined = {\n",
        "        \"final_label\": final_label,    # â† risk_pct ê°€ë“œë ˆì¼ ë°˜ì˜ëœ ë¼ë²¨\n",
        "        \"total_score\": total_score,    # â† í€ë”ë©˜í„¸ flag + ë‰´ìŠ¤ ì ìˆ˜\n",
        "        \"risk_pct\": risk_pct,          # â† calibrate_risk ê²°ê³¼\n",
        "        \"one_liner\": gpt_one_liner(company, news, fundamental, {\n",
        "            \"final_label\": final_label,\n",
        "            \"total_score\": total_score,\n",
        "            \"risk_pct\": risk_pct,\n",
        "        }, top_titles)                 # â† í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ GPT í•œì¤„ìš”ì•½\n",
        "    }\n",
        "\n",
        "\n",
        "    # 7) ê²°ê³¼ ë°˜í™˜\n",
        "    return {\n",
        "        \"company\": company,\n",
        "        \"news\": news,\n",
        "        \"fundamental\": fundamental,\n",
        "        \"combined\": combined,\n",
        "        \"items\": items\n",
        "    }\n",
        "\n",
        "# ===== CLI(ë…¸íŠ¸ë¶/ì„œë²„ ê³µí†µ) =====\n",
        "def main():\n",
        "    print(\"ğŸ” ê²€ìƒ‰í•  ê¸°ì—…ëª… ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\", flush=True)\n",
        "    try:\n",
        "        keyword = input(\"> \").strip()\n",
        "    except EOFError:\n",
        "        print(\"âŒ í‘œì¤€ì…ë ¥ ì—†ìŒ\"); return\n",
        "    if not keyword:\n",
        "        print(\"âŒ ê¸°ì—…ëª… í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"); return\n",
        "\n",
        "    result = evaluate_company(keyword)\n",
        "    corp = result[\"company\"]; news = result[\"news\"]; fundamental = result[\"fundamental\"]; combined = result[\"combined\"]\n",
        "\n",
        "    # print(f\"\\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {news['news_count']}ê±´ (ìµœê·¼ {DEFAULT_DAYS}ì¼)\")\n",
        "    # print(\"\\n===== ê²°ê³¼ =====\")\n",
        "    # print(f\"ê¸°ì—…: {corp['corp_name']} (ë¹„ìƒì¥, corp_code {corp['corp_code']})\")\n",
        "    # # print(f\"[ë‰´ìŠ¤] ë¶€ì •ë¹„ìœ¨(ìŠ¤ë¬´ë”©): {news['neg_ratio']*100:.1f}% | ë ˆë“œí‚¤ì›Œë“œ: {news['red_hits']}\")\n",
        "    # print(f\"[ë¦¬ìŠ¤í¬ ì¡°ì–¸] flag: {fundamental.get('flag',0)} | ì‚¬ìœ : {', '.join(fundamental.get('reasons',[])) or 'ì—†ìŒ'}\")\n",
        "    # # print(f\"[ì¢…í•©íŒì •] ë¼ë²¨: {combined['final_label']} | ì´ì : {combined['total_score']}/{MAX_TOTAL_SCORE} | ìœ„í—˜ë„(ì •ê·œí™”): {combined['risk_pct']:.1f}%\")\n",
        "    # # print(f\"í•œì¤„ìš”ì•½: {combined.get('one_liner','')}\")\n",
        "\n",
        "    # ë¶€ì • í™•ë¥  ìƒìœ„ ê¸°ì‚¬ (ìµœëŒ€ 50ê°œ)\n",
        "    top_neg = sorted(\n",
        "        zip(analyze_sentiment_backup([f\"{i.get('title','')} {i.get('description','')}\".strip() for i in result[\"items\"]]),\n",
        "            result[\"items\"]),\n",
        "        key=lambda x: x[0], reverse=True\n",
        "    )[:50]\n",
        "\n",
        "    # if top_neg:\n",
        "    #     print(\"\\nâš ï¸ ë¶€ì • í™•ë¥  ë†’ì€ ê¸°ì‚¬ Top 50\")\n",
        "    #     for p, it in top_neg:\n",
        "    #         dt = it.get(\"date\",\"\")[:19].replace(\"T\",\" \")\n",
        "    #         print(f\"- {p*100:5.1f}% | {dt} | {it.get('title','').strip()}\")\n",
        "\n",
        "    # ìƒ‰ìƒÂ·ë¦¬ìŠ¤í¬ ê´€ë ¨ ì½”ë©˜íŠ¸\n",
        "    mapping = {\"ì•ˆì •\": \"âœ… ì•ˆì • - ì´ˆë¡ìƒ‰\", \"ì–‘í˜¸\": \"ğŸŸ¨ ì–‘í˜¸ - ë…¸ë€ìƒ‰\", \"ì£¼ì˜\": \"ğŸŸ¥ ì£¼ì˜ - ë¹¨ê°„ìƒ‰\"}\n",
        "    print(\"\\n\" + mapping.get(combined[\"final_label\"], \"â„¹ï¸ ì•Œ ìˆ˜ ì—†ìŒ - íšŒìƒ‰\"),\n",
        "          f\"(ì ì¬ì  ìœ„í—˜ë„ {combined['risk_pct']:.1f}%)\")\n",
        "    print(f\"[ë¦¬ìŠ¤í¬ ì¡°ì–¸] {', '.join(fundamental.get('reasons',[])) or 'ì—†ìŒ'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUCbNgQFJfhM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNENEBZPWKAFJJ+F7F5cufR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}